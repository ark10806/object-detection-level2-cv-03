diff --git a/.dev_scripts/batch_test_list.py b/.dev_scripts/batch_test_list.py
index 29c33ec2..1e74ce20 100644
--- a/.dev_scripts/batch_test_list.py
+++ b/.dev_scripts/batch_test_list.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # yapf: disable
 atss = dict(
     config='configs/atss/atss_r50_fpn_1x_coco.py',
@@ -45,7 +46,7 @@ centripetalnet = dict(
 )
 cornernet = dict(
     config='configs/cornernet/cornernet_hourglass104_mstest_8x6_210e_coco.py',
-    checkpoint='cornernet_hourglass104_mstest_10x5_210e_coco_20200824_185720-5fefbf1c.pth',  # noqa
+    checkpoint='cornernet_hourglass104_mstest_8x6_210e_coco_20200825_150618-79b44c30.pth',  # noqa
     eval='bbox',
     metric=dict(bbox_mAP=41.2),
 )
@@ -275,7 +276,7 @@ rpn = dict(
 )
 sabl = [
     dict(
-        config='configs/sabl/sabl_retinanet_r50_fpn_1x_coco.py ',
+        config='configs/sabl/sabl_retinanet_r50_fpn_1x_coco.py',
         checkpoint='sabl_retinanet_r50_fpn_1x_coco-6c54fd4f.pth',
         eval='bbox',
         metric=dict(bbox_mAP=37.7),
@@ -299,12 +300,20 @@ sparse_rcnn = dict(
     eval='bbox',
     metric=dict(bbox_mAP=37.9),
 )
-ssd = dict(
-    config='configs/ssd/ssd300_coco.py',
-    checkpoint='ssd300_coco_20200307-a92d2092.pth',
-    eval='bbox',
-    metric=dict(bbox_mAP=25.6),
-)
+ssd = [
+    dict(
+        config='configs/ssd/ssd300_coco.py',
+        checkpoint='ssd300_coco_20210803_015428-d231a06e.pth',
+        eval='bbox',
+        metric=dict(bbox_mAP=25.5),
+    ),
+    dict(
+        config='configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py',
+        checkpoint='ssdlite_mobilenetv2_scratch_600e_coco_20210629_110627-974d9307.pth',# noqa
+        eval='bbox',
+        metric=dict(bbox_mAP=21.3),
+    ),
+]
 tridentnet = dict(
     config='configs/tridentnet/tridentnet_r50_caffe_1x_coco.py',
     checkpoint='tridentnet_r50_caffe_1x_coco_20201230_141838-2ec0b530.pth',
@@ -337,8 +346,14 @@ yolof = dict(
 )
 centernet = dict(
     config='configs/centernet/centernet_resnet18_dcnv2_140e_coco.py',
-    checkpoint='centernet_resnet18_dcnv2_140e_coco_20210520_101209-da388ba2.pth',  # noqa
+    checkpoint='centernet_resnet18_dcnv2_140e_coco_20210702_155131-c8cd631f.pth',  # noqa
     eval='bbox',
     metric=dict(bbox_mAP=29.5),
 )
+yolox = dict(
+    config='configs/yolox/yolox_tiny_8x8_300e_coco.py',
+    checkpoint='yolox_tiny_8x8_300e_coco_20210806_234250-4ff3b67e.pth',  # noqa
+    eval='bbox',
+    metric=dict(bbox_mAP=31.5),
+)
 # yapf: enable
diff --git a/.dev_scripts/batch_train_list.txt b/.dev_scripts/batch_train_list.txt
index 7fe77b46..48ad90ce 100644
--- a/.dev_scripts/batch_train_list.txt
+++ b/.dev_scripts/batch_train_list.txt
@@ -39,7 +39,7 @@ configs/sabl/sabl_retinanet_r50_fpn_1x_coco.py
 configs/ssd/ssd300_coco.py
 configs/tridentnet/tridentnet_r50_caffe_1x_coco.py
 configs/vfnet/vfnet_r50_fpn_1x_coco.py
-configs/yolact/yolact_r50_1x8_coco.py
+configs/yolact/yolact_r50_8x8_coco.py
 configs/yolo/yolov3_d53_320_273e_coco.py
 configs/sparse_rcnn/sparse_rcnn_r50_fpn_1x_coco.py
 configs/scnet/scnet_r50_fpn_1x_coco.py
@@ -61,3 +61,6 @@ configs/resnest/mask_rcnn_s50_fpn_syncbn-backbone+head_mstrain_1x_coco.py
 configs/res2net/faster_rcnn_r2_101_fpn_2x_coco.py
 configs/groie/faster_rcnn_r50_fpn_groie_1x_coco.py
 configs/centernet/centernet_resnet18_dcnv2_140e_coco.py
+configs/yolox/yolox_tiny_8x8_300e_coco.py
+configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py
+configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py
diff --git a/.dev_scripts/benchmark_filter.py b/.dev_scripts/benchmark_filter.py
index 81b363ed..7921ef9d 100644
--- a/.dev_scripts/benchmark_filter.py
+++ b/.dev_scripts/benchmark_filter.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
diff --git a/.dev_scripts/benchmark_inference_fps.py b/.dev_scripts/benchmark_inference_fps.py
index 2befc901..b4582592 100644
--- a/.dev_scripts/benchmark_inference_fps.py
+++ b/.dev_scripts/benchmark_inference_fps.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
diff --git a/.dev_scripts/benchmark_test_image.py b/.dev_scripts/benchmark_test_image.py
index cf37382f..75f7576d 100644
--- a/.dev_scripts/benchmark_test_image.py
+++ b/.dev_scripts/benchmark_test_image.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import logging
 import os.path as osp
 from argparse import ArgumentParser
diff --git a/.dev_scripts/convert_test_benchmark_script.py b/.dev_scripts/convert_test_benchmark_script.py
index bb3cefbb..c31cad4e 100644
--- a/.dev_scripts/convert_test_benchmark_script.py
+++ b/.dev_scripts/convert_test_benchmark_script.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
diff --git a/.dev_scripts/convert_train_benchmark_script.py b/.dev_scripts/convert_train_benchmark_script.py
index db4181b8..1ccd8e94 100644
--- a/.dev_scripts/convert_train_benchmark_script.py
+++ b/.dev_scripts/convert_train_benchmark_script.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
diff --git a/.dev_scripts/gather_models.py b/.dev_scripts/gather_models.py
index 0c1048f4..9908f068 100644
--- a/.dev_scripts/gather_models.py
+++ b/.dev_scripts/gather_models.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import glob
 import json
@@ -29,6 +30,12 @@ def process_checkpoint(in_file, out_file):
     # remove optimizer for smaller file size
     if 'optimizer' in checkpoint:
         del checkpoint['optimizer']
+
+    # remove ema state_dict
+    for key in list(checkpoint['state_dict']):
+        if key.startswith('ema_'):
+            checkpoint['state_dict'].pop(key)
+
     # if it is necessary to remove some sensitive data in checkpoint['meta'],
     # add the code here.
     if torch.__version__ >= '1.6':
@@ -78,6 +85,7 @@ def get_dataset_name(config):
     name_map = dict(
         CityscapesDataset='Cityscapes',
         CocoDataset='COCO',
+        CocoPanopticDataset='COCO',
         DeepFashionDataset='Deep Fashion',
         LVISV05Dataset='LVIS v0.5',
         LVISV1Dataset='LVIS v1',
@@ -124,6 +132,13 @@ def convert_model_info_to_pwc(model_infos):
                     Task='Instance Segmentation',
                     Dataset=dataset_name,
                     Metrics={'mask AP': metric}))
+        if 'PQ' in model['results']:
+            metric = round(model['results']['PQ'], 1)
+            results.append(
+                OrderedDict(
+                    Task='Panoptic Segmentation',
+                    Dataset=dataset_name,
+                    Metrics={'PQ': metric}))
         pwc_model_info['Results'] = results
 
         link_string = 'https://download.openmmlab.com/mmdetection/v2.0/'
@@ -189,7 +204,10 @@ def main():
         if not isinstance(results_lut, list):
             results_lut = [results_lut]
         # case when using VOC, the evaluation key is only 'mAP'
-        results_lut = [key + '_mAP' for key in results_lut if 'mAP' not in key]
+        # when using Panoptic Dataset, the evaluation key is 'PQ'.
+        for i, key in enumerate(results_lut):
+            if 'mAP' not in key and 'PQ' not in key:
+                results_lut[i] = key + 'm_AP'
         model_performance = get_final_results(log_json_path, final_epoch,
                                               results_lut)
 
diff --git a/.dev_scripts/gather_test_benchmark_metric.py b/.dev_scripts/gather_test_benchmark_metric.py
index c8f3f662..07c6bf42 100644
--- a/.dev_scripts/gather_test_benchmark_metric.py
+++ b/.dev_scripts/gather_test_benchmark_metric.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import glob
 import os.path as osp
diff --git a/.dev_scripts/gather_train_benchmark_metric.py b/.dev_scripts/gather_train_benchmark_metric.py
index 1ad602ab..f9c6c804 100644
--- a/.dev_scripts/gather_train_benchmark_metric.py
+++ b/.dev_scripts/gather_train_benchmark_metric.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import glob
 import os.path as osp
diff --git a/.dev_scripts/test_benchmark.sh b/.dev_scripts/test_benchmark.sh
index eb086725..cb799509 100644
--- a/.dev_scripts/test_benchmark.sh
+++ b/.dev_scripts/test_benchmark.sh
@@ -16,7 +16,7 @@ GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION crpn_fas
 echo 'configs/centripetalnet/centripetalnet_hourglass104_mstest_16x6_210e_coco.py' &
 GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION centripetalnet_hourglass104_mstest_16x6_210e_coco configs/centripetalnet/centripetalnet_hourglass104_mstest_16x6_210e_coco.py $CHECKPOINT_DIR/centripetalnet_hourglass104_mstest_16x6_210e_coco_20200915_204804-3ccc61e5.pth --work-dir tools/batch_test/centripetalnet_hourglass104_mstest_16x6_210e_coco --eval bbox --cfg-option dist_params.port=29672  &
 echo 'configs/cornernet/cornernet_hourglass104_mstest_8x6_210e_coco.py' &
-GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION cornernet_hourglass104_mstest_8x6_210e_coco configs/cornernet/cornernet_hourglass104_mstest_8x6_210e_coco.py $CHECKPOINT_DIR/cornernet_hourglass104_mstest_10x5_210e_coco_20200824_185720-5fefbf1c.pth --work-dir tools/batch_test/cornernet_hourglass104_mstest_8x6_210e_coco --eval bbox --cfg-option dist_params.port=29673  &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION cornernet_hourglass104_mstest_8x6_210e_coco configs/cornernet/cornernet_hourglass104_mstest_8x6_210e_coco.py $CHECKPOINT_DIR/cornernet_hourglass104_mstest_8x6_210e_coco_20200825_150618-79b44c30.pth --work-dir tools/batch_test/cornernet_hourglass104_mstest_8x6_210e_coco --eval bbox --cfg-option dist_params.port=29673  &
 echo 'configs/dcn/faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py' &
 GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco configs/dcn/faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py $CHECKPOINT_DIR/faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco_20200130-d68aed1e.pth --work-dir tools/batch_test/faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco --eval bbox --cfg-option dist_params.port=29674  &
 echo 'configs/deformable_detr/deformable_detr_r50_16x2_50e_coco.py' &
@@ -100,7 +100,7 @@ GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION scnet_r5
 echo 'configs/sparse_rcnn/sparse_rcnn_r50_fpn_1x_coco.py' &
 GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION sparse_rcnn_r50_fpn_1x_coco configs/sparse_rcnn/sparse_rcnn_r50_fpn_1x_coco.py $CHECKPOINT_DIR/sparse_rcnn_r50_fpn_1x_coco_20201222_214453-dc79b137.pth --work-dir tools/batch_test/sparse_rcnn_r50_fpn_1x_coco --eval bbox --cfg-option dist_params.port=29714  &
 echo 'configs/ssd/ssd300_coco.py' &
-GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION ssd300_coco configs/ssd/ssd300_coco.py $CHECKPOINT_DIR/ssd300_coco_20200307-a92d2092.pth --work-dir tools/batch_test/ssd300_coco --eval bbox --cfg-option dist_params.port=29715  &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION ssd300_coco configs/ssd/ssd300_coco.py $CHECKPOINT_DIR/ssd300_coco_20210803_015428-d231a06e.pth --work-dir tools/batch_test/ssd300_coco --eval bbox --cfg-option dist_params.port=29715  &
 echo 'configs/tridentnet/tridentnet_r50_caffe_1x_coco.py' &
 GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION tridentnet_r50_caffe_1x_coco configs/tridentnet/tridentnet_r50_caffe_1x_coco.py $CHECKPOINT_DIR/tridentnet_r50_caffe_1x_coco_20201230_141838-2ec0b530.pth --work-dir tools/batch_test/tridentnet_r50_caffe_1x_coco --eval bbox --cfg-option dist_params.port=29716  &
 echo 'configs/vfnet/vfnet_r50_fpn_1x_coco.py' &
@@ -112,4 +112,8 @@ GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION yolov3_d
 echo 'configs/yolof/yolof_r50_c5_8x8_1x_coco.py' &
 GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION yolof_r50_c5_8x8_1x_coco configs/yolof/yolof_r50_c5_8x8_1x_coco.py $CHECKPOINT_DIR/yolof_r50_c5_8x8_1x_coco_20210425_024427-8e864411.pth --work-dir tools/batch_test/yolof_r50_c5_8x8_1x_coco --eval bbox --cfg-option dist_params.port=29720  &
 echo 'configs/centernet/centernet_resnet18_dcnv2_140e_coco.py' &
-GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION centernet_resnet18_dcnv2_140e_coco configs/centernet/centernet_resnet18_dcnv2_140e_coco.py $CHECKPOINT_DIR/centernet_resnet18_dcnv2_140e_coco_20210520_101209-da388ba2.pth --work-dir tools/batch_test/centernet_resnet18_dcnv2_140e_coco --eval bbox --cfg-option dist_params.port=29721  &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION centernet_resnet18_dcnv2_140e_coco configs/centernet/centernet_resnet18_dcnv2_140e_coco.py $CHECKPOINT_DIR/centernet_resnet18_dcnv2_140e_coco_20210702_155131-c8cd631f.pth --work-dir tools/batch_test/centernet_resnet18_dcnv2_140e_coco --eval bbox --cfg-option dist_params.port=29721  &
+echo 'configs/yolox/yolox_tiny_8x8_300e_coco.py' &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION yolox_tiny_8x8_300e_coco configs/yolox/yolox_tiny_8x8_300e_coco.py $CHECKPOINT_DIR/yolox_tiny_8x8_300e_coco_20210806_234250-4ff3b67e.pth --work-dir tools/batch_test/yolox_tiny_8x8_300e_coco --eval bbox --cfg-option dist_params.port=29722  &
+echo 'configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py' &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 tools/slurm_test.sh $PARTITION ssdlite_mobilenetv2_scratch_600e_coco configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py $CHECKPOINT_DIR/ssdlite_mobilenetv2_scratch_600e_coco_20210629_110627-974d9307.pth --work-dir tools/batch_test/ssdlite_mobilenetv2_scratch_600e_coco --eval bbox --cfg-option dist_params.port=29723  &
diff --git a/.dev_scripts/test_init_backbone.py b/.dev_scripts/test_init_backbone.py
index e8ba7a88..862f4afb 100644
--- a/.dev_scripts/test_init_backbone.py
+++ b/.dev_scripts/test_init_backbone.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """Check out backbone whether successfully load pretrained checkpoint."""
 import copy
 import os
diff --git a/.dev_scripts/train_benchmark.sh b/.dev_scripts/train_benchmark.sh
index a263ca05..a8086db9 100644
--- a/.dev_scripts/train_benchmark.sh
+++ b/.dev_scripts/train_benchmark.sh
@@ -126,3 +126,9 @@ echo 'configs/groie/faster_rcnn_r50_fpn_groie_1x_coco.py' &
 GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 ./tools/slurm_train.sh openmmlab faster_rcnn_r50_fpn_groie_1x_coco configs/groie/faster_rcnn_r50_fpn_groie_1x_coco.py ./tools/work_dir/faster_rcnn_r50_fpn_groie_1x_coco --cfg-options checkpoint_config.max_keep_ckpts=1 >/dev/null &
 echo 'configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py' &
 GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 ./tools/slurm_train.sh openmmlab mask_rcnn_r50_fpn_1x_cityscapes configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py ./tools/work_dir/mask_rcnn_r50_fpn_1x_cityscapes --cfg-options checkpoint_config.max_keep_ckpts=1 >/dev/null &
+echo 'configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py' &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 ./tools/slurm_train.sh openmmlab panoptic_fpn_r50_fpn_1x_coco configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py ./tools/work_dir/panoptic_fpn_r50_fpn_1x_coco --cfg-options checkpoint_config.max_keep_ckpts=1 >/dev/null &
+echo 'configs/yolox/yolox_tiny_8x8_300e_coco.py' &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 ./tools/slurm_train.sh openmmlab yolox_tiny_8x8_300e_coco configs/yolox/yolox_tiny_8x8_300e_coco.py ./tools/work_dir/yolox_tiny_8x8_300e_coco --cfg-options checkpoint_config.max_keep_ckpts=1 >/dev/null &
+echo 'configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py' &
+GPUS=8  GPUS_PER_NODE=8  CPUS_PER_TASK=2 ./tools/slurm_train.sh openmmlab ssdlite_mobilenetv2_scratch_600e_coco configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py ./tools/work_dir/ssdlite_mobilenetv2_scratch_600e_coco --cfg-options checkpoint_config.max_keep_ckpts=1 >/dev/null &
diff --git a/.github/workflows/build.yml b/.github/workflows/build.yml
index c3ed07b8..1a767b65 100644
--- a/.github/workflows/build.yml
+++ b/.github/workflows/build.yml
@@ -23,11 +23,11 @@ jobs:
           interrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex "__repr__" --fail-under 80 mmdet
 
   build_cpu:
-    runs-on: ubuntu-latest
+    runs-on: ubuntu-18.04
     strategy:
       matrix:
         python-version: [3.7]
-        torch: [1.3.1, 1.5.1, 1.6.0]
+        torch: [1.3.1, 1.5.1, 1.6.0, 1.7.0, 1.8.0, 1.9.0]
         include:
           - torch: 1.3.1
             torchvision: 0.4.2
@@ -44,6 +44,9 @@ jobs:
           - torch: 1.8.0
             torchvision: 0.9.0
             mmcv: "latest+torch1.8.0+cpu"
+          - torch: 1.9.0
+            torchvision: 0.10.0
+            mmcv: "latest+torch1.9.0+cpu"
     steps:
       - uses: actions/checkout@v2
       - name: Set up Python ${{ matrix.python-version }}
@@ -62,6 +65,7 @@ jobs:
       - name: Install unittest dependencies
         run: |
           pip install -r requirements/tests.txt -r requirements/optional.txt
+          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations
           pip install git+https://github.com/cocodataset/panopticapi.git
       - name: Build and install
         run: rm -rf .eggs && pip install -e .
@@ -71,48 +75,43 @@ jobs:
           coverage xml
           coverage report -m
 
-  build_cuda:
-    runs-on: ubuntu-latest
+  build_cuda101:
+    runs-on: ubuntu-18.04
+    container:
+      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel
 
-    env:
-      CUDA: 10.1.105-1
-      CUDA_SHORT: 10.1
-      UBUNTU_VERSION: ubuntu1804
     strategy:
       matrix:
         python-version: [3.7]
-        torch: [1.3.1, 1.5.1+cu101, 1.6.0+cu101, 1.7.0+cu101, 1.8.0+cu101]
+        torch:
+          [
+            1.3.1,
+            1.5.1+cu101,
+            1.6.0+cu101,
+            1.7.0+cu101,
+            1.8.0+cu101,
+          ]
         include:
           - torch: 1.3.1
             torch_version: torch1.3.1
             torchvision: 0.4.2
-            mmcv: "latest+torch1.3.0+cu101"
+            mmcv_link: "torch1.3.0"
           - torch: 1.5.1+cu101
             torch_version: torch1.5.1
             torchvision: 0.6.1+cu101
-            mmcv: "latest+torch1.5.0+cu101"
-          - torch: 1.6.0+cu101
-            torch_version: torch1.6.0
-            torchvision: 0.7.0+cu101
-            mmcv: "latest+torch1.6.0+cu101"
-          - torch: 1.6.0+cu101
-            torch_version: torch1.6.0
-            torchvision: 0.7.0+cu101
-            mmcv: "latest+torch1.6.0+cu101"
-            python-version: 3.6
+            mmcv_link: "torch1.5.0"
           - torch: 1.6.0+cu101
             torch_version: torch1.6.0
             torchvision: 0.7.0+cu101
-            mmcv: "latest+torch1.6.0+cu101"
-            python-version: 3.8
+            mmcv_link: "torch1.6.0"
           - torch: 1.7.0+cu101
             torch_version: torch1.7.0
             torchvision: 0.8.1+cu101
-            mmcv: "latest+torch1.7.0+cu101"
+            mmcv_link: "torch1.7.0"
           - torch: 1.8.0+cu101
             torch_version: torch1.8.0
             torchvision: 0.9.0+cu101
-            mmcv: "latest+torch1.8.0+cu101"
+            mmcv_link: "torch1.8.0"
 
     steps:
       - uses: actions/checkout@v2
@@ -120,29 +119,27 @@ jobs:
         uses: actions/setup-python@v2
         with:
           python-version: ${{ matrix.python-version }}
-      - name: Install CUDA
+      - name: Install system dependencies
         run: |
-          export INSTALLER=cuda-repo-${UBUNTU_VERSION}_${CUDA}_amd64.deb
-          wget http://developer.download.nvidia.com/compute/cuda/repos/${UBUNTU_VERSION}/x86_64/${INSTALLER}
-          sudo dpkg -i ${INSTALLER}
-          wget https://developer.download.nvidia.com/compute/cuda/repos/${UBUNTU_VERSION}/x86_64/7fa2af80.pub
-          sudo apt-key add 7fa2af80.pub
-          sudo apt update -qq
-          sudo apt install -y cuda-${CUDA_SHORT/./-} cuda-cufft-dev-${CUDA_SHORT/./-}
-          sudo apt clean
-          export CUDA_HOME=/usr/local/cuda-${CUDA_SHORT}
-          export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${CUDA_HOME}/include:${LD_LIBRARY_PATH}
-          export PATH=${CUDA_HOME}/bin:${PATH}
+          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 python${{matrix.python-version}}-dev
+          apt-get clean
+          rm -rf /var/lib/apt/lists/*
       - name: Install Pillow
-        run: pip install Pillow==6.2.2
+        run: python -m pip install Pillow==6.2.2
         if: ${{matrix.torchvision < 0.5}}
       - name: Install PyTorch
-        run: pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html
+        run: python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html
+      - name: Install dependencies for compiling onnx when python=3.9
+        run: python -m pip install protobuf && apt-get install libprotobuf-dev protobuf-compiler
+        if: ${{matrix.python-version == '3.9'}}
       - name: Install mmdet dependencies
         run: |
-          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html
-          pip install -r requirements.txt
-          pip install git+https://github.com/cocodataset/panopticapi.git
+          python -V
+          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.mmcv_link}}/index.html
+          python -m pip install pycocotools
+          python -m pip install -r requirements/tests.txt -r requirements/optional.txt
+          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations
+          python -m pip install git+https://github.com/cocodataset/panopticapi.git
           python -c 'import mmcv; print(mmcv.__version__)'
       - name: Build and install
         run: |
@@ -162,3 +159,68 @@ jobs:
           env_vars: OS,PYTHON
           name: codecov-umbrella
           fail_ci_if_error: false
+
+  build_cuda102:
+    runs-on: ubuntu-18.04
+    container:
+      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel
+
+    strategy:
+      matrix:
+        python-version: [3.6, 3.7, 3.8, 3.9-dev]
+        torch: [1.9.0+cu102]
+        include:
+          - torch: 1.9.0+cu102
+            torch_version: torch1.9.0
+            torchvision: 0.10.0+cu102
+            mmcv_link: "torch1.9.0"
+
+    steps:
+      - uses: actions/checkout@v2
+      - name: Set up Python ${{ matrix.python-version }}
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ matrix.python-version }}
+      - name: Install python-dev
+        run: apt-get update && apt-get install -y python${{matrix.python-version}}-dev
+        if: ${{matrix.python-version != '3.9-dev'}}
+      - name: Install system dependencies
+        run: |
+          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6
+          apt-get clean
+          rm -rf /var/lib/apt/lists/*
+      - name: Install Pillow
+        run: python -m pip install Pillow==6.2.2
+        if: ${{matrix.torchvision < 0.5}}
+      - name: Install PyTorch
+        run: python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html
+      - name: Install dependencies for compiling onnx when python=3.9
+        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake
+        if: ${{matrix.python-version == '3.9-dev'}}
+      - name: Install mmdet dependencies
+        run: |
+          python -V
+          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.mmcv_link}}/index.html
+          python -m pip install pycocotools
+          python -m pip install -r requirements/tests.txt -r requirements/optional.txt
+          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations
+          python -m pip install git+https://github.com/cocodataset/panopticapi.git
+          python -c 'import mmcv; print(mmcv.__version__)'
+      - name: Build and install
+        run: |
+          rm -rf .eggs
+          python setup.py check -m -s
+          TORCH_CUDA_ARCH_LIST=7.0 pip install .
+      - name: Run unittests and generate coverage report
+        run: |
+          coverage run --branch --source mmdet -m pytest tests/
+          coverage xml
+          coverage report -m
+      - name: Upload coverage to Codecov
+        uses: codecov/codecov-action@v2
+        with:
+          files: ./coverage.xml
+          flags: unittests
+          env_vars: OS,PYTHON
+          name: codecov-umbrella
+          fail_ci_if_error: false
diff --git a/CITATION.cff b/CITATION.cff
new file mode 100644
index 00000000..aac93137
--- /dev/null
+++ b/CITATION.cff
@@ -0,0 +1,8 @@
+cff-version: 1.2.0
+message: "If you use this software, please cite it as below."
+authors:
+  - name: "MMDetection Contributors"
+title: "OpenMMLab Detection Toolbox and Benchmark"
+date-released: 2018-08-22
+url: "https://github.com/open-mmlab/mmdetection"
+license: Apache-2.0
diff --git a/LICENSE b/LICENSE
index 04adf5cb..1bfc23e4 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,4 +1,4 @@
-Copyright 2018-2019 Open-MMLab. All rights reserved.
+Copyright 2018-2023 OpenMMLab. All rights reserved.
 
                                  Apache License
                            Version 2.0, January 2004
@@ -188,7 +188,7 @@ Copyright 2018-2019 Open-MMLab. All rights reserved.
       same "printed page" as the copyright notice for easier
       identification within third-party archives.
 
-   Copyright 2018-2019 Open-MMLab.
+   Copyright 2018-2023 OpenMMLab.
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
diff --git a/README.md b/README.md
index 2870cc4f..b935ac33 100644
--- a/README.md
+++ b/README.md
@@ -49,7 +49,7 @@ This project is released under the [Apache 2.0 license](LICENSE).
 
 ## Changelog
 
-v2.15.1 was released in 11/08/2021, which supports YOLOX.
+v2.17.0 was released in 28/09/2021.
 Please refer to [changelog.md](docs/changelog.md) for details and release history.
 A comparison between v1.x and v2.0 codebases can be found in [compatibility.md](docs/compatibility.md).
 
@@ -62,10 +62,14 @@ Supported backbones:
 - [x] ResNet (CVPR'2016)
 - [x] ResNeXt (CVPR'2017)
 - [x] VGG (ICLR'2015)
+- [x] MobileNetV2 (CVPR'2018)
 - [x] HRNet (CVPR'2019)
 - [x] RegNet (CVPR'2020)
 - [x] Res2Net (TPAMI'2020)
 - [x] ResNeSt (ArXiv'2020)
+- [X] Swin (CVPR'2021)
+- [x] PVT (ICCV'2021)
+- [x] PVTv2 (ArXiv'2021)
 
 Supported methods:
 
@@ -104,7 +108,7 @@ Supported methods:
 - [x] [Mixed Precision (FP16) Training (ArXiv'2017)](configs/fp16/README.md)
 - [x] [InstaBoost (ICCV'2019)](configs/instaboost/README.md)
 - [x] [GRoIE (ICPR'2020)](configs/groie/README.md)
-- [x] [DetectoRS (ArXix'2020)](configs/detectors/README.md)
+- [x] [DetectoRS (ArXiv'2020)](configs/detectors/README.md)
 - [x] [Generalized Focal Loss (NeurIPS'2020)](configs/gfl/README.md)
 - [x] [CornerNet (ECCV'2018)](configs/cornernet/README.md)
 - [x] [Side-Aware Boundary Localization (ECCV'2020)](configs/sabl/README.md)
@@ -112,16 +116,17 @@ Supported methods:
 - [x] [PAA (ECCV'2020)](configs/paa/README.md)
 - [x] [YOLACT (ICCV'2019)](configs/yolact/README.md)
 - [x] [CentripetalNet (CVPR'2020)](configs/centripetalnet/README.md)
-- [x] [VFNet (ArXix'2020)](configs/vfnet/README.md)
+- [x] [VFNet (ArXiv'2020)](configs/vfnet/README.md)
 - [x] [DETR (ECCV'2020)](configs/detr/README.md)
 - [x] [Deformable DETR (ICLR'2021)](configs/deformable_detr/README.md)
 - [x] [CascadeRPN (NeurIPS'2019)](configs/cascade_rpn/README.md)
 - [x] [SCNet (AAAI'2021)](configs/scnet/README.md)
-- [x] [AutoAssign (ArXix'2020)](configs/autoassign/README.md)
+- [x] [AutoAssign (ArXiv'2020)](configs/autoassign/README.md)
 - [x] [YOLOF (CVPR'2021)](configs/yolof/README.md)
 - [x] [Seasaw Loss (CVPR'2021)](configs/seesaw_loss/README.md)
 - [x] [CenterNet (CVPR'2019)](configs/centernet/README.md)
-- [x] [YOLOX (ArXix'2021)](configs/yolox/README.md)
+- [x] [YOLOX (ArXiv'2021)](configs/yolox/README.md)
+- [x] [SOLO (ECCV'2020)](configs/solo/README.md)
 
 Some other methods are also supported in [projects using MMDetection](./docs/projects.md).
 
diff --git a/README_zh-CN.md b/README_zh-CN.md
index 64e2fc18..94c0b7f4 100644
--- a/README_zh-CN.md
+++ b/README_zh-CN.md
@@ -44,7 +44,7 @@ v1.x 的历史版本支持 PyTorch 1.1 到 1.4，但是我们强烈建议用户
 
 ## 更新日志
 
-最新的月度版本 v2.15.1 在 2021.08.11 发布，新版本支持了 YOLOX。
+最新的月度版本 v2.17.0 在 2021.09.28 发布。
 如果想了解更多版本更新细节和历史信息，请阅读[更新日志](docs/changelog.md)。
 在[兼容性说明文档](docs_zh-CN/compatibility.md)中我们提供了 1.x 和 2.0 版本的详细比较。
 
@@ -57,10 +57,14 @@ v1.x 的历史版本支持 PyTorch 1.1 到 1.4，但是我们强烈建议用户
 - [x] ResNet (CVPR'2016)
 - [x] ResNeXt (CVPR'2017)
 - [x] VGG (ICLR'2015)
+- [x] MobileNetV2 (CVPR'2018)
 - [x] HRNet (CVPR'2019)
 - [x] RegNet (CVPR'2020)
 - [x] Res2Net (TPAMI'2020)
 - [x] ResNeSt (ArXiv'2020)
+- [X] Swin (CVPR'2021)
+- [x] PVT (ICCV'2021)
+- [x] PVTv2 (ArXiv'2021)
 
 已支持的算法：
 
@@ -99,7 +103,7 @@ v1.x 的历史版本支持 PyTorch 1.1 到 1.4，但是我们强烈建议用户
 - [x] [Mixed Precision (FP16) Training (ArXiv'2017)](configs/fp16/README.md)
 - [x] [InstaBoost (ICCV'2019)](configs/instaboost/README.md)
 - [x] [GRoIE (ICPR'2020)](configs/groie/README.md)
-- [x] [DetectoRS (ArXix'2020)](configs/detectors/README.md)
+- [x] [DetectoRS (ArXiv'2020)](configs/detectors/README.md)
 - [x] [Generalized Focal Loss (NeurIPS'2020)](configs/gfl/README.md)
 - [x] [CornerNet (ECCV'2018)](configs/cornernet/README.md)
 - [x] [Side-Aware Boundary Localization (ECCV'2020)](configs/sabl/README.md)
@@ -107,16 +111,17 @@ v1.x 的历史版本支持 PyTorch 1.1 到 1.4，但是我们强烈建议用户
 - [x] [PAA (ECCV'2020)](configs/paa/README.md)
 - [x] [YOLACT (ICCV'2019)](configs/yolact/README.md)
 - [x] [CentripetalNet (CVPR'2020)](configs/centripetalnet/README.md)
-- [x] [VFNet (ArXix'2020)](configs/vfnet/README.md)
+- [x] [VFNet (ArXiv'2020)](configs/vfnet/README.md)
 - [x] [DETR (ECCV'2020)](configs/detr/README.md)
 - [x] [Deformable DETR (ICLR'2021)](configs/deformable_detr/README.md)
 - [x] [CascadeRPN (NeurIPS'2019)](configs/cascade_rpn/README.md)
 - [x] [SCNet (AAAI'2021)](configs/scnet/README.md)
-- [x] [AutoAssign (ArXix'2020)](configs/autoassign/README.md)
+- [x] [AutoAssign (ArXiv'2020)](configs/autoassign/README.md)
 - [x] [YOLOF (CVPR'2021)](configs/yolof/README.md)
 - [x] [Seasaw Loss (CVPR'2021)](configs/seesaw_loss/README.md)
 - [x] [CenterNet (CVPR'2019)](configs/centernet/README.md)
-- [x] [YOLOX (ArXix'2021)](configs/yolox/README.md)
+- [x] [YOLOX (ArXiv'2021)](configs/yolox/README.md)
+- [x] [SOLO (ECCV'2020)](configs/solo/README.md)
 
 我们在[基于 MMDetection 的项目](./docs/projects.md)中列举了一些其他的支持的算法。
 
diff --git a/configs/_base_/datasets/coco_panoptic.py b/configs/_base_/datasets/coco_panoptic.py
index f7cdcd8f..dbade7c0 100644
--- a/configs/_base_/datasets/coco_panoptic.py
+++ b/configs/_base_/datasets/coco_panoptic.py
@@ -56,4 +56,4 @@ data = dict(
         img_prefix=data_root + 'val2017/',
         seg_prefix=data_root + 'annotations/panoptic_val2017/',
         pipeline=test_pipeline))
-evaluation = dict(interval=1, metric=['pq'])
+evaluation = dict(interval=1, metric=['PQ'])
diff --git a/configs/atss/metafile.yml b/configs/atss/metafile.yml
index 92187e34..f4c567ef 100644
--- a/configs/atss/metafile.yml
+++ b/configs/atss/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - ATSS
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/1912.02424
+    Paper:
+      URL: https://arxiv.org/abs/1912.02424
+      Title: 'Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection'
     README: configs/atss/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/atss.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: atss_r50_fpn_1x_coco
diff --git a/configs/autoassign/metafile.yml b/configs/autoassign/metafile.yml
index 88caebf9..f1e90519 100644
--- a/configs/autoassign/metafile.yml
+++ b/configs/autoassign/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - AutoAssign
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/2007.03496
+    Paper:
+      URL: https://arxiv.org/abs/2007.03496
+      Title: 'AutoAssign: Differentiable Label Assignment for Dense Object Detection'
     README: configs/autoassign/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.12.0/mmdet/models/detectors/autoassign.py#L6
+      Version: v2.12.0
 
 Models:
   - Name: autoassign_r50_fpn_8x2_1x_coco
diff --git a/configs/boostcamp_trash/dataset.py b/configs/boostcamp_trash/dataset.py
deleted file mode 100644
index b06aa32b..00000000
--- a/configs/boostcamp_trash/dataset.py
+++ /dev/null
@@ -1,51 +0,0 @@
-# dataset settings
-dataset_type = 'CocoDataset'
-data_root = '/home/data/data/'
-
-img_norm_cfg = dict(
-    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
-train_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(type='LoadAnnotations', with_bbox=True),
-    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),
-    dict(type='RandomFlip', flip_ratio=0.5),
-    dict(type='Normalize', **img_norm_cfg),
-    dict(type='DefaultFormatBundle'),
-    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
-]
-test_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(
-        type='MultiScaleFlipAug',
-        img_scale=(512, 512),
-        flip=False,
-        transforms=[
-            dict(type='Resize', keep_ratio=True),
-            dict(type='RandomFlip'),
-            dict(type='Normalize', **img_norm_cfg),
-            dict(type='ImageToTensor', keys=['img']),
-            dict(type='Collect', keys=['img']),
-        ])
-]
-
-classes = ("UNKNOWN", "General trash", "Paper", "Paper pack", "Metal", "Glass",
-           "Plastic", "Styrofoam", "Plastic bag", "Battery", "Clothing")
-
-data = dict(
-    samples_per_gpu=4,
-    workers_per_gpu=2,
-    train=dict(
-        type=dataset_type,
-        ann_file=data_root + 'train.json',
-        img_prefix=data_root,
-        pipeline=train_pipeline),
-    val=dict(
-        type=dataset_type,
-        ann_file=data_root + 'val.json',
-        img_prefix=data_root,
-        pipeline=test_pipeline),
-    test=dict(
-        type=dataset_type,
-        ann_file=data_root + 'test.json',
-        img_prefix=data_root,
-        pipeline=test_pipeline))
diff --git a/configs/boostcamp_trash/trash/faster_rcnn_r50_fpn_1x_trash.py b/configs/boostcamp_trash/trash/faster_rcnn_r50_fpn_1x_trash.py
deleted file mode 100644
index de17256a..00000000
--- a/configs/boostcamp_trash/trash/faster_rcnn_r50_fpn_1x_trash.py
+++ /dev/null
@@ -1,19 +0,0 @@
-_base_ = [
-    '../../_base_/models/faster_rcnn_r50_fpn.py',
-    '../dataset.py',
-    '../../_base_/schedules/schedule_1x.py',
-    '../../_base_/default_runtime.py'
-]
-
-model = dict(
-    roi_head=dict(
-        bbox_head=dict(
-            num_classes=11
-        )
-    )
-)
-
-optimizer_config = dict(
-    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))
-
-checkpoint_config = dict(max_keep_ckpts=3, interval=1)
\ No newline at end of file
diff --git a/configs/cascade_rcnn/metafile.yml b/configs/cascade_rcnn/metafile.yml
index 0248db54..1007f2eb 100644
--- a/configs/cascade_rcnn/metafile.yml
+++ b/configs/cascade_rcnn/metafile.yml
@@ -12,8 +12,13 @@ Collections:
         - RPN
         - ResNet
         - RoIAlign
-    Paper: http://dx.doi.org/10.1109/tpami.2019.2956516
+    Paper:
+      URL: http://dx.doi.org/10.1109/tpami.2019.2956516
+      Title: 'Cascade R-CNN: Delving into High Quality Object Detection'
     README: configs/cascade_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/cascade_rcnn.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: cascade_rcnn_r50_caffe_fpn_1x_coco
diff --git a/configs/cascade_rpn/README.md b/configs/cascade_rpn/README.md
index aa7782c3..5e1e60a1 100644
--- a/configs/cascade_rpn/README.md
+++ b/configs/cascade_rpn/README.md
@@ -17,13 +17,13 @@ We provide the code for reproducing experiment results of [Cascade RPN](https://
 
 ### Region proposal performance
 
-| Method | Backbone | Style | Mem (GB) | Train time (s/iter) | Inf time (fps) | AR 1000 |                Download                |
-|:------:|:--------:|:-----:|:--------:|:-------------------:|:--------------:|:-------:|:--------------------------------------:|
-|  CRPN  | R-50-FPN | caffe |     -    |          -          |        -       |   72.0  | [model](https://drive.google.com/file/d/1qxVdOnCgK-ee7_z0x6mvAir_glMu2Ihi/view?usp=sharing) |
+| Method | Backbone | Style | Mem (GB) | Train time (s/iter) | Inf time (fps) | AR 1000 | Config |               Download                |
+|:------:|:--------:|:-----:|:--------:|:-------------------:|:--------------:|:-------:|:-------:|:--------------------------------------:|
+|  CRPN  | R-50-FPN | caffe |     -    |          -          |        -       |   72.0  | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/cascade_rpn/crpn_r50_caffe_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/cascade_rpn/crpn_r50_caffe_fpn_1x_coco/cascade_rpn_r50_caffe_fpn_1x_coco-7aa93cef.pth) |
 
 ### Detection performance
 
-|     Method    |   Proposal  | Backbone |  Style  | Schedule | Mem (GB) | Train time (s/iter) | Inf time (fps) | box AP |                   Download                   |
-|:-------------:|:-----------:|:--------:|:-------:|:--------:|:--------:|:-------------------:|:--------------:|:------:|:--------------------------------------------:|
-|   Fast R-CNN  | Cascade RPN | R-50-FPN |  caffe  |    1x    |    -     |          -          |        -       |  39.9  | [model](https://drive.google.com/file/d/1NmbnuY5VHi8I9FE8xnp5uNvh2i-t-6_L/view?usp=sharing) |
-|  Faster R-CNN | Cascade RPN | R-50-FPN |  caffe  |    1x    |    -     |          -          |        -       |  40.4  | [model](https://drive.google.com/file/d/1dS3Q66qXMJpcuuQgDNkLp669E5w1UMuZ/view?usp=sharing) |
+|     Method    |   Proposal  | Backbone |  Style  | Schedule | Mem (GB) | Train time (s/iter) | Inf time (fps) | box AP | Config |           Download                   |
+|:-------------:|:-----------:|:--------:|:-------:|:--------:|:--------:|:-------------------:|:--------------:|:------:|:-------:|:--------------------------------------------:|
+|   Fast R-CNN  | Cascade RPN | R-50-FPN |  caffe  |    1x    |    -     |          -          |        -       |  39.9  | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/cascade_rpn/crpn_fast_rcnn_r50_caffe_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/cascade_rpn/crpn_fast_rcnn_r50_caffe_fpn_1x_coco/crpn_fast_rcnn_r50_caffe_fpn_1x_coco-cb486e66.pth) |
+|  Faster R-CNN | Cascade RPN | R-50-FPN |  caffe  |    1x    |    -     |          -          |        -       |  40.4  | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/cascade_rpn/crpn_faster_rcnn_r50_caffe_fpn_1x_coco.py) |[model](https://download.openmmlab.com/mmdetection/v2.0/cascade_rpn/crpn_faster_rcnn_r50_caffe_fpn_1x_coco/crpn_faster_rcnn_r50_caffe_fpn_1x_coco-c8283cca.pth) |
diff --git a/configs/centernet/metafile.yml b/configs/centernet/metafile.yml
index cbccff1b..e86e57b5 100644
--- a/configs/centernet/metafile.yml
+++ b/configs/centernet/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x TITANXP GPUs
       Architecture:
         - ResNet
-    Paper: https://arxiv.org/abs/1904.07850
+    Paper:
+      URL: https://arxiv.org/abs/1904.07850
+      Title: 'Objects as Points'
     README: configs/centernet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.13.0/mmdet/models/detectors/centernet.py#L10
+      Version: v2.13.0
 
 Models:
   - Name: centernet_resnet18_dcnv2_140e_coco
diff --git a/configs/centripetalnet/metafile.yml b/configs/centripetalnet/metafile.yml
index be350396..61aed3e5 100644
--- a/configs/centripetalnet/metafile.yml
+++ b/configs/centripetalnet/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Architecture:
         - Corner Pooling
         - Stacked Hourglass Network
-    Paper: https://arxiv.org/abs/2003.09119
+    Paper:
+      URL: https://arxiv.org/abs/2003.09119
+      Title: 'CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection'
     README: configs/centripetalnet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.5.0/mmdet/models/detectors/cornernet.py#L9
+      Version: v2.5.0
 
 Models:
   - Name: centripetalnet_hourglass104_mstest_16x6_210e_coco
diff --git a/configs/common/lsj_100e_coco_instance.py b/configs/common/lsj_100e_coco_instance.py
new file mode 100644
index 00000000..cacf23d7
--- /dev/null
+++ b/configs/common/lsj_100e_coco_instance.py
@@ -0,0 +1,90 @@
+_base_ = '../_base_/default_runtime.py'
+# dataset settings
+dataset_type = 'CocoDataset'
+data_root = 'data/coco/'
+img_norm_cfg = dict(
+    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+image_size = (1024, 1024)
+
+file_client_args = dict(backend='disk')
+# comment out the code below to use different file client
+# file_client_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection/',
+#         'data/': 's3://openmmlab/datasets/detection/'
+#     }))
+
+train_pipeline = [
+    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(
+        type='Resize',
+        img_scale=image_size,
+        ratio_range=(0.1, 2.0),
+        multiscale_mode='range',
+        keep_ratio=True),
+    dict(
+        type='RandomCrop',
+        crop_type='absolute_range',
+        crop_size=image_size,
+        recompute_bbox=True,
+        allow_negative_crop=True),
+    dict(type='FilterAnnotations', min_gt_bbox_wh=(1e-2, 1e-2)),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size=image_size),  # padding to image_size leads 0.5+ mAP
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(1333, 800),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='Pad', size_divisor=32),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+
+# Use RepeatDataset to speed up training
+data = dict(
+    samples_per_gpu=2,
+    workers_per_gpu=2,
+    train=dict(
+        type='RepeatDataset',
+        times=4,  # simply change this from 2 to 16 for 50e - 400e training.
+        dataset=dict(
+            type=dataset_type,
+            ann_file=data_root + 'annotations/instances_train2017.json',
+            img_prefix=data_root + 'train2017/',
+            pipeline=train_pipeline)),
+    val=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_val2017.json',
+        img_prefix=data_root + 'val2017/',
+        pipeline=test_pipeline),
+    test=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_val2017.json',
+        img_prefix=data_root + 'val2017/',
+        pipeline=test_pipeline))
+evaluation = dict(interval=5, metric=['bbox', 'segm'])
+
+# optimizer assumes bs=64
+optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.00004)
+optimizer_config = dict(grad_clip=None)
+
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=0.067,
+    step=[22, 24])
+runner = dict(type='EpochBasedRunner', max_epochs=25)
diff --git a/configs/cornernet/metafile.yml b/configs/cornernet/metafile.yml
index d2b23960..c2f6143a 100644
--- a/configs/cornernet/metafile.yml
+++ b/configs/cornernet/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Architecture:
         - Corner Pooling
         - Stacked Hourglass Network
-    Paper: https://arxiv.org/abs/1808.01244
+    Paper:
+      URL: https://arxiv.org/abs/1808.01244
+      Title: 'CornerNet: Detecting Objects as Paired Keypoints'
     README: configs/cornernet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.3.0/mmdet/models/detectors/cornernet.py#L9
+      Version: v2.3.0
 
 Models:
   - Name: cornernet_hourglass104_mstest_10x5_210e_coco
diff --git a/configs/dcn/metafile.yml b/configs/dcn/metafile.yml
index d5b53f42..4fcbce69 100644
--- a/configs/dcn/metafile.yml
+++ b/configs/dcn/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - Deformable Convolution
-    Paper: https://arxiv.org/abs/1811.11168
+    Paper:
+      URL: https://arxiv.org/abs/1811.11168
+      Title: 'Deformable ConvNets v2: More Deformable, Better Results'
     README: configs/dcn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/ops/dcn/deform_conv.py#L15
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco
diff --git a/configs/deepfashion/README.md b/configs/deepfashion/README.md
index 68e57e4f..e21cce30 100644
--- a/configs/deepfashion/README.md
+++ b/configs/deepfashion/README.md
@@ -53,4 +53,4 @@ or creating your own config file.
 
 |   Backbone  |  Model type  |       Dataset       |  bbox detection Average Precision  | segmentation Average Precision |  Config |      Download (Google)      |
 | :---------: | :----------: | :-----------------: | :--------------------------------: | :----------------------------: | :---------:| :-------------------------: |
-|   ResNet50  |   Mask RCNN  | DeepFashion-In-shop |                0.599               |              0.584             |[config](https://github.com/open-mmlab/mmdetection/blob/master/configs/deepfashion/mask_rcnn_r50_fpn_15e_deepfashion.py)|  [model](https://drive.google.com/open?id=1q6zF7J6Gb-FFgM87oIORIt6uBozaXp5r) &#124; [log](https://drive.google.com/file/d/1qTK4Dr4FFLa9fkdI6UVko408gkrfTRLP/view?usp=sharing)   |
+|   ResNet50  |   Mask RCNN  | DeepFashion-In-shop |                0.599               |              0.584             |[config](https://github.com/open-mmlab/mmdetection/blob/master/configs/deepfashion/mask_rcnn_r50_fpn_15e_deepfashion.py)|  [model](https://download.openmmlab.com/mmdetection/v2.0/deepfashion/mask_rcnn_r50_fpn_15e_deepfashion/mask_rcnn_r50_fpn_15e_deepfashion_20200329_192752.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/deepfashion/mask_rcnn_r50_fpn_15e_deepfashion/20200329_192752.log.json)   |
diff --git a/configs/deformable_detr/metafile.yml b/configs/deformable_detr/metafile.yml
index a63ffbeb..873292db 100644
--- a/configs/deformable_detr/metafile.yml
+++ b/configs/deformable_detr/metafile.yml
@@ -10,8 +10,13 @@ Collections:
       Architecture:
         - ResNet
         - Transformer
-    Paper: https://openreview.net/forum?id=gZ9hCDWe6ke
+    Paper:
+      URL: https://openreview.net/forum?id=gZ9hCDWe6ke
+      Title: 'Deformable DETR: Deformable Transformers for End-to-End Object Detection'
     README: configs/deformable_detr/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.12.0/mmdet/models/detectors/deformable_detr.py#L6
+      Version: v2.12.0
 
 Models:
   - Name: deformable_detr_r50_16x2_50e_coco
diff --git a/configs/detectors/metafile.yml b/configs/detectors/metafile.yml
index bf503375..4bed5694 100644
--- a/configs/detectors/metafile.yml
+++ b/configs/detectors/metafile.yml
@@ -14,8 +14,13 @@ Collections:
         - ResNet
         - RoIAlign
         - SAC
-    Paper: https://arxiv.org/abs/2006.02334
+    Paper:
+      URL: https://arxiv.org/abs/2006.02334
+      Title: 'DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution'
     README: configs/detectors/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.2.0/mmdet/models/backbones/detectors_resnet.py#L205
+      Version: v2.2.0
 
 Models:
   - Name: cascade_rcnn_r50_rfp_1x_coco
diff --git a/configs/detr/metafile.yml b/configs/detr/metafile.yml
index 4c6a86ab..45622cf9 100644
--- a/configs/detr/metafile.yml
+++ b/configs/detr/metafile.yml
@@ -10,8 +10,13 @@ Collections:
       Architecture:
         - ResNet
         - Transformer
-    Paper: https://arxiv.org/abs/2005.12872
+    Paper:
+      URL: https://arxiv.org/abs/2005.12872
+      Title: 'End-to-End Object Detection with Transformers'
     README: configs/detr/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.7.0/mmdet/models/detectors/detr.py#L7
+      Version: v2.7.0
 
 Models:
   - Name: detr_r50_8x2_150e_coco
diff --git a/configs/double_heads/metafile.yml b/configs/double_heads/metafile.yml
index 3a2a2fdb..6fe9b7af 100644
--- a/configs/double_heads/metafile.yml
+++ b/configs/double_heads/metafile.yml
@@ -11,8 +11,13 @@ Collections:
         - RPN
         - ResNet
         - RoIAlign
-    Paper: https://arxiv.org/pdf/1904.06493
+    Paper:
+      URL: https://arxiv.org/pdf/1904.06493
+      Title: 'Rethinking Classification and Localization for Object Detection'
     README: configs/double_heads/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/roi_heads/double_roi_head.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: dh_faster_rcnn_r50_fpn_1x_coco
diff --git a/configs/dynamic_rcnn/metafile.yml b/configs/dynamic_rcnn/metafile.yml
index 3297255e..fec43db4 100644
--- a/configs/dynamic_rcnn/metafile.yml
+++ b/configs/dynamic_rcnn/metafile.yml
@@ -12,8 +12,13 @@ Collections:
         - RPN
         - ResNet
         - RoIAlign
-    Paper: https://arxiv.org/pdf/2004.06002
+    Paper:
+      URL: https://arxiv.org/pdf/2004.06002
+      Title: 'Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training'
     README: configs/dynamic_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.2.0/mmdet/models/roi_heads/dynamic_roi_head.py#L11
+      Version: v2.2.0
 
 Models:
   - Name: dynamic_rcnn_r50_fpn_1x_coco
diff --git a/configs/empirical_attention/metafile.yml b/configs/empirical_attention/metafile.yml
index 853b7865..923bcb20 100644
--- a/configs/empirical_attention/metafile.yml
+++ b/configs/empirical_attention/metafile.yml
@@ -13,8 +13,13 @@ Collections:
         - ResNet
         - RoIAlign
         - Spatial Attention
-    Paper: https://arxiv.org/pdf/1904.05873
+    Paper:
+      URL: https://arxiv.org/pdf/1904.05873
+      Title: 'An Empirical Study of Spatial Attention Mechanisms in Deep Networks'
     README: configs/empirical_attention/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/ops/generalized_attention.py#L10
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_r50_fpn_attention_1111_1x_coco
diff --git a/configs/faster_rcnn/faster_rcnn_r50_fpn_ciou_1x_coco.py b/configs/faster_rcnn/faster_rcnn_r50_fpn_ciou_1x_coco.py
new file mode 100644
index 00000000..886d5668
--- /dev/null
+++ b/configs/faster_rcnn/faster_rcnn_r50_fpn_ciou_1x_coco.py
@@ -0,0 +1,6 @@
+_base_ = './faster_rcnn_r50_fpn_1x_coco.py'
+model = dict(
+    roi_head=dict(
+        bbox_head=dict(
+            reg_decoded_bbox=True,
+            loss_bbox=dict(type='CIoULoss', loss_weight=12.0))))
diff --git a/configs/faster_rcnn/metafile.yml b/configs/faster_rcnn/metafile.yml
index c314df89..97faf536 100644
--- a/configs/faster_rcnn/metafile.yml
+++ b/configs/faster_rcnn/metafile.yml
@@ -11,8 +11,13 @@ Collections:
         - RPN
         - ResNet
         - RoIPool
-    Paper: https://arxiv.org/abs/1506.01497
+    Paper:
+      URL: https://arxiv.org/abs/1506.01497
+      Title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
     README: configs/faster_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/faster_rcnn.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_r50_caffe_dc5_1x_coco
diff --git a/configs/fcos/README.md b/configs/fcos/README.md
index ae5470af..dd20d573 100644
--- a/configs/fcos/README.md
+++ b/configs/fcos/README.md
@@ -17,16 +17,16 @@
 
 | Backbone  | Style   | GN      | MS train | Tricks  | DCN     | Lr schd | Mem (GB) | Inf time (fps) | box AP | Config | Download |
 |:---------:|:-------:|:-------:|:--------:|:-------:|:-------:|:-------:|:--------:|:--------------:|:------:|:------:|:--------:|
-| R-50      | caffe   | Y       | N        | N       | N       | 1x      | 3.6      | 22.7           | 36.6   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/fcos_r50_caffe_fpn_gn-head_1x_coco-821213aa.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/20201227_180009.log.json) |
-| R-50      | caffe   | Y       | N        | Y       | N       | 1x      | 3.7      | -              | 38.7   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco-0a0d75a8.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco/20210105_135818.log.json)|
-| R-50      | caffe   | Y       | N        | Y       | Y       | 1x      | 3.8      | -              | 42.3   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco-ae4d8b3d.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco/20210105_224556.log.json)|
-| R-101     | caffe   | Y       | N        | N       | N       | 1x      | 5.5      | 17.3           | 39.1   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco/fcos_r101_caffe_fpn_gn-head_1x_coco-0e37b982.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco/20210103_155046.log.json) |
+| R-50      | caffe   | Y       | N        | N       | N       | 1x      | 3.6      | 22.7           | 36.6   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/fcos_r50_caffe_fpn_gn-head_1x_coco-821213aa.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/20201227_180009.log.json) |
+| R-50      | caffe   | Y       | N        | Y       | N       | 1x      | 3.7      | -              | 38.7   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco-0a0d75a8.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco/20210105_135818.log.json)|
+| R-50      | caffe   | Y       | N        | Y       | Y       | 1x      | 3.8      | -              | 42.3   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco-ae4d8b3d.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco/20210105_224556.log.json)|
+| R-101     | caffe   | Y       | N        | N       | N       | 1x      | 5.5      | 17.3           | 39.1   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco/fcos_r101_caffe_fpn_gn-head_1x_coco-0e37b982.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco/20210103_155046.log.json) |
 
 | Backbone  | Style   | GN      | MS train | Lr schd | Mem (GB) | Inf time (fps) | box AP | Config | Download |
 |:---------:|:-------:|:-------:|:--------:|:-------:|:--------:|:--------------:|:------:|:------:|:--------:|
-| R-50      | caffe   | Y       | Y        | 2x      | 2.6      | 22.9           | 38.5   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco-d92ceeea.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco/20201227_161900.log.json) |
-| R-101     | caffe   | Y       | Y        | 2x      | 5.5      | 17.3           | 40.8   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco-511424d6.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco/20210103_155046.log.json) |
-| X-101     | pytorch | Y       | Y        | 2x      | 10.0     | 9.7            | 42.6   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco-ede514a8.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco/20210114_133041.log.json) |
+| R-50      | caffe   | Y       | Y        | 2x      | 2.6      | 22.9           | 38.5   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco-d92ceeea.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco/20201227_161900.log.json) |
+| R-101     | caffe   | Y       | Y        | 2x      | 5.5      | 17.3           | 40.8   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco-511424d6.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco/20210103_155046.log.json) |
+| X-101     | pytorch | Y       | Y        | 2x      | 10.0     | 9.7            | 42.6   | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco-ede514a8.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco/20210114_133041.log.json) |
 
 **Notes:**
 
diff --git a/configs/fcos/metafile.yml b/configs/fcos/metafile.yml
index 88db4d40..ae922eb9 100644
--- a/configs/fcos/metafile.yml
+++ b/configs/fcos/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - Group Normalization
         - ResNet
-    Paper: https://arxiv.org/abs/1904.01355
+    Paper:
+      URL: https://arxiv.org/abs/1904.01355
+      Title: 'FCOS: Fully Convolutional One-Stage Object Detection'
     README: configs/fcos/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/fcos.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: fcos_r50_caffe_fpn_gn-head_1x_coco
@@ -32,7 +37,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 36.6
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/fcos_r50_caffe_fpn_gn-head_1x_coco-821213aa.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/fcos_r50_caffe_fpn_gn-head_1x_coco-821213aa.pth
 
   - Name: fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco
     In Collection: FCOS
@@ -45,7 +50,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 38.7
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco-0a0d75a8.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_1x_coco-0a0d75a8.pth
 
   - Name: fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco
     In Collection: FCOS
@@ -58,7 +63,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 42.3
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco-ae4d8b3d.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco/fcos_center-normbbox-centeronreg-giou_r50_caffe_fpn_gn-head_dcn_1x_coco-ae4d8b3d.pth
 
   - Name: fcos_r101_caffe_fpn_gn-head_1x_coco
     In Collection: FCOS
@@ -78,7 +83,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 39.1
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco/fcos_r101_caffe_fpn_gn-head_1x_coco-0e37b982.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_1x_coco/fcos_r101_caffe_fpn_gn-head_1x_coco-0e37b982.pth
 
   - Name: fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco
     In Collection: FCOS
@@ -98,7 +103,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 38.5
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco-d92ceeea.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco-d92ceeea.pth
 
   - Name: fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco
     In Collection: FCOS
@@ -118,7 +123,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 40.8
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco-511424d6.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco-511424d6.pth
 
   - Name: fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco
     In Collection: FCOS
@@ -138,4 +143,4 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 42.6
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco-ede514a8.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_2x_coco-ede514a8.pth
diff --git a/configs/foveabox/metafile.yml b/configs/foveabox/metafile.yml
index f5f892fb..fe9a2834 100644
--- a/configs/foveabox/metafile.yml
+++ b/configs/foveabox/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Architecture:
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/1904.03797
+    Paper:
+      URL: https://arxiv.org/abs/1904.03797
+      Title: 'FoveaBox: Beyond Anchor-based Object Detector'
     README: configs/foveabox/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/fovea.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: fovea_r50_fpn_4x4_1x_coco
diff --git a/configs/fp16/metafile.yml b/configs/fp16/metafile.yml
index af577c73..70d66490 100644
--- a/configs/fp16/metafile.yml
+++ b/configs/fp16/metafile.yml
@@ -5,8 +5,13 @@ Collections:
       Training Techniques:
         - Mixed Precision Training
       Training Resources: 8x V100 GPUs
-    Paper: https://arxiv.org/abs/1710.03740
+    Paper:
+      URL: https://arxiv.org/abs/1710.03740
+      Title: 'Mixed Precision Training'
     README: configs/fp16/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/core/fp16/hooks.py#L11
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_r50_fpn_fp16_1x_coco
diff --git a/configs/fpg/metafile.yml b/configs/fpg/metafile.yml
index 31e80a71..885d8573 100644
--- a/configs/fpg/metafile.yml
+++ b/configs/fpg/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - Feature Pyramid Grids
-    Paper: https://arxiv.org/abs/2004.03580
+    Paper:
+      URL: https://arxiv.org/abs/2004.03580
+      Title: 'Feature Pyramid Grids'
     README: configs/fpg/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.10.0/mmdet/models/necks/fpg.py#L101
+      Version: v2.10.0
 
 Models:
   - Name: faster_rcnn_r50_fpg_crop640_50e_coco
diff --git a/configs/free_anchor/metafile.yml b/configs/free_anchor/metafile.yml
index 7e78cf4e..170fb5c0 100644
--- a/configs/free_anchor/metafile.yml
+++ b/configs/free_anchor/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Architecture:
         - FreeAnchor
         - ResNet
-    Paper: https://arxiv.org/abs/1909.02466
+    Paper:
+      URL: https://arxiv.org/abs/1909.02466
+      Title: 'FreeAnchor: Learning to Match Anchors for Visual Object Detection'
     README: configs/free_anchor/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/dense_heads/free_anchor_retina_head.py#L10
+      Version: v2.0.0
 
 Models:
   - Name: retinanet_free_anchor_r50_fpn_1x_coco
diff --git a/configs/fsaf/metafile.yml b/configs/fsaf/metafile.yml
index 012e891b..5434e9ad 100644
--- a/configs/fsaf/metafile.yml
+++ b/configs/fsaf/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - FSAF
         - ResNet
-    Paper: https://arxiv.org/abs/1903.00621
+    Paper:
+      URL: https://arxiv.org/abs/1903.00621
+      Title: 'Feature Selective Anchor-Free Module for Single-Shot Object Detection'
     README: configs/fsaf/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.1.0/mmdet/models/detectors/fsaf.py#L6
+      Version: v2.1.0
 
 Models:
   - Name: fsaf_r50_fpn_1x_coco
diff --git a/configs/gcnet/metafile.yml b/configs/gcnet/metafile.yml
index 5a15a9e5..1281122a 100644
--- a/configs/gcnet/metafile.yml
+++ b/configs/gcnet/metafile.yml
@@ -12,8 +12,13 @@ Collections:
         - RPN
         - ResNet
         - ResNeXt
-    Paper: https://arxiv.org/abs/1904.11492
+    Paper:
+      URL: https://arxiv.org/abs/1904.11492
+      Title: 'GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond'
     README: configs/gcnet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/ops/context_block.py#L13
+      Version: v2.0.0
 
 Models:
   - Name: mask_rcnn_r50_fpn_r16_gcb_c3-c5_1x_coco
diff --git a/configs/gfl/metafile.yml b/configs/gfl/metafile.yml
index d11eae00..8f049c6b 100644
--- a/configs/gfl/metafile.yml
+++ b/configs/gfl/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - Generalized Focal Loss
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/2006.04388
+    Paper:
+      URL: https://arxiv.org/abs/2006.04388
+      Title: 'Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection'
     README: configs/gfl/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.2.0/mmdet/models/detectors/gfl.py#L6
+      Version: v2.2.0
 
 Models:
   - Name: gfl_r50_fpn_1x_coco
diff --git a/configs/ghm/metafile.yml b/configs/ghm/metafile.yml
index 9c4c9343..b4f488c4 100644
--- a/configs/ghm/metafile.yml
+++ b/configs/ghm/metafile.yml
@@ -11,8 +11,13 @@ Collections:
         - GHM-R
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/1811.05181
+    Paper:
+      URL: https://arxiv.org/abs/1811.05181
+      Title: 'Gradient Harmonized Single-stage Detector'
     README: configs/ghm/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/losses/ghm_loss.py#L21
+      Version: v2.0.0
 
 Models:
   - Name: retinanet_ghm_r50_fpn_1x_coco
diff --git a/configs/gn+ws/metafile.yml b/configs/gn+ws/metafile.yml
index 6aa87829..bc89359c 100644
--- a/configs/gn+ws/metafile.yml
+++ b/configs/gn+ws/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Architecture:
         - Group Normalization
         - Weight Standardization
-    Paper: https://arxiv.org/abs/1903.10520
+    Paper:
+      URL: https://arxiv.org/abs/1903.10520
+      Title: 'Weight Standardization'
     README: configs/gn+ws/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/configs/gn%2Bws/mask_rcnn_r50_fpn_gn_ws-all_2x_coco.py
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_r50_fpn_gn_ws-all_1x_coco
diff --git a/configs/gn/metafile.yml b/configs/gn/metafile.yml
index 08209cfc..4a1ecae0 100644
--- a/configs/gn/metafile.yml
+++ b/configs/gn/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - Group Normalization
-    Paper: https://arxiv.org/abs/1803.08494
+    Paper:
+      URL: https://arxiv.org/abs/1803.08494
+      Title: 'Group Normalization'
     README: configs/gn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/configs/gn/mask_rcnn_r50_fpn_gn-all_2x_coco.py
+      Version: v2.0.0
 
 Models:
   - Name: mask_rcnn_r50_fpn_gn-all_2x_coco
diff --git a/configs/grid_rcnn/metafile.yml b/configs/grid_rcnn/metafile.yml
index c37fb0dc..d1aa8513 100644
--- a/configs/grid_rcnn/metafile.yml
+++ b/configs/grid_rcnn/metafile.yml
@@ -11,8 +11,13 @@ Collections:
         - Dilated Convolution
         - ResNet
         - RoIAlign
-    Paper: https://arxiv.org/abs/1906.05688
+    Paper:
+      URL: https://arxiv.org/abs/1906.05688
+      Title: 'Grid R-CNN'
     README: configs/grid_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/grid_rcnn.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: grid_rcnn_r50_fpn_gn-head_2x_coco
diff --git a/configs/groie/metafile.yml b/configs/groie/metafile.yml
index 5f9ce13f..269cb393 100644
--- a/configs/groie/metafile.yml
+++ b/configs/groie/metafile.yml
@@ -12,8 +12,13 @@ Collections:
         - RPN
         - ResNet
         - RoIAlign
-    Paper: https://arxiv.org/abs/2004.13665
+    Paper:
+      URL: https://arxiv.org/abs/2004.13665
+      Title: 'A novel Region of Interest Extraction Layer for Instance Segmentation'
     README: configs/groie/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.1.0/mmdet/models/roi_heads/roi_extractors/groie.py#L15
+      Version: v2.1.0
 
 Models:
   - Name: faster_rcnn_r50_fpn_groie_1x_coco
diff --git a/configs/guided_anchoring/metafile.yml b/configs/guided_anchoring/metafile.yml
index 63641040..3019d4a1 100644
--- a/configs/guided_anchoring/metafile.yml
+++ b/configs/guided_anchoring/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - Guided Anchoring
         - ResNet
-    Paper: https://arxiv.org/abs/1901.03278
+    Paper:
+      URL: https://arxiv.org/abs/1901.03278
+      Title: 'Region Proposal by Guided Anchoring'
     README: configs/guided_anchoring/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/dense_heads/ga_retina_head.py#L10
+      Version: v2.0.0
 
 Models:
   - Name: ga_rpn_r50_caffe_fpn_1x_coco
diff --git a/configs/hrnet/metafile.yml b/configs/hrnet/metafile.yml
index fce29bdb..37703aaa 100644
--- a/configs/hrnet/metafile.yml
+++ b/configs/hrnet/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - HRNet
-    Paper: https://arxiv.org/abs/1904.04514
+    Paper:
+      URL: https://arxiv.org/abs/1904.04514
+      Title: 'Deep High-Resolution Representation Learning for Visual Recognition'
     README: configs/hrnet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/backbones/hrnet.py#L195
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_hrnetv2p_w18_1x_coco
diff --git a/configs/htc/htc_r50_fpn_1x_coco.py b/configs/htc/htc_r50_fpn_1x_coco.py
index 929cf464..1e8e18a0 100644
--- a/configs/htc/htc_r50_fpn_1x_coco.py
+++ b/configs/htc/htc_r50_fpn_1x_coco.py
@@ -14,8 +14,8 @@ model = dict(
             in_channels=256,
             conv_out_channels=256,
             num_classes=183,
-            ignore_label=255,
-            loss_weight=0.2)))
+            loss_seg=dict(
+                type='CrossEntropyLoss', ignore_index=255, loss_weight=0.2))))
 data_root = 'data/coco/'
 img_norm_cfg = dict(
     mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
diff --git a/configs/htc/metafile.yml b/configs/htc/metafile.yml
index ad006a5e..acd038c7 100644
--- a/configs/htc/metafile.yml
+++ b/configs/htc/metafile.yml
@@ -13,8 +13,13 @@ Collections:
         - ResNet
         - ResNeXt
         - RoIAlign
-    Paper: https://arxiv.org/abs/1901.07518
+    Paper:
+      URL: https://arxiv.org/abs/1901.07518
+      Title: 'Hybrid Task Cascade for Instance Segmentation'
     README: configs/htc/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/htc.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: htc_r50_fpn_1x_coco
diff --git a/configs/instaboost/metafile.yml b/configs/instaboost/metafile.yml
index f85fdaaf..325283d3 100644
--- a/configs/instaboost/metafile.yml
+++ b/configs/instaboost/metafile.yml
@@ -7,8 +7,13 @@ Collections:
         - SGD with Momentum
         - Weight Decay
       Training Resources: 8x V100 GPUs
-    Paper: https://arxiv.org/abs/1908.07801
+    Paper:
+      URL: https://arxiv.org/abs/1908.07801
+      Title: 'Instaboost: Boosting instance segmentation via probability map guided copy-pasting'
     README: configs/instaboost/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/datasets/pipelines/instaboost.py#L7
+      Version: v2.0.0
 
 Models:
   - Name: mask_rcnn_r50_fpn_instaboost_4x_coco
diff --git a/configs/ld/metafile.yml b/configs/ld/metafile.yml
index dc05e5a7..cd833bf0 100644
--- a/configs/ld/metafile.yml
+++ b/configs/ld/metafile.yml
@@ -10,8 +10,13 @@ Collections:
       Architecture:
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/2102.12252
+    Paper:
+      URL: https://arxiv.org/abs/2102.12252
+      Title: 'Localization Distillation for Object Detection'
     README: configs/ld/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.11.0/mmdet/models/dense_heads/ld_head.py#L11
+      Version: v2.11.0
 
 Models:
   - Name: ld_r18_gflv1_r101_fpn_coco_1x
diff --git a/configs/libra_rcnn/README.md b/configs/libra_rcnn/README.md
index ef2f6fe2..8b78af40 100644
--- a/configs/libra_rcnn/README.md
+++ b/configs/libra_rcnn/README.md
@@ -6,6 +6,8 @@
 
 We provide config files to reproduce the results in the CVPR 2019 paper [Libra R-CNN](https://arxiv.org/pdf/1904.02701.pdf).
 
+The extended version of [Libra R-CNN](https://arxiv.org/pdf/2108.10175.pdf) is accpeted by IJCV.
+
 ```
 @inproceedings{pang2019libra,
   title={Libra R-CNN: Towards Balanced Learning for Object Detection},
@@ -13,6 +15,17 @@ We provide config files to reproduce the results in the CVPR 2019 paper [Libra R
   booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
   year={2019}
 }
+
+@article{pang2021towards,
+  title={Towards Balanced Learning for Instance Recognition},
+  author={Pang, Jiangmiao and Chen, Kai and Li, Qi and Xu, Zhihai and Feng, Huajun and Shi, Jianping and Ouyang, Wanli and Lin, Dahua},
+  journal={International Journal of Computer Vision},
+  volume={129},
+  number={5},
+  pages={1376--1393},
+  year={2021},
+  publisher={Springer}
+}
 ```
 
 ## Results and models
diff --git a/configs/libra_rcnn/metafile.yml b/configs/libra_rcnn/metafile.yml
index 574cac60..8c327959 100644
--- a/configs/libra_rcnn/metafile.yml
+++ b/configs/libra_rcnn/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - Balanced Feature Pyramid
-    Paper: https://arxiv.org/abs/1904.02701
+    Paper:
+      URL: https://arxiv.org/abs/1904.02701
+      Title: 'Libra R-CNN: Towards Balanced Learning for Object Detection'
     README: configs/libra_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/necks/bfp.py#L10
+      Version: v2.0.0
 
 Models:
   - Name: libra_faster_rcnn_r50_fpn_1x_coco
diff --git a/configs/lvis/README.md b/configs/lvis/README.md
index 157e8724..f613de82 100644
--- a/configs/lvis/README.md
+++ b/configs/lvis/README.md
@@ -15,14 +15,14 @@
 
 ## Common Setting
 
-* Please follow [install guide](../../docs/install.md#install-mmdetection) to install open-mmlab forked cocoapi first.
+* Please follow [install guide](../../docs/get_started.md#install-mmdetection) to install open-mmlab forked cocoapi first.
 * Run following scripts to install our forked lvis-api.
 
     ```shell
     pip install git+https://github.com/lvis-dataset/lvis-api.git
     ```
 
-* All experiments use oversample strategy [here](../../docs/tutorials/new_dataset.md#class-balanced-dataset) with oversample threshold `1e-3`.
+* All experiments use oversample strategy [here](../../docs/tutorials/customize_dataset.md#class-balanced-dataset) with oversample threshold `1e-3`.
 * The size of LVIS v0.5 is half of COCO, so schedule `2x` in LVIS is roughly the same iterations as `1x` in COCO.
 
 ## Results and models of LVIS v0.5
diff --git a/configs/mask_rcnn/metafile.yml b/configs/mask_rcnn/metafile.yml
index b8e72a67..747a99a5 100644
--- a/configs/mask_rcnn/metafile.yml
+++ b/configs/mask_rcnn/metafile.yml
@@ -14,8 +14,13 @@ Collections:
         - FPN
         - ResNet
         - RoIAlign
-    Paper: https://arxiv.org/abs/1703.06870v3
+    Paper:
+      URL: https://arxiv.org/abs/1703.06870v3
+      Title: 'Mask R-CNN'
     README: configs/mask_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/mask_rcnn.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: mask_rcnn_r50_caffe_fpn_1x_coco
diff --git a/configs/ms_rcnn/metafile.yml b/configs/ms_rcnn/metafile.yml
index 997e443e..a6c7dc59 100644
--- a/configs/ms_rcnn/metafile.yml
+++ b/configs/ms_rcnn/metafile.yml
@@ -11,8 +11,13 @@ Collections:
         - FPN
         - ResNet
         - RoIAlign
-    Paper: https://arxiv.org/abs/1903.00241
+    Paper:
+      URL: https://arxiv.org/abs/1903.00241
+      Title: 'Mask Scoring R-CNN'
     README: configs/ms_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/mask_scoring_rcnn.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: ms_rcnn_r50_caffe_fpn_1x_coco
diff --git a/configs/nas_fcos/metafile.yml b/configs/nas_fcos/metafile.yml
index 09811313..1ea28cfc 100644
--- a/configs/nas_fcos/metafile.yml
+++ b/configs/nas_fcos/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - NAS-FCOS
         - ResNet
-    Paper: https://arxiv.org/abs/1906.04423
+    Paper:
+      URL: https://arxiv.org/abs/1906.04423
+      Title: 'NAS-FCOS: Fast Neural Architecture Search for Object Detection'
     README: configs/nas_fcos/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.1.0/mmdet/models/detectors/nasfcos.py#L6
+      Version: v2.1.0
 
 Models:
   - Name: nas_fcos_nashead_r50_caffe_fpn_gn-head_4x4_1x_coco
diff --git a/configs/nas_fpn/metafile.yml b/configs/nas_fpn/metafile.yml
index a2ef30cb..ab8d6497 100644
--- a/configs/nas_fpn/metafile.yml
+++ b/configs/nas_fpn/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Architecture:
         - NAS-FPN
         - ResNet
-    Paper: https://arxiv.org/abs/1904.07392
+    Paper:
+      URL: https://arxiv.org/abs/1904.07392
+      Title: 'NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection'
     README: configs/nas_fpn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/necks/nas_fpn.py#L67
+      Version: v2.0.0
 
 Models:
   - Name: retinanet_r50_fpn_crop640_50e_coco
diff --git a/configs/paa/metafile.yml b/configs/paa/metafile.yml
index 8b6e1c6e..e08b663a 100644
--- a/configs/paa/metafile.yml
+++ b/configs/paa/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - Probabilistic Anchor Assignment
         - ResNet
-    Paper: https://arxiv.org/abs/2007.08103
+    Paper:
+      URL: https://arxiv.org/abs/2007.08103
+      Title: 'Probabilistic Anchor Assignment with IoU Prediction for Object Detection'
     README: configs/paa/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.4.0/mmdet/models/detectors/paa.py#L6
+      Version: v2.4.0
 
 Models:
   - Name: paa_r50_fpn_1x_coco
diff --git a/configs/pafpn/metafile.yml b/configs/pafpn/metafile.yml
index 14a96c58..f9cf97c8 100644
--- a/configs/pafpn/metafile.yml
+++ b/configs/pafpn/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - PAFPN
-    Paper: https://arxiv.org/abs/1803.01534
+    Paper:
+      URL: https://arxiv.org/abs/1803.01534
+      Title: 'Path Aggregation Network for Instance Segmentation'
     README: configs/pafpn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/necks/pafpn.py#L11
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_r50_pafpn_1x_coco
diff --git a/configs/panoptic_fpn/README.md b/configs/panoptic_fpn/README.md
new file mode 100644
index 00000000..d2091838
--- /dev/null
+++ b/configs/panoptic_fpn/README.md
@@ -0,0 +1,50 @@
+# Panoptic feature pyramid networks
+## Introduction
+
+<!-- [ALGORITHM] -->
+The base method for panoptic segmentation task.
+
+```
+@inproceedings{kirillov2018panopticfpn,
+  author = {
+    Alexander Kirillov,
+    Ross Girshick,
+    Kaiming He,
+    Piotr Dollar,
+  },
+  title = {Panoptic Feature Pyramid Networks},
+  booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
+  year = {2019}
+}
+```
+
+## Dataset
+
+PanopticFPN requires COCO and [COCO-panoptic](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip) dataset for training and evaluation. You need to download and extract it in the COCO dataset path.
+The directory should be like this.
+
+```none
+mmdetection
+├── mmdet
+├── tools
+├── configs
+├── data
+│   ├── coco
+│   │   ├── annotations
+│   │   │   ├── panoptic_train2017.json
+│   │   │   ├── panoptic_train2017
+│   │   │   ├── panoptic_val2017.json
+│   │   │   ├── panoptic_val2017
+│   │   ├── train2017
+│   │   ├── val2017
+│   │   ├── test2017
+```
+
+## Results and Models
+
+| Backbone      | style      | Lr schd | Mem (GB) | Inf time (fps) | PQ   |  SQ  |  RQ  | PQ_th | SQ_th | RQ_th | PQ_st | SQ_st | RQ_st | Config | Download |
+|:-------------:|:----------:|:-------:|:--------:|:--------------:|:----:|:----:|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:------:|:--------:|
+| R-50-FPN      | pytorch    | 1x      |   4.7    |                | 40.2 | 77.8 | 49.3 | 47.8  | 80.9  | 57.5  | 28.9  | 73.1  | 37.0  | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco/panoptic_fpn_r50_fpn_1x_coco_20210821_101153-9668fd13.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco/panoptic_fpn_r50_fpn_1x_coco_20210821_101153.log.json) |
+| R-50-FPN      | pytorch    | 3x      |   -      |     -          | 42.5 | 78.1 | 51.7 | 50.3  | 81.5  | 60.3  | 30.7  | 73.0  | 38.8  | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco/panoptic_fpn_r50_fpn_mstrain_3x_coco_20210824_171155-5650f98b.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco/panoptic_fpn_r50_fpn_mstrain_3x_coco_20210824_171155.log.json) |
+| R-101-FPN     | pytorch    | 1x      |   6.7    |                | 42.2 | 78.3 | 51.4 | 50.1  | 81.4  | 59.9  | 30.3  | 73.6  | 38.5  | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco/panoptic_fpn_r101_fpn_1x_coco_20210820_193950-ab9157a2.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco/panoptic_fpn_r101_fpn_1x_coco_20210820_193950.log.json) |
+| R-101-FPN     | pytorch    | 3x      |   -      |     -          | 44.1 | 78.9 | 53.6 | 52.1  | 81.7  | 62.3  | 32.0  | 74.6  | 40.3  | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco/panoptic_fpn_r101_fpn_mstrain_3x_coco_20210823_114712-9c99acc4.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco/panoptic_fpn_r101_fpn_mstrain_3x_coco_20210823_114712.log.json) |
diff --git a/configs/panoptic_fpn/metafile.yml b/configs/panoptic_fpn/metafile.yml
new file mode 100644
index 00000000..8c9d39dc
--- /dev/null
+++ b/configs/panoptic_fpn/metafile.yml
@@ -0,0 +1,70 @@
+Collections:
+  - Name: PanopticFPN
+    Metadata:
+      Training Data: COCO
+      Training Techniques:
+        - SGD with Momentum
+        - Weight Decay
+      Training Resources: 8x V100 GPUs
+      Architecture:
+        - PanopticFPN
+    Paper:
+      URL: https://arxiv.org/pdf/1901.02446
+      Title: 'Panoptic feature pyramid networks'
+    README: configs/panoptic_fpn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.16.0/mmdet/models/detectors/panoptic_fpn.py#L7
+      Version: v2.16.0
+
+Models:
+  - Name: panoptic_fpn_r50_fpn_1x_coco
+    In Collection: PanopticFPN
+    Config: configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 4.6
+      Epochs: 12
+    Results:
+    - Task: Panoptic Segmentation
+      Dataset: COCO
+      Metrics:
+        PQ: 40.2
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco/panoptic_fpn_r50_fpn_1x_coco_20210821_101153-9668fd13.pth
+
+  - Name: panoptic_fpn_r50_fpn_mstrain_3x_coco
+    In Collection: PanopticFPN
+    Config: configs/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco.py
+    Metadata:
+      Training Memory (GB): 4.6
+      Epochs: 36
+    Results:
+    - Task: Panoptic Segmentation
+      Dataset: COCO
+      Metrics:
+        PQ: 42.5
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco/panoptic_fpn_r50_fpn_mstrain_3x_coco_20210824_171155-5650f98b.pth
+
+  - Name: panoptic_fpn_r101_fpn_1x_coco
+    In Collection: PanopticFPN
+    Config: configs/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 6.5
+      Epochs: 12
+    Results:
+    - Task: Panoptic Segmentation
+      Dataset: COCO
+      Metrics:
+        PQ: 42.2
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco/panoptic_fpn_r101_fpn_1x_coco_20210820_193950-ab9157a2.pth
+
+  - Name: panoptic_fpn_r101_fpn_mstrain_3x_coco
+    In Collection: PanopticFPN
+    Config: configs/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco.py
+    Metadata:
+      Training Memory (GB): 6.5
+      Epochs: 36
+    Results:
+    - Task: Panoptic Segmentation
+      Dataset: COCO
+      Metrics:
+        PQ: 44.1
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco/panoptic_fpn_r101_fpn_mstrain_3x_coco_20210823_114712-9c99acc4.pth
diff --git a/configs/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco.py b/configs/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco.py
new file mode 100644
index 00000000..78b80798
--- /dev/null
+++ b/configs/panoptic_fpn/panoptic_fpn_r101_fpn_1x_coco.py
@@ -0,0 +1,6 @@
+_base_ = './panoptic_fpn_r50_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        depth=101,
+        init_cfg=dict(type='Pretrained',
+                      checkpoint='torchvision://resnet101')))
diff --git a/configs/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco.py b/configs/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco.py
new file mode 100644
index 00000000..057e4811
--- /dev/null
+++ b/configs/panoptic_fpn/panoptic_fpn_r101_fpn_mstrain_3x_coco.py
@@ -0,0 +1,6 @@
+_base_ = './panoptic_fpn_r50_fpn_mstrain_3x_coco.py'
+model = dict(
+    backbone=dict(
+        depth=101,
+        init_cfg=dict(type='Pretrained',
+                      checkpoint='torchvision://resnet101')))
diff --git a/configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py b/configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py
new file mode 100644
index 00000000..600fe7f0
--- /dev/null
+++ b/configs/panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py
@@ -0,0 +1,32 @@
+_base_ = [
+    '../_base_/models/mask_rcnn_r50_fpn.py',
+    '../_base_/datasets/coco_panoptic.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+model = dict(
+    type='PanopticFPN',
+    semantic_head=dict(
+        type='PanopticFPNHead',
+        num_classes=54,
+        in_channels=256,
+        inner_channels=128,
+        start_level=0,
+        end_level=4,
+        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True),
+        conv_cfg=None,
+        loss_seg=dict(
+            type='CrossEntropyLoss', ignore_index=255, loss_weight=0.5)),
+    panoptic_fusion_head=dict(
+        type='HeuristicFusionHead',
+        num_things_classes=80,
+        num_stuff_classes=53),
+    test_cfg=dict(
+        panoptic=dict(
+            score_thr=0.6,
+            max_per_img=100,
+            mask_thr_binary=0.5,
+            mask_overlap=0.5,
+            nms=dict(type='nms', iou_threshold=0.5, class_agnostic=True),
+            stuff_area_limit=4096)))
+
+custom_hooks = []
diff --git a/configs/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco.py b/configs/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco.py
new file mode 100644
index 00000000..b5109353
--- /dev/null
+++ b/configs/panoptic_fpn/panoptic_fpn_r50_fpn_mstrain_3x_coco.py
@@ -0,0 +1,61 @@
+_base_ = './panoptic_fpn_r50_fpn_1x_coco.py'
+
+# dataset settings
+dataset_type = 'CocoPanopticDataset'
+data_root = 'data/coco/'
+img_norm_cfg = dict(
+    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+
+# In mstrain 3x config, img_scale=[(1333, 640), (1333, 800)],
+# multiscale_mode='range'
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='LoadPanopticAnnotations',
+        with_bbox=True,
+        with_mask=True,
+        with_seg=True),
+    dict(
+        type='Resize',
+        img_scale=[(1333, 640), (1333, 800)],
+        multiscale_mode='range',
+        keep_ratio=True),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size_divisor=32),
+    dict(type='SegRescale', scale_factor=1 / 4),
+    dict(type='DefaultFormatBundle'),
+    dict(
+        type='Collect',
+        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks', 'gt_semantic_seg']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(1333, 800),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='Pad', size_divisor=32),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+
+# Use RepeatDataset to speed up training
+data = dict(
+    train=dict(
+        _delete_=True,
+        type='RepeatDataset',
+        times=3,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=data_root + 'annotations/panoptic_train2017.json',
+            img_prefix=data_root + 'train2017/',
+            seg_prefix=data_root + 'annotations/panoptic_train2017/',
+            pipeline=train_pipeline)),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
diff --git a/configs/pisa/metafile.yml b/configs/pisa/metafile.yml
index 7f017cfb..cd43afb0 100644
--- a/configs/pisa/metafile.yml
+++ b/configs/pisa/metafile.yml
@@ -12,8 +12,13 @@ Collections:
         - RPN
         - ResNet
         - RoIPool
-    Paper: https://arxiv.org/abs/1904.04821
+    Paper:
+      URL: https://arxiv.org/abs/1904.04821
+      Title: 'Prime Sample Attention in Object Detection'
     README: configs/pisa/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.1.0/mmdet/models/roi_heads/pisa_roi_head.py#L8
+      Version: v2.1.0
 
 Models:
   - Name: pisa_faster_rcnn_r50_fpn_1x_coco
diff --git a/configs/point_rend/metafile.yml b/configs/point_rend/metafile.yml
index 015d9f9a..82aea05b 100644
--- a/configs/point_rend/metafile.yml
+++ b/configs/point_rend/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - PointRend
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/1912.08193
+    Paper:
+      URL: https://arxiv.org/abs/1912.08193
+      Title: 'PointRend: Image Segmentation as Rendering'
     README: configs/point_rend/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.2.0/mmdet/models/detectors/point_rend.py#L6
+      Version: v2.2.0
 
 Models:
   - Name: point_rend_r50_caffe_fpn_mstrain_1x_coco
diff --git a/configs/pvt/README.md b/configs/pvt/README.md
new file mode 100644
index 00000000..99ace876
--- /dev/null
+++ b/configs/pvt/README.md
@@ -0,0 +1,43 @@
+# Pyramid vision transformer: A versatile backbone for dense prediction without convolutions
+
+## Introduction
+
+<!-- [ALGORITHM] -->
+
+```latex
+@article{wang2021pyramid,
+  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
+  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
+  journal={arXiv preprint arXiv:2102.12122},
+  year={2021}
+}
+```
+
+```latex
+@article{wang2021pvtv2,
+  title={PVTv2: Improved Baselines with Pyramid Vision Transformer},
+  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
+  journal={arXiv preprint arXiv:2106.13797},
+  year={2021}
+}
+```
+## Results and Models
+
+### RetinaNet (PVTv1)
+
+| Backbone    | Lr schd | Mem (GB) | box AP | Config | Download |
+|:-----------:|:-------:|:--------:|:------:|:------:|:--------:|
+| PVT-Tiny    | 12e     |8.5       |36.6    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_t_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-t_fpn_1x_coco/retinanet_pvt-t_fpn_1x_coco_20210831_103110-17b566bd.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-t_fpn_1x_coco/retinanet_pvt-t_fpn_1x_coco_20210831_103110.log.json) |
+| PVT-Small   | 12e     |14.5      |40.4    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_s_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-s_fpn_1x_coco/retinanet_pvt-s_fpn_1x_coco_20210906_142921-b6c94a5b.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-s_fpn_1x_coco/retinanet_pvt-s_fpn_1x_coco_20210906_142921.log.json) |
+| PVT-Medium  | 12e     |20.9      |41.7    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_m_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-m_fpn_1x_coco/retinanet_pvt-m_fpn_1x_coco_20210831_103243-55effa1b.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-m_fpn_1x_coco/retinanet_pvt-m_fpn_1x_coco_20210831_103243.log.json) |
+
+### RetinaNet (PVTv2)
+
+| Backbone    | Lr schd | Mem (GB) | box AP | Config | Download |
+|:-----------:|:-------:|:--------:|:------:|:------:|:--------:|
+| PVTv2-B0    | 12e     |7.4       |37.1    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_v2_b0_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b0_fpn_1x_coco/retinanet_pvtv2-b0_fpn_1x_coco_20210831_103157-13e9aabe.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b0_fpn_1x_coco/retinanet_pvtv2-b0_fpn_1x_coco_20210831_103157.log.json) |
+| PVTv2-B1    | 12e     |9.5       |41.2    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_v2_b1_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b1_fpn_1x_coco/retinanet_pvtv2-b1_fpn_1x_coco_20210831_103318-7e169a7d.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b1_fpn_1x_coco/retinanet_pvtv2-b1_fpn_1x_coco_20210831_103318.log.json) |
+| PVTv2-B2    | 12e     |16.2      |44.6    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_v2_b2_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b2_fpn_1x_coco/retinanet_pvtv2-b2_fpn_1x_coco_20210901_174843-529f0b9a.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b2_fpn_1x_coco/retinanet_pvtv2-b2_fpn_1x_coco_20210901_174843.log.json) |
+| PVTv2-B3    | 12e     |23.0      |46.0    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_v2_b3_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b3_fpn_1x_coco/retinanet_pvtv2-b3_fpn_1x_coco_20210903_151512-8357deff.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b3_fpn_1x_coco/retinanet_pvtv2-b3_fpn_1x_coco_20210903_151512.log.json) |
+| PVTv2-B4    | 12e     |17.0      |46.3    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_v2_b4_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b4_fpn_1x_coco/retinanet_pvtv2-b4_fpn_1x_coco_20210901_170151-83795c86.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b4_fpn_1x_coco/retinanet_pvtv2-b4_fpn_1x_coco_20210901_170151.log.json) |
+| PVTv2-B5    | 12e     |18.7      |46.1    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/pvt/retinanet_pvt_v2_b5_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b5_fpn_1x_coco/retinanet_pvtv2-b5_fpn_1x_coco_20210902_201800-3420eb57.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b5_fpn_1x_coco/retinanet_pvtv2-b5_fpn_1x_coco_20210902_201800.log.json) |
diff --git a/configs/pvt/metafile.yml b/configs/pvt/metafile.yml
new file mode 100644
index 00000000..48a0e2c2
--- /dev/null
+++ b/configs/pvt/metafile.yml
@@ -0,0 +1,136 @@
+Collections:
+  - Name: PVT
+    Metadata:
+      Training Data: COCO
+      Training Techniques:
+        - SGD with Momentum
+        - Weight Decay
+      Training Resources: 8x NVIDIA V100 GPUs
+      Architecture:
+        - RetinaNet
+        - PyramidVisionTransformer
+        - FPN
+    Paper: https://arxiv.org/abs/2102.12122
+    README: configs/pvt/README.md
+  - Name: PVT-v2
+    Metadata:
+      Training Data: COCO
+      Training Techniques:
+        - SGD with Momentum
+        - Weight Decay
+      Training Resources: 8x NVIDIA V100 GPUs
+      Architecture:
+        - RetinaNet
+        - PyramidVisionTransformerV2
+        - FPN
+    Paper: https://arxiv.org/abs/2106.13797
+    README: configs/pvt/README.md
+Models:
+  - Name: retinanet_pvt-t_fpn_1x_coco
+    In Collection: PVT
+    Config: configs/pvt/retinanet_pvt-t_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 8.5
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 36.6
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-t_fpn_1x_coco/retinanet_pvt-t_fpn_1x_coco_20210831_103110-17b566bd.pth
+  - Name: retinanet_pvt-s_fpn_1x_coco
+    In Collection: PVT
+    Config: configs/pvt/retinanet_pvt-s_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 14.5
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 40.4
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-s_fpn_1x_coco/retinanet_pvt-s_fpn_1x_coco_20210906_142921-b6c94a5b.pth
+  - Name: retinanet_pvt-m_fpn_1x_coco
+    In Collection: PVT
+    Config: configs/pvt/retinanet_pvt-m_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 20.9
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 41.7
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvt-m_fpn_1x_coco/retinanet_pvt-m_fpn_1x_coco_20210831_103243-55effa1b.pth
+  - Name: retinanet_pvtv2-b0_fpn_1x_coco
+    In Collection: PVT-v2
+    Config: configs/pvt/retinanet_pvtv2-b0_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 7.4
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 37.1
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b0_fpn_1x_coco/retinanet_pvtv2-b0_fpn_1x_coco_20210831_103157-13e9aabe.pth
+  - Name: retinanet_pvtv2-b1_fpn_1x_coco
+    In Collection: PVT-v2
+    Config: configs/pvt/retinanet_pvtv2-b1_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 9.5
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 41.2
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b1_fpn_1x_coco/retinanet_pvtv2-b1_fpn_1x_coco_20210831_103318-7e169a7d.pth
+  - Name: retinanet_pvtv2-b2_fpn_1x_coco
+    In Collection: PVT-v2
+    Config: configs/pvt/retinanet_pvtv2-b2_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 16.2
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 44.6
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b2_fpn_1x_coco/retinanet_pvtv2-b2_fpn_1x_coco_20210901_174843-529f0b9a.pth
+  - Name: retinanet_pvtv2-b3_fpn_1x_coco
+    In Collection: PVT-v2
+    Config: configs/pvt/retinanet_pvtv2-b3_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 23.0
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 46.0
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b3_fpn_1x_coco/retinanet_pvtv2-b3_fpn_1x_coco_20210903_151512-8357deff.pth
+  - Name: retinanet_pvtv2-b4_fpn_1x_coco
+    In Collection: PVT-v2
+    Config: configs/pvt/retinanet_pvtv2-b4_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 17.0
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 46.3
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b4_fpn_1x_coco/retinanet_pvtv2-b4_fpn_1x_coco_20210901_170151-83795c86.pth
+  - Name: retinanet_pvtv2-b5_fpn_1x_coco
+    In Collection: PVT-v2
+    Config: configs/pvt/retinanet_pvtv2-b5_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 18.7
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 46.1
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/pvt/retinanet_pvtv2-b5_fpn_1x_coco/retinanet_pvtv2-b5_fpn_1x_coco_20210902_201800-3420eb57.pth
diff --git a/configs/pvt/retinanet_pvt-l_fpn_1x_coco.py b/configs/pvt/retinanet_pvt-l_fpn_1x_coco.py
new file mode 100644
index 00000000..e299f2a0
--- /dev/null
+++ b/configs/pvt/retinanet_pvt-l_fpn_1x_coco.py
@@ -0,0 +1,7 @@
+_base_ = 'retinanet_pvt-t_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        num_layers=[3, 8, 27, 3],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_large.pth')))
+fp16 = dict(loss_scale=dict(init_scale=512))
diff --git a/configs/pvt/retinanet_pvt-m_fpn_1x_coco.py b/configs/pvt/retinanet_pvt-m_fpn_1x_coco.py
new file mode 100644
index 00000000..b888f788
--- /dev/null
+++ b/configs/pvt/retinanet_pvt-m_fpn_1x_coco.py
@@ -0,0 +1,6 @@
+_base_ = 'retinanet_pvt-t_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        num_layers=[3, 4, 18, 3],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_medium.pth')))
diff --git a/configs/pvt/retinanet_pvt-s_fpn_1x_coco.py b/configs/pvt/retinanet_pvt-s_fpn_1x_coco.py
new file mode 100644
index 00000000..46603488
--- /dev/null
+++ b/configs/pvt/retinanet_pvt-s_fpn_1x_coco.py
@@ -0,0 +1,6 @@
+_base_ = 'retinanet_pvt-t_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        num_layers=[3, 4, 6, 3],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_small.pth')))
diff --git a/configs/pvt/retinanet_pvt-t_fpn_1x_coco.py b/configs/pvt/retinanet_pvt-t_fpn_1x_coco.py
new file mode 100644
index 00000000..a6cff7d0
--- /dev/null
+++ b/configs/pvt/retinanet_pvt-t_fpn_1x_coco.py
@@ -0,0 +1,16 @@
+_base_ = [
+    '../_base_/models/retinanet_r50_fpn.py',
+    '../_base_/datasets/coco_detection.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+model = dict(
+    type='RetinaNet',
+    backbone=dict(
+        _delete_=True,
+        type='PyramidVisionTransformer',
+        num_layers=[2, 2, 2, 2],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_tiny.pth')),
+    neck=dict(in_channels=[64, 128, 320, 512]))
+# optimizer
+optimizer = dict(_delete_=True, type='AdamW', lr=0.0001, weight_decay=0.0001)
diff --git a/configs/pvt/retinanet_pvtv2-b0_fpn_1x_coco.py b/configs/pvt/retinanet_pvtv2-b0_fpn_1x_coco.py
new file mode 100644
index 00000000..cbe2295d
--- /dev/null
+++ b/configs/pvt/retinanet_pvtv2-b0_fpn_1x_coco.py
@@ -0,0 +1,17 @@
+_base_ = [
+    '../_base_/models/retinanet_r50_fpn.py',
+    '../_base_/datasets/coco_detection.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+model = dict(
+    type='RetinaNet',
+    backbone=dict(
+        _delete_=True,
+        type='PyramidVisionTransformerV2',
+        embed_dims=32,
+        num_layers=[2, 2, 2, 2],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_v2_b0.pth')),
+    neck=dict(in_channels=[32, 64, 160, 256]))
+# optimizer
+optimizer = dict(_delete_=True, type='AdamW', lr=0.0001, weight_decay=0.0001)
diff --git a/configs/pvt/retinanet_pvtv2-b1_fpn_1x_coco.py b/configs/pvt/retinanet_pvtv2-b1_fpn_1x_coco.py
new file mode 100644
index 00000000..5374c509
--- /dev/null
+++ b/configs/pvt/retinanet_pvtv2-b1_fpn_1x_coco.py
@@ -0,0 +1,7 @@
+_base_ = 'retinanet_pvtv2-b0_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        embed_dims=64,
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_v2_b1.pth')),
+    neck=dict(in_channels=[64, 128, 320, 512]))
diff --git a/configs/pvt/retinanet_pvtv2-b2_fpn_1x_coco.py b/configs/pvt/retinanet_pvtv2-b2_fpn_1x_coco.py
new file mode 100644
index 00000000..cf9a18de
--- /dev/null
+++ b/configs/pvt/retinanet_pvtv2-b2_fpn_1x_coco.py
@@ -0,0 +1,8 @@
+_base_ = 'retinanet_pvtv2-b0_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        embed_dims=64,
+        num_layers=[3, 4, 6, 3],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_v2_b2.pth')),
+    neck=dict(in_channels=[64, 128, 320, 512]))
diff --git a/configs/pvt/retinanet_pvtv2-b3_fpn_1x_coco.py b/configs/pvt/retinanet_pvtv2-b3_fpn_1x_coco.py
new file mode 100644
index 00000000..7a47f820
--- /dev/null
+++ b/configs/pvt/retinanet_pvtv2-b3_fpn_1x_coco.py
@@ -0,0 +1,8 @@
+_base_ = 'retinanet_pvtv2-b0_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        embed_dims=64,
+        num_layers=[3, 4, 18, 3],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_v2_b3.pth')),
+    neck=dict(in_channels=[64, 128, 320, 512]))
diff --git a/configs/pvt/retinanet_pvtv2-b4_fpn_1x_coco.py b/configs/pvt/retinanet_pvtv2-b4_fpn_1x_coco.py
new file mode 100644
index 00000000..ec9103b4
--- /dev/null
+++ b/configs/pvt/retinanet_pvtv2-b4_fpn_1x_coco.py
@@ -0,0 +1,13 @@
+_base_ = 'retinanet_pvtv2-b0_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        embed_dims=64,
+        num_layers=[3, 8, 27, 3],
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_v2_b4.pth')),
+    neck=dict(in_channels=[64, 128, 320, 512]))
+# optimizer
+optimizer = dict(
+    _delete_=True, type='AdamW', lr=0.0001 / 1.4, weight_decay=0.0001)
+# dataset settings
+data = dict(samples_per_gpu=1, workers_per_gpu=1)
diff --git a/configs/pvt/retinanet_pvtv2-b5_fpn_1x_coco.py b/configs/pvt/retinanet_pvtv2-b5_fpn_1x_coco.py
new file mode 100644
index 00000000..d8e6d23c
--- /dev/null
+++ b/configs/pvt/retinanet_pvtv2-b5_fpn_1x_coco.py
@@ -0,0 +1,14 @@
+_base_ = 'retinanet_pvtv2-b0_fpn_1x_coco.py'
+model = dict(
+    backbone=dict(
+        embed_dims=64,
+        num_layers=[3, 6, 40, 3],
+        mlp_ratios=(4, 4, 4, 4),
+        init_cfg=dict(checkpoint='https://github.com/whai362/PVT/'
+                      'releases/download/v2/pvt_v2_b5.pth')),
+    neck=dict(in_channels=[64, 128, 320, 512]))
+# optimizer
+optimizer = dict(
+    _delete_=True, type='AdamW', lr=0.0001 / 1.4, weight_decay=0.0001)
+# dataset settings
+data = dict(samples_per_gpu=1, workers_per_gpu=1)
diff --git a/configs/regnet/metafile.yml b/configs/regnet/metafile.yml
index ac06708e..5390a353 100644
--- a/configs/regnet/metafile.yml
+++ b/configs/regnet/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - RegNet
-    Paper: https://arxiv.org/abs/2003.13678
+    Paper:
+      URL: https://arxiv.org/abs/2003.13678
+      Title: 'Designing Network Design Spaces'
     README: configs/regnet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.1.0/mmdet/models/backbones/regnet.py#L11
+      Version: v2.1.0
 
 Models:
   - Name: mask_rcnn_regnetx-3.2GF_fpn_1x_coco
diff --git a/configs/reppoints/metafile.yml b/configs/reppoints/metafile.yml
index 8ec2056d..cd4312c4 100644
--- a/configs/reppoints/metafile.yml
+++ b/configs/reppoints/metafile.yml
@@ -11,8 +11,13 @@ Collections:
         - FPN
         - RepPoints
         - ResNet
-    Paper: https://arxiv.org/abs/1904.11490
+    Paper:
+      URL: https://arxiv.org/abs/1904.11490
+      Title: 'RepPoints: Point Set Representation for Object Detection'
     README: configs/reppoints/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/reppoints_detector.py#L9
+      Version: v2.0.0
 
 Models:
   - Name: bbox_r50_grid_fpn_gn-neck+head_1x_coco
diff --git a/configs/res2net/metafile.yml b/configs/res2net/metafile.yml
index a91bab2b..71809f30 100644
--- a/configs/res2net/metafile.yml
+++ b/configs/res2net/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - Res2Net
-    Paper: https://arxiv.org/abs/1904.01169
+    Paper:
+      URL: https://arxiv.org/abs/1904.01169
+      Title: 'Res2Net for object detection and instance segmentation'
     README: configs/res2net/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.1.0/mmdet/models/backbones/res2net.py#L239
+      Version: v2.1.0
 
 Models:
   - Name: faster_rcnn_r2_101_fpn_2x_coco
diff --git a/configs/resnest/metafile.yml b/configs/resnest/metafile.yml
index beb8ab28..d7f68e5c 100644
--- a/configs/resnest/metafile.yml
+++ b/configs/resnest/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - ResNeSt
-    Paper: https://arxiv.org/abs/2004.08955
+    Paper:
+      URL: https://arxiv.org/abs/2004.08955
+      Title: 'ResNeSt: Split-Attention Networks'
     README: configs/renest/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.7.0/mmdet/models/backbones/resnest.py#L273
+      Version: v2.7.0
 
 Models:
   - Name: faster_rcnn_s50_fpn_syncbn-backbone+head_mstrain-range_1x_coco
diff --git a/configs/retinanet/metafile.yml b/configs/retinanet/metafile.yml
index 11bc54ea..efff0905 100644
--- a/configs/retinanet/metafile.yml
+++ b/configs/retinanet/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - Focal Loss
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/1708.02002
+    Paper:
+      URL: https://arxiv.org/abs/1708.02002
+      Title: 'Focal Loss for Dense Object Detection'
     README: configs/retinanet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/retinanet.py#L6
+      Version: v2.0.0
 
 Models:
   - Name: retinanet_r50_caffe_fpn_1x_coco
diff --git a/configs/sabl/metafile.yml b/configs/sabl/metafile.yml
index 7680afd9..23c51cff 100644
--- a/configs/sabl/metafile.yml
+++ b/configs/sabl/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - ResNet
         - SABL
-    Paper: https://arxiv.org/abs/1912.04260
+    Paper:
+      URL: https://arxiv.org/abs/1912.04260
+      Title: 'Side-Aware Boundary Localization for More Precise Object Detection'
     README: configs/sabl/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.4.0/mmdet/models/roi_heads/bbox_heads/sabl_head.py#L14
+      Version: v2.4.0
 
 Models:
   - Name: sabl_faster_rcnn_r50_fpn_1x_coco
diff --git a/configs/scnet/README.md b/configs/scnet/README.md
index 3757be13..9dcc4ae6 100644
--- a/configs/scnet/README.md
+++ b/configs/scnet/README.md
@@ -40,10 +40,10 @@ The results on COCO 2017val are shown in the below table. (results on test-dev a
 
 |     Backbone    |  Style  | Lr schd | Mem (GB) | Inf speed (fps) | box AP | mask AP | TTA box AP | TTA mask AP | Config |   Download   |
 |:---------------:|:-------:|:-------:|:--------:|:---------------:|:------:|:-------:|:----------:|:-----------:|:------:|:------------:|
-|     R-50-FPN    | pytorch |    1x   |    7.0   |       6.2       |  43.5  |   39.2  |    44.8    |     40.9    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_r50_fpn_1x_coco.py) | [model](https://drive.google.com/file/d/1K5_8-P0EC43WZFtoO3q9_JE-df8pEc7J/view?usp=sharing) \| [log](https://drive.google.com/file/d/1ZFS6QhFfxlOnDYPiGpSDP_Fzgb7iDGN3/view?usp=sharing) |
-|     R-50-FPN    | pytorch |   20e   |    7.0   |       6.2       |  44.5  |   40.0  |    45.8    |     41.5    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_r50_fpn_20e_coco.py) | [model](https://drive.google.com/file/d/15VGLCt5-IO5TbzB4Kw6ZyoF6QH0Q511A/view?usp=sharing) \| [log](https://drive.google.com/file/d/1-LnkOXN8n5ojQW34H0qZ625cgrnWpqSX/view?usp=sharing) |
-|    R-101-FPN    | pytorch |   20e   |    8.9   |       5.8       |  45.8  |   40.9  |    47.3    |     42.7    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_r101_fpn_20e_coco.py) | [model](https://drive.google.com/file/d/1aeCGHsOBdfIqVBnBPp0JUE_RSIau3583/view?usp=sharing) \| [log](https://drive.google.com/file/d/1iRx-9GRgTaIDsz-we3DGwFVH22nbvCLa/view?usp=sharing) |
-| X-101-64x4d-FPN | pytorch |   20e   |   13.2   |       4.9       |  47.5  |   42.3  |    48.9    |     44.0    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_x101_64x4d_fpn_20e_coco.py) | [model](https://drive.google.com/file/d/1YjgutUKz4TTPpqSWGKUTkZJ8_X-kyCfY/view?usp=sharing) \| [log](https://drive.google.com/file/d/1OsfQJ8gwtqIQ61k358yxY21sCvbUcRjs/view?usp=sharing) |
+|     R-50-FPN    | pytorch |    1x   |    7.0   |       6.2       |  43.5  |   39.2  |    44.8    |     40.9    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_r50_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r50_fpn_1x_coco/scnet_r50_fpn_1x_coco-c3f09857.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r50_fpn_1x_coco/scnet_r50_fpn_1x_coco_20210117_192725.log.json) |
+|     R-50-FPN    | pytorch |   20e   |    7.0   |       6.2       |  44.5  |   40.0  |    45.8    |     41.5    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_r50_fpn_20e_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r50_fpn_20e_coco/scnet_r50_fpn_20e_coco-a569f645.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r50_fpn_20e_coco/scnet_r50_fpn_20e_coco_20210116_060148.log.json) |
+|    R-101-FPN    | pytorch |   20e   |    8.9   |       5.8       |  45.8  |   40.9  |    47.3    |     42.7    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_r101_fpn_20e_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r101_fpn_20e_coco/scnet_r101_fpn_20e_coco-294e312c.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r101_fpn_20e_coco/scnet_r101_fpn_20e_coco_20210118_175824.log.json) |
+| X-101-64x4d-FPN | pytorch |   20e   |   13.2   |       4.9       |  47.5  |   42.3  |    48.9    |     44.0    | [config](https://github.com/open-mmlab/mmdetection/tree/master/configs/scnet/scnet_x101_64x4d_fpn_20e_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_x101_64x4d_fpn_20e_coco/scnet_x101_64x4d_fpn_20e_coco-fb09dec9.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_x101_64x4d_fpn_20e_coco/scnet_x101_64x4d_fpn_20e_coco_20210120_045959.log.json) |
 
 ### Notes
 
diff --git a/configs/scnet/metafile.yml b/configs/scnet/metafile.yml
index c6429bba..15eaebfa 100644
--- a/configs/scnet/metafile.yml
+++ b/configs/scnet/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - ResNet
         - SCNet
-    Paper: https://arxiv.org/abs/2012.10150
+    Paper:
+      URL: https://arxiv.org/abs/2012.10150
+      Title: 'SCNet: Training Inference Sample Consistency for Instance Segmentation'
     README: configs/scnet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.9.0/mmdet/models/detectors/scnet.py#L6
+      Version: v2.9.0
 
 Models:
   - Name: scnet_r50_fpn_1x_coco
@@ -36,7 +41,7 @@ Models:
         Dataset: COCO
         Metrics:
           mask AP: 39.2
-    Weights: https://drive.google.com/file/d/1K5_8-P0EC43WZFtoO3q9_JE-df8pEc7J/view?usp=sharing
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r50_fpn_1x_coco/scnet_r50_fpn_1x_coco-c3f09857.pth
 
   - Name: scnet_r50_fpn_20e_coco
     In Collection: SCNet
@@ -60,7 +65,7 @@ Models:
         Dataset: COCO
         Metrics:
           mask AP: 40.0
-    Weights: https://drive.google.com/file/d/15VGLCt5-IO5TbzB4Kw6ZyoF6QH0Q511A/view?usp=sharing
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r50_fpn_20e_coco/scnet_r50_fpn_20e_coco-a569f645.pth
 
   - Name: scnet_r101_fpn_20e_coco
     In Collection: SCNet
@@ -84,7 +89,7 @@ Models:
         Dataset: COCO
         Metrics:
           mask AP: 40.9
-    Weights: https://drive.google.com/file/d/1aeCGHsOBdfIqVBnBPp0JUE_RSIau3583/view?usp=sharing
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_r101_fpn_20e_coco/scnet_r101_fpn_20e_coco-294e312c.pth
 
   - Name: scnet_x101_64x4d_fpn_20e_coco
     In Collection: SCNet
@@ -108,4 +113,4 @@ Models:
         Dataset: COCO
         Metrics:
           mask AP: 42.3
-    Weights: https://drive.google.com/file/d/1YjgutUKz4TTPpqSWGKUTkZJ8_X-kyCfY/view?usp=sharing
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_x101_64x4d_fpn_20e_coco/scnet_x101_64x4d_fpn_20e_coco-fb09dec9.pth
diff --git a/configs/scnet/scnet_r50_fpn_1x_coco.py b/configs/scnet/scnet_r50_fpn_1x_coco.py
index e4215a6d..fe03b0d4 100644
--- a/configs/scnet/scnet_r50_fpn_1x_coco.py
+++ b/configs/scnet/scnet_r50_fpn_1x_coco.py
@@ -94,8 +94,8 @@ model = dict(
             in_channels=256,
             conv_out_channels=256,
             num_classes=183,
-            ignore_label=255,
-            loss_weight=0.2,
+            loss_seg=dict(
+                type='CrossEntropyLoss', ignore_index=255, loss_weight=0.2),
             conv_to_res=True),
         glbctx_head=dict(
             type='GlobalContextHead',
diff --git a/configs/scratch/metafile.yml b/configs/scratch/metafile.yml
index cbcfa435..65025fac 100644
--- a/configs/scratch/metafile.yml
+++ b/configs/scratch/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - RPN
         - ResNet
-    Paper: https://arxiv.org/abs/1811.08883
+    Paper:
+      URL: https://arxiv.org/abs/1811.08883
+      Title: 'Rethinking ImageNet Pre-training'
     README: configs/scratch/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/configs/scratch/faster_rcnn_r50_fpn_gn-all_scratch_6x_coco.py
+      Version: v2.0.0
 
 Models:
   - Name: faster_rcnn_r50_fpn_gn-all_scratch_6x_coco
diff --git a/configs/solo/README.md b/configs/solo/README.md
new file mode 100644
index 00000000..709e246f
--- /dev/null
+++ b/configs/solo/README.md
@@ -0,0 +1,42 @@
+# SOLO: Segmenting Objects by Locations
+
+## Introduction
+
+```
+@inproceedings{wang2020solo,
+  title     =  {{SOLO}: Segmenting Objects by Locations},
+  author    =  {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},
+  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},
+  year      =  {2020}
+}
+```
+
+## Results and Models
+
+### SOLO
+
+| Backbone  | Style   | MS train | Lr schd | Mem (GB) | Inf time (fps) | mask AP | Download |
+|:---------:|:-------:|:--------:|:-------:|:--------:|:--------------:|:------:|:--------:|
+| R-50      | pytorch | N        | 1x      |  8.0     |   14.0         |  33.1  |  [model](https://download.openmmlab.com/mmdetection/v2.0/solo/solo_r50_fpn_1x_coco/solo_r50_fpn_1x_coco_20210821_035055-2290a6b8.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/solo/solo_r50_fpn_1x_coco/solo_r50_fpn_1x_coco_20210821_035055.log.json) |
+| R-50      | pytorch | Y        | 3x      |  7.4     |   14.0         |  35.9  |  [model](https://download.openmmlab.com/mmdetection/v2.0/solo/solo_r50_fpn_3x_coco/solo_r50_fpn_3x_coco_20210901_012353-11d224d7.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/solo/solo_r50_fpn_3x_coco/solo_r50_fpn_3x_coco_20210901_012353.log.json) |
+
+### Decoupled SOLO
+
+| Backbone  | Style   | MS train | Lr schd | Mem (GB) | Inf time (fps) | mask AP | Download |
+|:---------:|:-------:|:--------:|:-------:|:--------:|:--------------:|:-------:|:--------:|
+| R-50      | pytorch | N        | 1x      |  7.8     |    12.5        |  33.9   |  [model](https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_r50_fpn_1x_coco/decoupled_solo_r50_fpn_1x_coco_20210820_233348-6337c589.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_r50_fpn_1x_coco/decoupled_solo_r50_fpn_1x_coco_20210820_233348.log.json) |
+| R-50      | pytorch | Y        | 3x      |  7.9     |   12.5         |  36.7   |  [model](https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_r50_fpn_3x_coco/decoupled_solo_r50_fpn_3x_coco_20210821_042504-7b3301ec.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_r50_fpn_3x_coco/decoupled_solo_r50_fpn_3x_coco_20210821_042504.log.json) |
+
+- Decoupled SOLO has a decoupled head which is different from SOLO head.
+Decoupled SOLO serves as an efficient and equivalent variant in accuracy
+of SOLO. Please refer to the corresponding config files for details.
+
+### Decoupled Light SOLO
+
+| Backbone  | Style   | MS train | Lr schd | Mem (GB) | Inf time (fps) | mask AP | Download |
+|:---------:|:-------:|:--------:|:-------:|:--------:|:--------------:|:------:|:--------:|
+| R-50      | pytorch | Y        | 3x      | 2.2      |    31.2        | 32.9   |  [model](https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_light_r50_fpn_3x_coco/decoupled_solo_light_r50_fpn_3x_coco_20210906_142703-e70e226f.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_light_r50_fpn_3x_coco/decoupled_solo_light_r50_fpn_3x_coco_20210906_142703.log.json) |
+
+- Decoupled Light SOLO using decoupled structure similar to Decoupled
+SOLO head, with light-weight head and smaller input size, Please refer
+to the corresponding config files for details.
diff --git a/configs/solo/decoupled_solo_light_r50_fpn_3x_coco.py b/configs/solo/decoupled_solo_light_r50_fpn_3x_coco.py
new file mode 100644
index 00000000..101f8f1d
--- /dev/null
+++ b/configs/solo/decoupled_solo_light_r50_fpn_3x_coco.py
@@ -0,0 +1,63 @@
+_base_ = './decoupled_solo_r50_fpn_3x_coco.py'
+
+# model settings
+model = dict(
+    mask_head=dict(
+        type='DecoupledSOLOLightHead',
+        num_classes=80,
+        in_channels=256,
+        stacked_convs=4,
+        feat_channels=256,
+        strides=[8, 8, 16, 32, 32],
+        scale_ranges=((1, 64), (32, 128), (64, 256), (128, 512), (256, 2048)),
+        pos_scale=0.2,
+        num_grids=[40, 36, 24, 16, 12],
+        cls_down_index=0,
+        loss_mask=dict(
+            type='DiceLoss', use_sigmoid=True, activate=False,
+            loss_weight=3.0),
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)))
+
+img_norm_cfg = dict(
+    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(
+        type='Resize',
+        img_scale=[(852, 512), (852, 480), (852, 448), (852, 416), (852, 384),
+                   (852, 352)],
+        multiscale_mode='value',
+        keep_ratio=True),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size_divisor=32),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(852, 512),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='Pad', size_divisor=32),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+
+data = dict(
+    train=dict(pipeline=train_pipeline),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
diff --git a/configs/solo/decoupled_solo_r50_fpn_1x_coco.py b/configs/solo/decoupled_solo_r50_fpn_1x_coco.py
new file mode 100644
index 00000000..b611cdf4
--- /dev/null
+++ b/configs/solo/decoupled_solo_r50_fpn_1x_coco.py
@@ -0,0 +1,28 @@
+_base_ = [
+    './solo_r50_fpn_1x_coco.py',
+]
+# model settings
+model = dict(
+    mask_head=dict(
+        type='DecoupledSOLOHead',
+        num_classes=80,
+        in_channels=256,
+        stacked_convs=7,
+        feat_channels=256,
+        strides=[8, 8, 16, 32, 32],
+        scale_ranges=((1, 96), (48, 192), (96, 384), (192, 768), (384, 2048)),
+        pos_scale=0.2,
+        num_grids=[40, 36, 24, 16, 12],
+        cls_down_index=0,
+        loss_mask=dict(
+            type='DiceLoss', use_sigmoid=True, activate=False,
+            loss_weight=3.0),
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)))
+
+optimizer = dict(type='SGD', lr=0.01)
diff --git a/configs/solo/decoupled_solo_r50_fpn_3x_coco.py b/configs/solo/decoupled_solo_r50_fpn_3x_coco.py
new file mode 100644
index 00000000..4a8c19de
--- /dev/null
+++ b/configs/solo/decoupled_solo_r50_fpn_3x_coco.py
@@ -0,0 +1,25 @@
+_base_ = './solo_r50_fpn_3x_coco.py'
+
+# model settings
+model = dict(
+    mask_head=dict(
+        type='DecoupledSOLOHead',
+        num_classes=80,
+        in_channels=256,
+        stacked_convs=7,
+        feat_channels=256,
+        strides=[8, 8, 16, 32, 32],
+        scale_ranges=((1, 96), (48, 192), (96, 384), (192, 768), (384, 2048)),
+        pos_scale=0.2,
+        num_grids=[40, 36, 24, 16, 12],
+        cls_down_index=0,
+        loss_mask=dict(
+            type='DiceLoss', use_sigmoid=True, activate=False,
+            loss_weight=3.0),
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)))
diff --git a/configs/solo/metafile.yml b/configs/solo/metafile.yml
new file mode 100644
index 00000000..b6244e80
--- /dev/null
+++ b/configs/solo/metafile.yml
@@ -0,0 +1,115 @@
+Collections:
+  - Name: SOLO
+    Metadata:
+      Training Data: COCO
+      Training Techniques:
+        - SGD with Momentum
+        - Weight Decay
+      Training Resources: 8x V100 GPUs
+      Architecture:
+        - FPN
+        - Convolution
+        - ResNet
+    Paper: https://arxiv.org/abs/1912.04488
+    README: configs/solo/README.md
+
+Models:
+  - Name: decoupled_solo_r50_fpn_1x_coco
+    In Collection: SOLO
+    Config: configs/solo/decoupled_solo_r50_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 7.8
+      Epochs: 12
+    inference time (ms/im):
+      - value: 116.4
+        hardware: V100
+        backend: PyTorch
+        batch size: 1
+        mode: FP32
+        resolution: (1333, 800)
+    Results:
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 33.9
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_r50_fpn_1x_coco/decoupled_solo_r50_fpn_1x_coco_20210820_233348-6337c589.pth
+
+  - Name: decoupled_solo_r50_fpn_3x_coco
+    In Collection: SOLO
+    Config: configs/solo/decoupled_solo_r50_fpn_3x_coco.py
+    Metadata:
+      Training Memory (GB): 7.9
+      Epochs: 36
+    inference time (ms/im):
+      - value: 117.2
+        hardware: V100
+        backend: PyTorch
+        batch size: 1
+        mode: FP32
+        resolution: (1333, 800)
+    Results:
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 36.7
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_r50_fpn_3x_coco/decoupled_solo_r50_fpn_3x_coco_20210821_042504-7b3301ec.pth
+
+  - Name: decoupled_solo_light_r50_fpn_3x_coco
+    In Collection: SOLO
+    Config: configs/solo/decoupled_solo_light_r50_fpn_3x_coco.py
+    Metadata:
+      Training Memory (GB): 2.2
+      Epochs: 36
+    inference time (ms/im):
+      - value: 35.0
+        hardware: V100
+        backend: PyTorch
+        batch size: 1
+        mode: FP32
+        resolution: (852, 512)
+    Results:
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 32.9
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/solo/decoupled_solo_light_r50_fpn_3x_coco/decoupled_solo_light_r50_fpn_3x_coco_20210906_142703-e70e226f.pth
+
+  - Name: solo_r50_fpn_3x_coco
+    In Collection: SOLO
+    Config: configs/solo/solo_r50_fpn_3x_coco.py
+    Metadata:
+      Training Memory (GB): 7.4
+      Epochs: 36
+    inference time (ms/im):
+      - value: 94.2
+        hardware: V100
+        backend: PyTorch
+        batch size: 1
+        mode: FP32
+        resolution: (1333, 800)
+    Results:
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 35.9
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/solo/solo_r50_fpn_3x_coco/solo_r50_fpn_3x_coco_20210901_012353-11d224d7.pth
+
+  - Name: solo_r50_fpn_1x_coco
+    In Collection: SOLO
+    Config: configs/solo/solo_r50_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 8.0
+      Epochs: 12
+    inference time (ms/im):
+      - value: 95.1
+        hardware: V100
+        backend: PyTorch
+        batch size: 1
+        mode: FP32
+        resolution: (1333, 800)
+    Results:
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 33.1
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/solo/solo_r50_fpn_1x_coco/solo_r50_fpn_1x_coco_20210821_035055-2290a6b8.pth
diff --git a/configs/solo/solo_r50_fpn_1x_coco.py b/configs/solo/solo_r50_fpn_1x_coco.py
new file mode 100644
index 00000000..9093a504
--- /dev/null
+++ b/configs/solo/solo_r50_fpn_1x_coco.py
@@ -0,0 +1,53 @@
+_base_ = [
+    '../_base_/datasets/coco_instance.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+
+# model settings
+model = dict(
+    type='SOLO',
+    backbone=dict(
+        type='ResNet',
+        depth=50,
+        num_stages=4,
+        out_indices=(0, 1, 2, 3),
+        frozen_stages=1,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        style='pytorch'),
+    neck=dict(
+        type='FPN',
+        in_channels=[256, 512, 1024, 2048],
+        out_channels=256,
+        start_level=0,
+        num_outs=5),
+    mask_head=dict(
+        type='SOLOHead',
+        num_classes=80,
+        in_channels=256,
+        stacked_convs=7,
+        feat_channels=256,
+        strides=[8, 8, 16, 32, 32],
+        scale_ranges=((1, 96), (48, 192), (96, 384), (192, 768), (384, 2048)),
+        pos_scale=0.2,
+        num_grids=[40, 36, 24, 16, 12],
+        cls_down_index=0,
+        loss_mask=dict(type='DiceLoss', use_sigmoid=True, loss_weight=3.0),
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)),
+    # model training and testing settings
+    test_cfg=dict(
+        nms_pre=500,
+        score_thr=0.1,
+        mask_thr=0.5,
+        filter_thr=0.05,
+        kernel='gaussian',  # gaussian/linear
+        sigma=2.0,
+        max_per_img=100))
+
+# optimizer
+optimizer = dict(type='SGD', lr=0.01)
diff --git a/configs/solo/solo_r50_fpn_3x_coco.py b/configs/solo/solo_r50_fpn_3x_coco.py
new file mode 100644
index 00000000..52302cdf
--- /dev/null
+++ b/configs/solo/solo_r50_fpn_3x_coco.py
@@ -0,0 +1,28 @@
+_base_ = './solo_r50_fpn_1x_coco.py'
+
+img_norm_cfg = dict(
+    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(
+        type='Resize',
+        img_scale=[(1333, 800), (1333, 768), (1333, 736), (1333, 704),
+                   (1333, 672), (1333, 640)],
+        multiscale_mode='value',
+        keep_ratio=True),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size_divisor=32),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
+]
+data = dict(train=dict(pipeline=train_pipeline))
+
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 3,
+    step=[27, 33])
+runner = dict(type='EpochBasedRunner', max_epochs=36)
diff --git a/configs/sparse_rcnn/metafile.yml b/configs/sparse_rcnn/metafile.yml
index 53280cc3..bb1273ec 100644
--- a/configs/sparse_rcnn/metafile.yml
+++ b/configs/sparse_rcnn/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - ResNet
         - Sparse R-CNN
-    Paper: https://arxiv.org/abs/2011.12450
+    Paper:
+      URL: https://arxiv.org/abs/2011.12450
+      Title: 'Sparse R-CNN: End-to-End Object Detection with Learnable Proposals'
     README: configs/sparse_rcnn/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.9.0/mmdet/models/detectors/sparse_rcnn.py#L6
+      Version: v2.9.0
 
 Models:
   - Name: sparse_rcnn_r50_fpn_1x_coco
diff --git a/configs/ssd/metafile.yml b/configs/ssd/metafile.yml
index 31ead0de..b9ee79cd 100644
--- a/configs/ssd/metafile.yml
+++ b/configs/ssd/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - VGG
-    Paper: https://arxiv.org/abs/1512.02325
+    Paper:
+      URL: https://arxiv.org/abs/1512.02325
+      Title: 'SSD: Single Shot MultiBox Detector'
     README: configs/ssd/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.14.0/mmdet/models/dense_heads/ssd_head.py#L16
+      Version: v2.14.0
 
 Models:
   - Name: ssd300_coco
diff --git a/configs/ssd/ssd300_coco.py b/configs/ssd/ssd300_coco.py
index 75c5e4e5..10981874 100644
--- a/configs/ssd/ssd300_coco.py
+++ b/configs/ssd/ssd300_coco.py
@@ -60,3 +60,7 @@ data = dict(
 # optimizer
 optimizer = dict(type='SGD', lr=2e-3, momentum=0.9, weight_decay=5e-4)
 optimizer_config = dict(_delete_=True)
+custom_hooks = [
+    dict(type='NumClassCheckHook'),
+    dict(type='CheckInvalidLossHook', interval=50, priority='VERY_LOW')
+]
diff --git a/configs/ssd/ssd512_coco.py b/configs/ssd/ssd512_coco.py
index 0bbbd3f6..3d5f346a 100644
--- a/configs/ssd/ssd512_coco.py
+++ b/configs/ssd/ssd512_coco.py
@@ -73,3 +73,7 @@ data = dict(
 # optimizer
 optimizer = dict(type='SGD', lr=2e-3, momentum=0.9, weight_decay=5e-4)
 optimizer_config = dict(_delete_=True)
+custom_hooks = [
+    dict(type='NumClassCheckHook'),
+    dict(type='CheckInvalidLossHook', interval=50, priority='VERY_LOW')
+]
diff --git a/configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py b/configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py
index 378403d7..653fff15 100644
--- a/configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py
+++ b/configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py
@@ -139,3 +139,7 @@ runner = dict(type='EpochBasedRunner', max_epochs=120)
 # Avoid evaluation and saving weights too frequently
 evaluation = dict(interval=5, metric='bbox')
 checkpoint_config = dict(interval=5)
+custom_hooks = [
+    dict(type='NumClassCheckHook'),
+    dict(type='CheckInvalidLossHook', interval=50, priority='VERY_LOW')
+]
diff --git a/configs/strong_baselines/README.md b/configs/strong_baselines/README.md
new file mode 100644
index 00000000..c1487ef9
--- /dev/null
+++ b/configs/strong_baselines/README.md
@@ -0,0 +1,18 @@
+# Strong Baselines
+
+We train Mask R-CNN with large-scale jittor and longer schedule as strong baselines.
+The modifications follow those in [Detectron2](https://github.com/facebookresearch/detectron2/tree/master/configs/new_baselines).
+
+## Results and models
+
+|    Backbone     |  Style  | Lr schd | Mem (GB) | Inf time (fps) | box AP | mask AP | Config | Download |
+| :-------------: | :-----: | :-----: | :------: | :------------: | :----: | :-----: | :------: | :--------: |
+|    R-50-FPN     | pytorch |   50e   |          |                |        |         |  [config](./mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_50e_coco.py) | [model]() &#124; [log]() |
+|    R-50-FPN     | pytorch |   100e  |          |                |        |         |  [config](./mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py) | [model]() &#124; [log]() |
+|    R-50-FPN     | caffe |   100e  |          |                |   44.7  |  40.4   |  [config](./mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py) | [model]() &#124; [log]() |
+|    R-50-FPN     | caffe |   400e  |          |                |        |         |  [config](./mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_400e_coco.py) | [model]() &#124; [log]() |
+
+## Notice
+
+When using large-scale jittering, there are sometimes empty proposals in the box and mask heads during training.
+This requires MMSyncBN that allows empty tensors. Therefore, please use mmcv-full>=1.3.14 to train models supported in this directory.
diff --git a/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py b/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py
new file mode 100644
index 00000000..a40d6a03
--- /dev/null
+++ b/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py
@@ -0,0 +1,80 @@
+_base_ = [
+    '../_base_/models/mask_rcnn_r50_fpn.py',
+    '../common/lsj_100e_coco_instance.py'
+]
+
+norm_cfg = dict(type='SyncBN', requires_grad=True)
+# Use MMSyncBN that handles empty tensor in head. It can be changed to
+# SyncBN after https://github.com/pytorch/pytorch/issues/36530 is fixed
+# Requires MMCV-full after  https://github.com/open-mmlab/mmcv/pull/1205.
+head_norm_cfg = dict(type='MMSyncBN', requires_grad=True)
+model = dict(
+    backbone=dict(
+        frozen_stages=-1,
+        norm_eval=False,
+        norm_cfg=norm_cfg,
+        init_cfg=None,
+        style='caffe'),
+    neck=dict(norm_cfg=norm_cfg),
+    rpn_head=dict(num_convs=2),
+    roi_head=dict(
+        bbox_head=dict(
+            type='Shared4Conv1FCBBoxHead',
+            conv_out_channels=256,
+            norm_cfg=head_norm_cfg),
+        mask_head=dict(norm_cfg=head_norm_cfg)))
+
+file_client_args = dict(backend='disk')
+# file_client_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection/',
+#         'data/': 's3://openmmlab/datasets/detection/'
+#     }))
+
+img_norm_cfg = dict(
+    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
+image_size = (1024, 1024)
+train_pipeline = [
+    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(
+        type='Resize',
+        img_scale=image_size,
+        ratio_range=(0.1, 2.0),
+        multiscale_mode='range',
+        keep_ratio=True),
+    dict(
+        type='RandomCrop',
+        crop_type='absolute_range',
+        crop_size=image_size,
+        recompute_bbox=True,
+        allow_negative_crop=True),
+    dict(type='FilterAnnotations', min_gt_bbox_wh=(1e-2, 1e-2)),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size=image_size),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(1333, 800),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='Pad', size_divisor=32),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+
+# Use RepeatDataset to speed up training
+data = dict(
+    train=dict(dataset=dict(pipeline=train_pipeline)),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
diff --git a/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_fp16_coco.py b/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_fp16_coco.py
new file mode 100644
index 00000000..31824eb5
--- /dev/null
+++ b/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_fp16_coco.py
@@ -0,0 +1,2 @@
+_base_ = 'mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py'
+fp16 = dict(loss_scale=512.)
diff --git a/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_400e_coco.py b/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_400e_coco.py
new file mode 100644
index 00000000..1211925d
--- /dev/null
+++ b/configs/strong_baselines/mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_400e_coco.py
@@ -0,0 +1,6 @@
+_base_ = './mask_rcnn_r50_caffe_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py'
+
+# Use RepeatDataset to speed up training
+# change repeat time from 4 (for 100 epochs) to 16 (for 400 epochs)
+data = dict(train=dict(times=4 * 4))
+lr_config = dict(warmup_iters=500 * 4)
diff --git a/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py b/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py
new file mode 100644
index 00000000..4a15d698
--- /dev/null
+++ b/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py
@@ -0,0 +1,22 @@
+_base_ = [
+    '../_base_/models/mask_rcnn_r50_fpn.py',
+    '../common/lsj_100e_coco_instance.py'
+]
+
+norm_cfg = dict(type='SyncBN', requires_grad=True)
+# Use MMSyncBN that handles empty tensor in head. It can be changed to
+# SyncBN after https://github.com/pytorch/pytorch/issues/36530 is fixed
+# Requires MMCV-full after  https://github.com/open-mmlab/mmcv/pull/1205.
+head_norm_cfg = dict(type='MMSyncBN', requires_grad=True)
+model = dict(
+    # the model is trained from scratch, so init_cfg is None
+    backbone=dict(
+        frozen_stages=-1, norm_eval=False, norm_cfg=norm_cfg, init_cfg=None),
+    neck=dict(norm_cfg=norm_cfg),
+    rpn_head=dict(num_convs=2),  # leads to 0.1+ mAP
+    roi_head=dict(
+        bbox_head=dict(
+            type='Shared4Conv1FCBBoxHead',
+            conv_out_channels=256,
+            norm_cfg=head_norm_cfg),
+        mask_head=dict(norm_cfg=head_norm_cfg)))
diff --git a/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_fp16_coco.py b/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_fp16_coco.py
new file mode 100644
index 00000000..7b97960a
--- /dev/null
+++ b/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_fp16_coco.py
@@ -0,0 +1,3 @@
+_base_ = 'mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py'
+# use FP16
+fp16 = dict(loss_scale=512.)
diff --git a/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_50e_coco.py b/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_50e_coco.py
new file mode 100644
index 00000000..922579a1
--- /dev/null
+++ b/configs/strong_baselines/mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_50e_coco.py
@@ -0,0 +1,5 @@
+_base_ = 'mask_rcnn_r50_fpn_syncbn-all_rpn-2conv_lsj_100e_coco.py'
+
+# Use RepeatDataset to speed up training
+# change repeat time from 4 (for 100 epochs) to 2 (for 50 epochs)
+data = dict(train=dict(times=2))
diff --git a/configs/swin/README.md b/configs/swin/README.md
new file mode 100644
index 00000000..d18632a3
--- /dev/null
+++ b/configs/swin/README.md
@@ -0,0 +1,25 @@
+# Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
+
+## Introduction
+
+<!-- [ALGORITHM] -->
+
+```latex
+@article{liu2021Swin,
+    title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
+    author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
+    journal={arXiv preprint arXiv:2103.14030},
+    year={2021}
+}
+```
+
+## Results and models
+
+### Mask R-CNN
+
+| Backbone | Pretrain    | Lr schd | Multi-scale crop     |   FP16   |Mem (GB) | Inf time (fps) | box AP | mask AP |  Config  |   Download  |
+| :------: | :---------: | :-----: | :-------------------:| :------: |:------: | :------------: | :----: | :-----: | :------: |  :--------: |
+|  Swin-T  | ImageNet-1K |    1x   |        no            |    no    |   7.6   |                |  42.7  |  39.3   | [config](./mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py)             | [model](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth)  &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937.log.json) |
+|  Swin-T  | ImageNet-1K |    3x   |        yes           |    no    |   10.2  |                |  46.0  |  41.6   | [config](./mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco.py)     | [model](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco_20210906_131725-bacf6f7b.pth)  &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco_20210906_131725.log.json) |
+|  Swin-T  | ImageNet-1K |    3x   |        yes           |    yes   |   7.8   |                |  46.0  |  41.7   | [config](./mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py)| [model](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco_20210908_165006-90a4008c.pth)  &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco_20210908_165006.log.json) |
+|  Swin-S  | ImageNet-1K |    3x   |        yes           |    yes   |   11.9  |                |  48.2  |  43.2   | [config](./mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py)| [model](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco_20210903_104808-b92c91f1.pth)  &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco_20210903_104808.log.json) |
diff --git a/configs/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py b/configs/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py
new file mode 100644
index 00000000..15d50a02
--- /dev/null
+++ b/configs/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py
@@ -0,0 +1,6 @@
+_base_ = './mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py'
+pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth'  # noqa
+model = dict(
+    backbone=dict(
+        depths=[2, 2, 18, 2],
+        init_cfg=dict(type='Pretrained', checkpoint=pretrained)))
diff --git a/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py b/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py
new file mode 100644
index 00000000..337e8581
--- /dev/null
+++ b/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py
@@ -0,0 +1,42 @@
+_base_ = [
+    '../_base_/models/mask_rcnn_r50_fpn.py',
+    '../_base_/datasets/coco_instance.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'  # noqa
+model = dict(
+    type='MaskRCNN',
+    backbone=dict(
+        _delete_=True,
+        type='SwinTransformer',
+        embed_dims=96,
+        depths=[2, 2, 6, 2],
+        num_heads=[3, 6, 12, 24],
+        window_size=7,
+        mlp_ratio=4,
+        qkv_bias=True,
+        qk_scale=None,
+        drop_rate=0.,
+        attn_drop_rate=0.,
+        drop_path_rate=0.2,
+        patch_norm=True,
+        out_indices=(0, 1, 2, 3),
+        with_cp=False,
+        convert_weights=True,
+        init_cfg=dict(type='Pretrained', checkpoint=pretrained)),
+    neck=dict(in_channels=[96, 192, 384, 768]))
+
+optimizer = dict(
+    _delete_=True,
+    type='AdamW',
+    lr=0.0001,
+    betas=(0.9, 0.999),
+    weight_decay=0.05,
+    paramwise_cfg=dict(
+        custom_keys={
+            'absolute_pos_embed': dict(decay_mult=0.),
+            'relative_position_bias_table': dict(decay_mult=0.),
+            'norm': dict(decay_mult=0.)
+        }))
+lr_config = dict(warmup_iters=1000, step=[8, 11])
+runner = dict(max_epochs=12)
diff --git a/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py b/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py
new file mode 100644
index 00000000..2be31143
--- /dev/null
+++ b/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py
@@ -0,0 +1,3 @@
+_base_ = './mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco.py'
+# you need to set mode='dynamic' if you are using pytorch<=1.5.0
+fp16 = dict(loss_scale=dict(init_scale=512))
diff --git a/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco.py b/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco.py
new file mode 100644
index 00000000..2612f6e3
--- /dev/null
+++ b/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco.py
@@ -0,0 +1,91 @@
+_base_ = [
+    '../_base_/models/mask_rcnn_r50_fpn.py',
+    '../_base_/datasets/coco_instance.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+
+pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'  # noqa
+
+model = dict(
+    type='MaskRCNN',
+    backbone=dict(
+        _delete_=True,
+        type='SwinTransformer',
+        embed_dims=96,
+        depths=[2, 2, 6, 2],
+        num_heads=[3, 6, 12, 24],
+        window_size=7,
+        mlp_ratio=4,
+        qkv_bias=True,
+        qk_scale=None,
+        drop_rate=0.,
+        attn_drop_rate=0.,
+        drop_path_rate=0.2,
+        patch_norm=True,
+        out_indices=(0, 1, 2, 3),
+        with_cp=False,
+        convert_weights=True,
+        init_cfg=dict(type='Pretrained', checkpoint=pretrained)),
+    neck=dict(in_channels=[96, 192, 384, 768]))
+
+img_norm_cfg = dict(
+    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+
+# augmentation strategy originates from DETR / Sparse RCNN
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(
+        type='AutoAugment',
+        policies=[[
+            dict(
+                type='Resize',
+                img_scale=[(480, 1333), (512, 1333), (544, 1333), (576, 1333),
+                           (608, 1333), (640, 1333), (672, 1333), (704, 1333),
+                           (736, 1333), (768, 1333), (800, 1333)],
+                multiscale_mode='value',
+                keep_ratio=True)
+        ],
+                  [
+                      dict(
+                          type='Resize',
+                          img_scale=[(400, 1333), (500, 1333), (600, 1333)],
+                          multiscale_mode='value',
+                          keep_ratio=True),
+                      dict(
+                          type='RandomCrop',
+                          crop_type='absolute_range',
+                          crop_size=(384, 600),
+                          allow_negative_crop=True),
+                      dict(
+                          type='Resize',
+                          img_scale=[(480, 1333), (512, 1333), (544, 1333),
+                                     (576, 1333), (608, 1333), (640, 1333),
+                                     (672, 1333), (704, 1333), (736, 1333),
+                                     (768, 1333), (800, 1333)],
+                          multiscale_mode='value',
+                          override=True,
+                          keep_ratio=True)
+                  ]]),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size_divisor=32),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
+]
+data = dict(train=dict(pipeline=train_pipeline))
+
+optimizer = dict(
+    _delete_=True,
+    type='AdamW',
+    lr=0.0001,
+    betas=(0.9, 0.999),
+    weight_decay=0.05,
+    paramwise_cfg=dict(
+        custom_keys={
+            'absolute_pos_embed': dict(decay_mult=0.),
+            'relative_position_bias_table': dict(decay_mult=0.),
+            'norm': dict(decay_mult=0.)
+        }))
+lr_config = dict(warmup_iters=1000, step=[27, 33])
+runner = dict(max_epochs=36)
diff --git a/configs/swin/metafile.yml b/configs/swin/metafile.yml
new file mode 100644
index 00000000..b265afe3
--- /dev/null
+++ b/configs/swin/metafile.yml
@@ -0,0 +1,85 @@
+Collections:
+  - Name: Swin Transformer
+    Metadata:
+      Training Data: COCO
+      Training Techniques:
+        - AdamW
+      Training Resources: 8x V100 GPUs
+      Architecture:
+        - Swin Transformer
+    Paper:
+      URL: https://arxiv.org/abs/2107.08430
+      Title: 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows'
+    README: configs/swin/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.16.0/mmdet/models/backbones/swin.py#L465
+      Version: v2.16.0
+
+Models:
+  - Name: mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco
+    In Collection: Swin Transformer
+    Config: configs/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py
+    Metadata:
+      Training Memory (GB): 11.9
+      Epochs: 36
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 48.2
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 43.2
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco_20210903_104808-b92c91f1.pth
+
+  - Name: mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco
+    In Collection: Swin Transformer
+    Config: configs/swin/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco.py
+    Metadata:
+      Training Memory (GB): 10.2
+      Epochs: 36
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 46.0
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 41.6
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco/mask_rcnn_swin-t-p4-w7_fpn_ms-crop-3x_coco_20210906_131725-bacf6f7b.pth
+
+  - Name: mask_rcnn_swin-t-p4-w7_fpn_1x_coco
+    In Collection: Swin Transformer
+    Config: configs/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py
+    Metadata:
+      Training Memory (GB): 7.6
+      Epochs: 12
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 42.7
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 39.3
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth
+
+  - Name: mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco
+    In Collection: Swin Transformer
+    Config: configs/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py
+    Metadata:
+      Training Memory (GB): 7.8
+      Epochs: 36
+    Results:
+      - Task: Object Detection
+        Dataset: COCO
+        Metrics:
+          box AP: 46.0
+      - Task: Instance Segmentation
+        Dataset: COCO
+        Metrics:
+          mask AP: 41.7
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco_20210908_165006-90a4008c.pth
diff --git a/configs/tridentnet/metafile.yml b/configs/tridentnet/metafile.yml
index 871c0bc9..2536f976 100644
--- a/configs/tridentnet/metafile.yml
+++ b/configs/tridentnet/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Architecture:
         - ResNet
         - TridentNet Block
-    Paper: https://arxiv.org/abs/1901.01892
+    Paper:
+      URL: https://arxiv.org/abs/1901.01892
+      Title: 'Scale-Aware Trident Networks for Object Detection'
     README: configs/tridentnet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.8.0/mmdet/models/detectors/trident_faster_rcnn.py#L6
+      Version: v2.8.0
 
 Models:
   - Name: tridentnet_r50_caffe_1x_coco
diff --git a/configs/vfnet/README.md b/configs/vfnet/README.md
index 363f1b90..5b5e4d12 100644
--- a/configs/vfnet/README.md
+++ b/configs/vfnet/README.md
@@ -26,14 +26,14 @@
 
 | Backbone     | Style     | DCN     | MS train | Lr schd |Inf time (fps) | box AP (val) | box AP (test-dev) | Config | Download |
 |:------------:|:---------:|:-------:|:--------:|:-------:|:-------------:|:------------:|:-----------------:|:------:|:--------:|
-| R-50         | pytorch   | N       | N        | 1x      | -          | 41.6         | 41.6              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r50_fpn_1x_coco.py) |  [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_1x_coco/vfnet_r50_fpn_1x_coco_20201027-38db6f58.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_1x_coco/vfnet_r50_fpn_1x_coco.json)|
-| R-50         | pytorch   | N       | Y        | 2x      | -          | 44.5         | 44.8              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r50_fpn_mstrain_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mstrain_2x_coco/vfnet_r50_fpn_mstrain_2x_coco_20201027-7cc75bd2.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mstrain_2x_coco/vfnet_r50_fpn_mstrain_2x_coco.json)|
-| R-50         | pytorch   | Y       | Y        | 2x      | -          | 47.8         | 48.0              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
-| R-101        | pytorch   | N       | N        | 1x      | -          | 43.0         | 43.6              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r101_fpn_1x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_1x_coco/vfnet_r101_fpn_1x_coco_20201027pth-c831ece7.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_1x_coco/vfnet_r101_fpn_1x_coco.json)|
-| R-101        | pytorch   | N       | Y        | 2x      | -          | 46.2         | 46.7              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r101_fpn_mstrain_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mstrain_2x_coco/vfnet_r101_fpn_mstrain_2x_coco_20201027pth-4a5d53f1.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mstrain_2x_coco/vfnet_r101_fpn_mstrain_2x_coco.json)|
-| R-101        | pytorch   | Y       | Y        | 2x      | -          | 49.0         | 49.2              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-7729adb5.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
-| X-101-32x4d  | pytorch   | Y       | Y        | 2x      | -          | 49.7         | 50.0              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-d300a6fc.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
-| X-101-64x4d  | pytorch   | Y       | Y        | 2x      |  -         | 50.4         | 50.8              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-b5f6da5e.pth) &#124; [log](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
+| R-50         | pytorch   | N       | N        | 1x      | -          | 41.6         | 41.6              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r50_fpn_1x_coco.py) |  [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_1x_coco/vfnet_r50_fpn_1x_coco_20201027-38db6f58.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_1x_coco/vfnet_r50_fpn_1x_coco.json)|
+| R-50         | pytorch   | N       | Y        | 2x      | -          | 44.5         | 44.8              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r50_fpn_mstrain_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mstrain_2x_coco/vfnet_r50_fpn_mstrain_2x_coco_20201027-7cc75bd2.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mstrain_2x_coco/vfnet_r50_fpn_mstrain_2x_coco.json)|
+| R-50         | pytorch   | Y       | Y        | 2x      | -          | 47.8         | 48.0              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
+| R-101        | pytorch   | N       | N        | 1x      | -          | 43.0         | 43.6              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r101_fpn_1x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_1x_coco/vfnet_r101_fpn_1x_coco_20201027pth-c831ece7.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_1x_coco/vfnet_r101_fpn_1x_coco.json)|
+| R-101        | pytorch   | N       | Y        | 2x      | -          | 46.2         | 46.7              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r101_fpn_mstrain_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mstrain_2x_coco/vfnet_r101_fpn_mstrain_2x_coco_20201027pth-4a5d53f1.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mstrain_2x_coco/vfnet_r101_fpn_mstrain_2x_coco.json)|
+| R-101        | pytorch   | Y       | Y        | 2x      | -          | 49.0         | 49.2              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-7729adb5.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
+| X-101-32x4d  | pytorch   | Y       | Y        | 2x      | -          | 49.7         | 50.0              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-d300a6fc.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
+| X-101-64x4d  | pytorch   | Y       | Y        | 2x      |  -         | 50.4         | 50.8              | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-b5f6da5e.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.json)|
 
 **Notes:**
 
diff --git a/configs/vfnet/metafile.yml b/configs/vfnet/metafile.yml
index dc6e78b1..bcbe576f 100644
--- a/configs/vfnet/metafile.yml
+++ b/configs/vfnet/metafile.yml
@@ -10,8 +10,13 @@ Collections:
         - FPN
         - ResNet
         - Varifocal Loss
-    Paper: https://arxiv.org/abs/2008.13367
+    Paper:
+      URL: https://arxiv.org/abs/2008.13367
+      Title: 'VarifocalNet: An IoU-aware Dense Object Detector'
     README: configs/vfnet/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.6.0/mmdet/models/detectors/vfnet.py#L6
+      Version: v2.6.0
 
 Models:
   - Name: vfnet_r50_fpn_1x_coco
@@ -24,7 +29,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 41.6
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_1x_coco/vfnet_r50_fpn_1x_coco_20201027-38db6f58.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_1x_coco/vfnet_r50_fpn_1x_coco_20201027-38db6f58.pth
 
   - Name: vfnet_r50_fpn_mstrain_2x_coco
     In Collection: VFNet
@@ -36,7 +41,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 44.8
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mstrain_2x_coco/vfnet_r50_fpn_mstrain_2x_coco_20201027-7cc75bd2.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mstrain_2x_coco/vfnet_r50_fpn_mstrain_2x_coco_20201027-7cc75bd2.pth
 
   - Name: vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco
     In Collection: VFNet
@@ -48,7 +53,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 48.0
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth
 
   - Name: vfnet_r101_fpn_1x_coco
     In Collection: VFNet
@@ -60,7 +65,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 43.6
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_1x_coco/vfnet_r101_fpn_1x_coco_20201027pth-c831ece7.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_1x_coco/vfnet_r101_fpn_1x_coco_20201027pth-c831ece7.pth
 
   - Name: vfnet_r101_fpn_mstrain_2x_coco
     In Collection: VFNet
@@ -72,7 +77,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 46.7
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mstrain_2x_coco/vfnet_r101_fpn_mstrain_2x_coco_20201027pth-4a5d53f1.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mstrain_2x_coco/vfnet_r101_fpn_mstrain_2x_coco_20201027pth-4a5d53f1.pth
 
   - Name: vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco
     In Collection: VFNet
@@ -84,7 +89,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 49.2
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-7729adb5.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-7729adb5.pth
 
   - Name: vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco
     In Collection: VFNet
@@ -96,7 +101,7 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 50.0
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-d300a6fc.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-d300a6fc.pth
 
   - Name: vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco
     In Collection: VFNet
@@ -108,4 +113,4 @@ Models:
         Dataset: COCO
         Metrics:
           box AP: 50.8
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-b5f6da5e.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-b5f6da5e.pth
diff --git a/configs/yolact/README.md b/configs/yolact/README.md
index da3559bb..f2d220e9 100644
--- a/configs/yolact/README.md
+++ b/configs/yolact/README.md
@@ -25,9 +25,9 @@ Here are our YOLACT models along with their FPS on a Titan Xp and mAP on COCO's
 
 | Image Size | GPU x BS | Backbone      | *FPS  | mAP  | Weights | Configs | Download |
 |:----------:|:--------:|:-------------:|:-----:|:----:|:-------:|:------:|:--------:|
-| 550        | 1x8      | Resnet50-FPN  | 42.5 | 29.0 | | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolact/yolact_r50_1x8_coco.py) |[model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/yolact/yolact_r50_1x8_coco_20200908-f38d58df.pth) |
-| 550        | 8x8      | Resnet50-FPN  | 42.5 | 28.4 | | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolact/yolact_r50_8x8_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/yolact/yolact_r50_8x8_coco_20200908-ca34f5db.pth) |
-| 550        | 1x8      | Resnet101-FPN | 33.5 | 30.4 | | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolact/yolact_r101_1x8_coco.py) | [model](https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/yolact/yolact_r101_1x8_coco_20200908-4cbe9101.pth) |
+| 550        | 1x8      | Resnet50-FPN  | 42.5 | 29.0 | | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolact/yolact_r50_1x8_coco.py) |[model](https://download.openmmlab.com/mmdetection/v2.0/yolact/yolact_r50_1x8_coco/yolact_r50_1x8_coco_20200908-f38d58df.pth) |
+| 550        | 8x8      | Resnet50-FPN  | 42.5 | 28.4 | | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolact/yolact_r50_8x8_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/yolact/yolact_r50_8x8_coco/yolact_r50_8x8_coco_20200908-ca34f5db.pth) |
+| 550        | 1x8      | Resnet101-FPN | 33.5 | 30.4 | | [config](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolact/yolact_r101_1x8_coco.py) | [model](https://download.openmmlab.com/mmdetection/v2.0/yolact/yolact_r101_1x8_coco/yolact_r101_1x8_coco_20200908-4cbe9101.pth) |
 
 *Note: The FPS is evaluated by the [original implementation](https://github.com/dbolya/yolact). When calculating FPS, only the model inference time is taken into account. Data loading and post-processing operations such as converting masks to RLE code, generating COCO JSON results, image rendering are not included.
 
diff --git a/configs/yolact/metafile.yml b/configs/yolact/metafile.yml
index 0baa3faf..e7019ae6 100644
--- a/configs/yolact/metafile.yml
+++ b/configs/yolact/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Architecture:
         - FPN
         - ResNet
-    Paper: https://arxiv.org/abs/1904.02689
+    Paper:
+      URL: https://arxiv.org/abs/1904.02689
+      Title: 'YOLACT: Real-time Instance Segmentation'
     README: configs/yolact/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.5.0/mmdet/models/detectors/yolact.py#L9
+      Version: v2.5.0
 
 Models:
   - Name: yolact_r50_1x8_coco
@@ -31,7 +36,7 @@ Models:
         Dataset: COCO
         Metrics:
           mask AP: 29.0
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/yolact/yolact_r50_1x8_coco_20200908-f38d58df.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/yolact/yolact_r50_1x8_coco/yolact_r50_1x8_coco_20200908-f38d58df.pth
 
   - Name: yolact_r50_8x8_coco
     In Collection: YOLACT
@@ -50,7 +55,7 @@ Models:
         Dataset: COCO
         Metrics:
           mask AP: 28.4
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/yolact/yolact_r50_8x8_coco_20200908-ca34f5db.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/yolact/yolact_r50_8x8_coco/yolact_r50_8x8_coco_20200908-ca34f5db.pth
 
   - Name: yolact_r101_1x8_coco
     In Collection: YOLACT
@@ -70,4 +75,4 @@ Models:
         Dataset: COCO
         Metrics:
           mask AP: 30.4
-    Weights: https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/yolact/yolact_r101_1x8_coco_20200908-4cbe9101.pth
+    Weights: https://download.openmmlab.com/mmdetection/v2.0/yolact/yolact_r101_1x8_coco/yolact_r101_1x8_coco_20200908-4cbe9101.pth
diff --git a/configs/yolo/metafile.yml b/configs/yolo/metafile.yml
index 98150209..22c35da5 100644
--- a/configs/yolo/metafile.yml
+++ b/configs/yolo/metafile.yml
@@ -8,8 +8,13 @@ Collections:
       Training Resources: 8x V100 GPUs
       Architecture:
         - DarkNet
-    Paper: https://arxiv.org/abs/1804.02767
+    Paper:
+      URL: https://arxiv.org/abs/1804.02767
+      Title: 'YOLOv3: An Incremental Improvement'
     README: configs/yolo/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.4.0/mmdet/models/detectors/yolo.py#L8
+      Version: v2.4.0
 
 Models:
   - Name: yolov3_d53_320_273e_coco
diff --git a/configs/yolof/metafile.yml b/configs/yolof/metafile.yml
index f2b22921..9436fee2 100644
--- a/configs/yolof/metafile.yml
+++ b/configs/yolof/metafile.yml
@@ -9,8 +9,13 @@ Collections:
       Architecture:
         - Dilated Encoder
         - ResNet
-    Paper: https://arxiv.org/abs/2103.09460
+    Paper:
+      URL: https://arxiv.org/abs/2103.09460
+      Title: 'You Only Look One-level Feature'
     README: configs/yolof/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.12.0/mmdet/models/detectors/yolof.py#L6
+      Version: v2.12.0
 
 Models:
   - Name: yolof_r50_c5_8x8_1x_coco
diff --git a/configs/yolox/metafile.yml b/configs/yolox/metafile.yml
index 6a639fc5..22ad6cc1 100644
--- a/configs/yolox/metafile.yml
+++ b/configs/yolox/metafile.yml
@@ -10,8 +10,13 @@ Collections:
       Architecture:
         - CSPDarkNet
         - PAFPN
-    Paper: https://arxiv.org/abs/2107.08430
+    Paper:
+      URL: https://arxiv.org/abs/2107.08430
+      Title: 'YOLOX: Exceeding YOLO Series in 2021'
     README: configs/yolox/README.md
+    Code:
+      URL: https://github.com/open-mmlab/mmdetection/blob/v2.15.1/mmdet/models/detectors/yolox.py#L6
+      Version: v2.15.1
 
 Models:
   - Name: yolox_tiny_8x8_300e_coco
diff --git a/configs/yolox/yolox_s_8x8_300e_coco.py b/configs/yolox/yolox_s_8x8_300e_coco.py
index 71374953..c414051b 100644
--- a/configs/yolox/yolox_s_8x8_300e_coco.py
+++ b/configs/yolox/yolox_s_8x8_300e_coco.py
@@ -129,7 +129,6 @@ custom_hooks = [
         type='SyncRandomSizeHook',
         ratio_range=(14, 26),
         img_scale=img_scale,
-        interval=interval,
         priority=48),
     dict(
         type='SyncNormHook',
diff --git a/configs/yolox/yolox_tiny_8x8_300e_coco.py b/configs/yolox/yolox_tiny_8x8_300e_coco.py
index 4d517cbc..292bae70 100644
--- a/configs/yolox/yolox_tiny_8x8_300e_coco.py
+++ b/configs/yolox/yolox_tiny_8x8_300e_coco.py
@@ -66,7 +66,6 @@ custom_hooks = [
         type='SyncRandomSizeHook',
         ratio_range=(10, 20),
         img_scale=img_scale,
-        interval=interval,
         priority=48),
     dict(
         type='SyncNormHook',
diff --git a/demo/create_result_gif.py b/demo/create_result_gif.py
index 6646c6b3..d2356e90 100644
--- a/demo/create_result_gif.py
+++ b/demo/create_result_gif.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
diff --git a/demo/image_demo.py b/demo/image_demo.py
index 95de4fd4..606cbe59 100644
--- a/demo/image_demo.py
+++ b/demo/image_demo.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import asyncio
 from argparse import ArgumentParser
 
diff --git a/demo/video_demo.py b/demo/video_demo.py
index 661130b4..4ee1fa67 100644
--- a/demo/video_demo.py
+++ b/demo/video_demo.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
 import cv2
diff --git a/demo/webcam_demo.py b/demo/webcam_demo.py
index 5bded14f..b9ead6e0 100644
--- a/demo/webcam_demo.py
+++ b/demo/webcam_demo.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
 import cv2
diff --git a/docker/Dockerfile b/docker/Dockerfile
index 8a2f1d69..2ebbf76d 100644
--- a/docker/Dockerfile
+++ b/docker/Dockerfile
@@ -13,7 +13,7 @@ RUN apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build
     && rm -rf /var/lib/apt/lists/*
 
 # Install MMCV
-RUN pip install mmcv-full==1.3.8 -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.html
+RUN pip install mmcv-full==1.3.14 -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.html
 
 # Install MMDetection
 RUN conda clean --all
diff --git a/docker/serve/Dockerfile b/docker/serve/Dockerfile
index ff73c759..6299893e 100644
--- a/docker/serve/Dockerfile
+++ b/docker/serve/Dockerfile
@@ -3,8 +3,8 @@ ARG CUDA="10.1"
 ARG CUDNN="7"
 FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel
 
-ARG MMCV="1.3.8"
-ARG MMDET="2.15.1"
+ARG MMCV="1.3.14"
+ARG MMDET="2.17.0"
 
 ENV PYTHONUNBUFFERED TRUE
 
diff --git a/docs/1_exist_data_model.md b/docs/1_exist_data_model.md
index 4c758cc1..1e61a287 100644
--- a/docs/1_exist_data_model.md
+++ b/docs/1_exist_data_model.md
@@ -211,6 +211,22 @@ mmdetection
 │   │   ├── stuffthingmaps
 ```
 
+Panoptic segmentation models like PanopticFPN require additional [COCO Panoptic](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip) datasets, you can download and unzip then move to the coco annotation folder. The directory should be like this.
+
+```text
+mmdetection
+├── data
+│   ├── coco
+│   │   ├── annotations
+│   │   │   ├── panoptic_train2017.json
+│   │   │   ├── panoptic_train2017
+│   │   │   ├── panoptic_val2017.json
+│   │   │   ├── panoptic_val2017
+│   │   ├── train2017
+│   │   ├── val2017
+│   │   ├── test2017
+```
+
 The [cityscapes](https://www.cityscapes-dataset.com/) annotations need to be converted into the coco format using `tools/dataset_converters/cityscapes.py`:
 
 ```shell
diff --git a/docs/_static/css/readthedocs.css b/docs/_static/css/readthedocs.css
new file mode 100644
index 00000000..57ed0ad0
--- /dev/null
+++ b/docs/_static/css/readthedocs.css
@@ -0,0 +1,6 @@
+.header-logo {
+    background-image: url("../image/mmdet-logo.png");
+    background-size: 156px 40px;
+    height: 40px;
+    width: 156px;
+}
diff --git a/docs/_static/image/mmdet-logo.png b/docs/_static/image/mmdet-logo.png
new file mode 100644
index 00000000..58e2b5e6
Binary files /dev/null and b/docs/_static/image/mmdet-logo.png differ
diff --git a/docs/api.rst b/docs/api.rst
index a0b8586c..e61c663d 100644
--- a/docs/api.rst
+++ b/docs/api.rst
@@ -1,6 +1,3 @@
-API Reference
-=================
-
 mmdet.apis
 --------------
 .. automodule:: mmdet.apis
diff --git a/docs/changelog.md b/docs/changelog.md
index c6826a71..af629d17 100644
--- a/docs/changelog.md
+++ b/docs/changelog.md
@@ -1,5 +1,111 @@
 ## Changelog
 
+### v2.17.0 (28/9/2021)
+
+#### Highlights
+
+- Support [PVT](https://arxiv.org/abs/2102.12122) and [PVTv2](https://arxiv.org/abs/2106.13797)
+- Support [SOLO](https://arxiv.org/abs/1912.04488)
+- Support large scale jittering and New Mask R-CNN baselines
+- Speed up `YOLOv3` inference
+
+#### New Features
+
+- Support [PVT](https://arxiv.org/abs/2102.12122) and [PVTv2](https://arxiv.org/abs/2106.13797) (#5780)
+- Support [SOLO](https://arxiv.org/abs/1912.04488) (#5832)
+- Support large scale jittering and New Mask R-CNN baselines (#6132)
+- Add a general data structrue for the results of models (#5508)
+- Added a base class for one-stage instance segmentation (#5904)
+- Speed up `YOLOv3` inference (#5991)
+- Release Swin Transformer pre-trained models (#6100)
+- Support mixed precision training in `YOLOX` (#5983)
+- Support `val` workflow in `YOLACT` (#5986)
+- Add script to test `torchserve` (#5936)
+- Support `onnxsim` with dynamic input shape (#6117)
+
+#### Bug Fixes
+
+- Fix the function naming errors in `model_wrappers` (#5975)
+- Fix regression loss bug when the input is an empty tensor (#5976)
+- Fix scores not contiguous error in `centernet_head` (#6016)
+- Fix missing parameters bug in `imshow_bboxes` (#6034)
+- Fix bug in `aug_test` of `HTC` when the length of `det_bboxes` is 0 (#6088)
+- Fix empty proposal errors in the training of some two-stage models (#5941)
+- Fix `dynamic_axes` parameter error in `ONNX` dynamic shape export (#6104)
+- Fix `dynamic_shape` bug of `SyncRandomSizeHook` (#6144)
+- Fix the Swin Transformer config link error in the configuration (#6172)
+
+#### Improvements
+
+- Add filter rules in `Mosaic` transform (#5897)
+- Add size divisor in get flops to avoid some potential bugs (#6076)
+- Add Chinese translation of `docs_zh-CN/tutorials/customize_dataset.md` (#5915)
+- Add Chinese translation of `conventions.md` (#5825)
+- Add description of the output of data pipeline (#5886)
+- Add dataset information in the README file for `PanopticFPN` (#5996)
+- Add `extra_repr` for `DropBlock` layer to get details in the model printing (#6140)
+- Fix CI out of memory and add PyTorch1.9 Python3.9 unit tests (#5862)
+- Fix download links error of some model (#6069)
+- Improve the generalization of XML dataset (#5943)
+- Polish assertion error messages (#6017)
+- Remove `opencv-python-headless` dependency by `albumentations` (#5868)
+- Check dtype in transform unit tests (#5969)
+- Replace the default theme of documentation with PyTorch Sphinx Theme (#6146)
+- Update the paper and code fields in the metafile (#6043)
+- Support to customize padding value of segmentation map (#6152)
+- Support to resize multiple segmentation maps (#5747)
+
+#### Contributors
+A total of 24 developers contributed to this release.
+Thanks @morkovka1337, @HarborYuan, @guillaumefrd, @guigarfr, @www516717402, @gaotongxiao, @ypwhs, @MartaYang, @shinya7y, @justiceeem, @zhaojinjian0000, @VVsssssk, @aravind-anantha, @wangbo-zhao, @czczup, @whai362, @czczup, @marijnl, @AronLin, @BIGWangYuDong, @hhaAndroid, @jshilong, @RangiLyu, @ZwwWayne
+
+### v2.16.0 (30/8/2021)
+
+#### Highlights
+
+- Support [Panoptic FPN](https://arxiv.org/abs/1901.02446) and [Swin Transformer](https://arxiv.org/abs/2103.14030)
+
+#### New Features
+
+- Support [Panoptic FPN](https://arxiv.org/abs/1901.02446) and release models (#5577, #5902)
+- Support Swin Transformer backbone (#5748)
+- Release RetinaNet models pre-trained with multi-scale 3x schedule (#5636)
+- Add script to convert unlabeled image list to coco format (#5643)
+- Add hook to check whether the loss value is valid (#5674)
+- Add YOLO anchor optimizing tool (#5644)
+- Support export onnx models without post process. (#5851)
+- Support classwise evaluation in CocoPanopticDataset (#5896)
+- Adapt browse_dataset for concatenated datasets. (#5935)
+- Add `PatchEmbed` and `PatchMerging` with `AdaptivePadding` (#5952)
+
+#### Bug Fixes
+
+- Fix unit tests of YOLOX (#5859)
+- Fix lose randomness in `imshow_det_bboxes` (#5845)
+- Make output result of `ImageToTensor` contiguous (#5756)
+- Fix inference bug when calling `regress_by_class` in RoIHead in some cases (#5884)
+- Fix bug in CIoU loss where alpha should not have gradient. (#5835)
+- Fix the bug that `multiscale_output` is defined but not used in HRNet (#5887)
+- Set the priority of EvalHook to LOW. (#5882)
+- Fix a YOLOX bug when applying bbox rescaling in test mode (#5899)
+- Fix mosaic coordinate error (#5947)
+- Fix dtype of bbox in RandomAffine. (#5930)
+
+#### Improvements
+
+- Add Chinese version of `data_pipeline` and  (#5662)
+- Support to remove state dicts of EMA when publishing models. (#5858)
+- Refactor the loss function in HTC and SCNet (#5881)
+- Use warnings instead of logger.warning (#5540)
+- Use legacy coordinate in metric of VOC (#5627)
+- Add Chinese version of customize_losses (#5826)
+- Add Chinese version of model_zoo (#5827)
+
+#### Contributors
+
+A total of 19 developers contributed to this release.
+Thanks @ypwhs, @zywvvd, @collinzrj, @OceanPang, @ddonatien, @@haotian-liu, @viibridges, @Muyun99, @guigarfr, @zhaojinjian0000, @jbwang1997,@wangbo-zhao, @xvjiarui, @RangiLyu, @jshilong, @AronLin, @BIGWangYuDong, @hhaAndroid, @ZwwWayne
+
 ### v2.15.1 (11/8/2021)
 
 #### Highlights
diff --git a/docs/conf.py b/docs/conf.py
index 34542680..ce126234 100644
--- a/docs/conf.py
+++ b/docs/conf.py
@@ -14,6 +14,8 @@ import os
 import subprocess
 import sys
 
+import pytorch_sphinx_theme
+
 sys.path.insert(0, os.path.abspath('..'))
 
 # -- Project information -----------------------------------------------------
@@ -44,6 +46,7 @@ extensions = [
     'sphinx.ext.viewcode',
     'recommonmark',
     'sphinx_markdown_tables',
+    'sphinx_copybutton',
 ]
 
 autodoc_mock_imports = [
@@ -74,12 +77,97 @@ exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']
 # The theme to use for HTML and HTML Help pages.  See the documentation for
 # a list of builtin themes.
 #
-html_theme = 'sphinx_rtd_theme'
+# html_theme = 'sphinx_rtd_theme'
+html_theme = 'pytorch_sphinx_theme'
+html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]
+
+html_theme_options = {
+    'menu': [
+        {
+            'name': 'GitHub',
+            'url': 'https://github.com/open-mmlab/mmcv'
+        },
+        {
+            'name':
+            'Projects',
+            'children': [
+                {
+                    'name': 'MMAction2',
+                    'url': 'https://github.com/open-mmlab/mmaction2',
+                },
+                {
+                    'name': 'MMClassification',
+                    'url': 'https://github.com/open-mmlab/mmclassification',
+                },
+                {
+                    'name': 'MMDetection',
+                    'url': 'https://github.com/open-mmlab/mmdetection',
+                },
+                {
+                    'name': 'MMDetection3D',
+                    'url': 'https://github.com/open-mmlab/mmdetection3d',
+                },
+                {
+                    'name': 'MMEditing',
+                    'url': 'https://github.com/open-mmlab/mmediting',
+                },
+                {
+                    'name': 'MMGeneration',
+                    'url': 'https://github.com/open-mmlab/mmgeneration',
+                },
+                {
+                    'name': 'MMOCR',
+                    'url': 'https://github.com/open-mmlab/mmocr',
+                },
+                {
+                    'name': 'MMPose',
+                    'url': 'https://github.com/open-mmlab/mmpose',
+                },
+                {
+                    'name': 'MMSegmentation',
+                    'url': 'https://github.com/open-mmlab/mmsegmentation',
+                },
+                {
+                    'name': 'MMTracking',
+                    'url': 'https://github.com/open-mmlab/mmtracking',
+                },
+            ]
+        },
+        {
+            'name':
+            'OpenMMLab',
+            'children': [
+                {
+                    'name': 'Homepage',
+                    'url': 'https://openmmlab.com/'
+                },
+                {
+                    'name': 'GitHub',
+                    'url': 'https://github.com/open-mmlab/'
+                },
+                {
+                    'name': 'Twitter',
+                    'url': 'https://twitter.com/OpenMMLab'
+                },
+                {
+                    'name': 'Zhihu',
+                    'url': 'https://zhihu.com/people/openmmlab'
+                },
+            ]
+        },
+    ]
+}
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
 html_static_path = ['_static']
+html_css_files = ['css/readthedocs.css']
+
+# -- Extension configuration -------------------------------------------------
+# Ignore >>> when copying code
+copybutton_prompt_text = r'>>> |\.\.\. '
+copybutton_prompt_is_regexp = True
 
 
 def builder_inited_handler(app):
diff --git a/docs/conventions.md b/docs/conventions.md
index ca52a6e6..8708c673 100644
--- a/docs/conventions.md
+++ b/docs/conventions.md
@@ -62,8 +62,17 @@ for i in range(self.num_stages):
                    if rois[j].shape[0] > 0:
                        bbox_label = cls_score[j][:, :-1].argmax(dim=1)
                        refine_roi = self.bbox_head[i].regress_by_class(
-                            rois[j], bbox_label[j], bbox_pred[j], img_metas[j])
+                            rois[j], bbox_label, bbox_pred[j], img_metas[j])
                        refine_roi_list.append(refine_roi)
 ```
 
 If you have customized `RoIHead`, you can refer to the above method to deal with empty proposals.
+
+## Coco Panoptic Dataset
+
+In MMDetection, we have supported COCO Panoptic dataset. We clarify a few conventions about the implementation of `CocoPanopticDataset` here.
+
+1. For mmdet<=2.16.0, the range of foreground and background labels in semantic segmentation are different from the default setting of MMDetection. The label `0` stands for `VOID` label and the category labels start from `1`.
+Since mmdet=2.17.0, the category labels of semantic segmentation start from `0` and label `255` stands for `VOID` for consistency with labels of bounding boxes.
+To achieve that, the `Pad` pipeline supports setting the padding value for `seg`.
+2. In the evaluation, the panoptic result is a map with the same shape as the original image. Each value in the result map has the format of `instance_id * INSTANCE_OFFSET + category_id`.
diff --git a/docs/get_started.md b/docs/get_started.md
index aa829328..b222338d 100644
--- a/docs/get_started.md
+++ b/docs/get_started.md
@@ -12,6 +12,8 @@ Compatible MMDetection and MMCV versions are shown as below. Please install the
 | MMDetection version |    MMCV version     |
 |:-------------------:|:-------------------:|
 | master              | mmcv-full>=1.3.8, <1.4.0 |
+| 2.17.0              | mmcv-full>=1.3.8, <1.4.0 |
+| 2.16.0              | mmcv-full>=1.3.8, <1.4.0 |
 | 2.15.1              | mmcv-full>=1.3.8, <1.4.0 |
 | 2.15.0              | mmcv-full>=1.3.8, <1.4.0 |
 | 2.14.0              | mmcv-full>=1.3.8, <1.4.0 |
@@ -117,7 +119,7 @@ Or you can still install MMDetection manually:
     pip install -v -e .  # or "python setup.py develop"
     ```
 
-3. Install extra dependencies for Instaboost, Panoptic Segmentation, or LVIS dataset
+3. Install extra dependencies for Instaboost, Panoptic Segmentation, LVIS dataset, or Albumentations.
 
     ```shell
     # for instaboost
@@ -126,6 +128,8 @@ Or you can still install MMDetection manually:
     pip install git+https://github.com/cocodataset/panopticapi.git
     # for LVIS dataset
     pip install git+https://github.com/lvis-dataset/lvis-api.git
+    # for albumentations
+    pip install albumentations>=0.3.2 --no-binary imgaug,albumentations
     ```
 
 **Note:**
@@ -139,6 +143,10 @@ you can install it before installing MMCV.
 c. Some dependencies are optional. Simply running `pip install -v -e .` will
  only install the minimum runtime requirements. To use optional dependencies like `albumentations` and `imagecorruptions` either install them manually with `pip install -r requirements/optional.txt` or specify desired extras when calling `pip` (e.g. `pip install -v -e .[optional]`). Valid keys for the extras field are: `all`, `tests`, `build`, and `optional`.
 
+d. If you would like to use `albumentations`, we suggest using
+`pip install albumentations>=0.3.2 --no-binary imgaug,albumentations`. If you simply use
+`pip install albumentations>=0.3.2`, it will install `opencv-python-headless` simultaneously (even though you have already installed `opencv-python`). We should not allow `opencv-python` and `opencv-python-headless` installed at the same time, because it might cause unexpected issues. Please refer to [official documentation](https://albumentations.ai/docs/getting_started/installation/#note-on-opencv-dependencies) for more details.
+
 ### Install without GPU support
 
 MMDetection can be built for CPU only environment (where CUDA isn't available).
diff --git a/docs/index.rst b/docs/index.rst
index 3271fb2b..c3406dad 100644
--- a/docs/index.rst
+++ b/docs/index.rst
@@ -45,6 +45,7 @@ Welcome to MMDetection's documentation!
    switch_language.md
 
 .. toctree::
+   :maxdepth: 1
    :caption: API Reference
 
    api.rst
diff --git a/docs/model_zoo.md b/docs/model_zoo.md
index 3756ce58..de3ca8c8 100644
--- a/docs/model_zoo.md
+++ b/docs/model_zoo.md
@@ -2,7 +2,7 @@
 
 ## Mirror sites
 
-We only use aliyun to maintain the model zoo since MMDetection V2.0. The model zoo of V1.x remains in AWS and will be deprecated in the future.
+We only use aliyun to maintain the model zoo since MMDetection V2.0. The model zoo of V1.x has been deprecated.
 
 ## Common settings
 
@@ -230,6 +230,14 @@ Please refer to [CenterNet](https://github.com/open-mmlab/mmdetection/blob/maste
 
 Please refer to [YOLOX](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolox) for details.
 
+### PVT
+
+Please refer to [PVT](https://github.com/open-mmlab/mmdetection/blob/master/configs/pvt) for details.
+
+### SOLO
+
+Please refer to [SOLO](https://github.com/open-mmlab/mmdetection/blob/master/configs/solo) for details.
+
 ### Other datasets
 
 We also benchmark some methods on [PASCAL VOC](https://github.com/open-mmlab/mmdetection/blob/master/configs/pascal_voc), [Cityscapes](https://github.com/open-mmlab/mmdetection/blob/master/configs/cityscapes) and [WIDER FACE](https://github.com/open-mmlab/mmdetection/blob/master/configs/wider_face).
diff --git a/docs/tutorials/robustness_benchmarking.md b/docs/robustness_benchmarking.md
similarity index 98%
rename from docs/tutorials/robustness_benchmarking.md
rename to docs/robustness_benchmarking.md
index 95ad0e2c..5be16dfa 100644
--- a/docs/tutorials/robustness_benchmarking.md
+++ b/docs/robustness_benchmarking.md
@@ -18,7 +18,7 @@ This page provides basic tutorials how to use the benchmark.
 }
 ```
 
-![image corruption example](../../resources/corruptions_sev_3.png)
+![image corruption example](../resources/corruptions_sev_3.png)
 
 ## About the benchmark
 
diff --git a/docs/tutorials/customize_dataset.md b/docs/tutorials/customize_dataset.md
index d1e956d4..921839f7 100644
--- a/docs/tutorials/customize_dataset.md
+++ b/docs/tutorials/customize_dataset.md
@@ -54,7 +54,7 @@ After the data pre-processing, there are two steps for users to train the custom
 1. Modify the config file for using the customized dataset.
 2. Check the annotations of the customized dataset.
 
-Here we give an example to show the above two steps, which uses a customized dataset of 5 classes with COCO format to train an existing Cascade MaskRCNN R50 FPN detector.
+Here we give an example to show the above two steps, which uses a customized dataset of 5 classes with COCO format to train an existing Cascade Mask R-CNN R50-FPN detector.
 
 #### 1. Modify the config file for using the customized dataset
 
@@ -485,3 +485,58 @@ data = dict(
 - Since the middle format only has box labels and does not contain the class names, when using `CustomDataset`, users cannot filter out the empty GT images through configs but only do this offline.
 - Please remember to modify the `num_classes` in the head when specifying `classes` in dataset. We implemented [NumClassCheckHook](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/utils.py) to check whether the numbers are consistent since v2.9.0(after PR#4508).
 - The features for setting dataset classes and dataset filtering will be refactored to be more user-friendly in the future (depends on the progress).
+
+## COCO Panoptic Dataset
+
+Now we support COCO Panoptic Dataset, the format of panoptic annotations is different from COCO format.
+Both the foreground and the background will exist in the annotation file.
+The annotation json files in COCO Panoptic format has the following necessary keys:
+
+```python
+'images': [
+    {
+        'file_name': '000000001268.jpg',
+        'height': 427,
+        'width': 640,
+        'id': 1268
+    },
+    ...
+]
+
+'annotations': [
+    {
+        'filename': '000000001268.jpg',
+        'image_id': 1268,
+        'segments_info': [
+            {
+                'id':8345037,  # One-to-one correspondence with the id in the annotation map.
+                'category_id': 51,
+                'iscrowd': 0,
+                'bbox': (x1, y1, w, h),  # The bbox of the background is the outer rectangle of its mask.
+                'area': 24315
+            },
+            ...
+        ]
+    },
+    ...
+]
+
+'categories': [  # including both foreground categories and background categories
+    {'id': 0, 'name': 'person'},
+    ...
+ ]
+```
+
+Moreover, the `seg_prefix` must be set to the path of the panoptic annotation images.
+
+```python
+data = dict(
+    type='CocoPanopticDataset',
+    train=dict(
+        seg_prefix = 'path/to/your/train/panoptic/image_annotation_data'
+    ),
+    val=dict(
+        seg_prefix = 'path/to/your/train/panoptic/image_annotation_data'
+    )
+)
+```
diff --git a/docs/tutorials/customize_losses.md b/docs/tutorials/customize_losses.md
index c3e1ddd8..9e4cb5e7 100644
--- a/docs/tutorials/customize_losses.md
+++ b/docs/tutorials/customize_losses.md
@@ -8,7 +8,7 @@ This tutorial first elaborate the computation pipeline of losses, then give some
 
 Given the input prediction and target, as well as the weights, a loss function maps the input tensor to the final loss scalar. The mapping can be divided into four steps:
 
-1. Get **element-wise** or sample-wise loss by the loss kernel function.
+1. Get **element-wise** or **sample-wise** loss by the loss kernel function.
 
 2. Weighting the loss with a weight tensor **element-wisely**.
 
diff --git a/docs/tutorials/data_pipeline.md b/docs/tutorials/data_pipeline.md
index 7ea5665f..9069ea14 100644
--- a/docs/tutorials/data_pipeline.md
+++ b/docs/tutorials/data_pipeline.md
@@ -146,28 +146,36 @@ For each operation, we list the related dict fields that are added/updated/remov
 
 ## Extend and use custom pipelines
 
-1. Write a new pipeline in any file, e.g., `my_pipeline.py`. It takes a dict as input and return a dict.
+1. Write a new pipeline in a file, e.g., in `my_pipeline.py`. It takes a dict as input and returns a dict.
 
     ```python
+    import random
     from mmdet.datasets import PIPELINES
 
+
     @PIPELINES.register_module()
     class MyTransform:
+        """Add your transform
+
+        Args:
+            p (float): Probability of shifts. Default 0.5.
+        """
+
+        def __init__(self, p=0.5):
+            self.p = p
 
         def __call__(self, results):
-            results['dummy'] = True
+            if random.random() > self.p:
+                results['dummy'] = True
             return results
     ```
 
-2. Import the new class.
+2. Import and use the pipeline in your config file.
+   Make sure the import is relative to where your train script is located.
 
     ```python
-    from .my_pipeline import MyTransform
-    ```
+    custom_imports = dict(imports=['path.to.my_pipeline'], allow_failed_imports=False)
 
-3. Use it in config files.
-
-    ```python
     img_norm_cfg = dict(
         mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
     train_pipeline = [
@@ -177,8 +185,15 @@ For each operation, we list the related dict fields that are added/updated/remov
         dict(type='RandomFlip', flip_ratio=0.5),
         dict(type='Normalize', **img_norm_cfg),
         dict(type='Pad', size_divisor=32),
-        dict(type='MyTransform'),
+        dict(type='MyTransform', p=0.2),
         dict(type='DefaultFormatBundle'),
         dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
     ]
     ```
+
+3. Visualize the output of your augmentation pipeline
+
+   To visualize the output of your agmentation pipeline, `tools/misc/browse_dataset.py`
+   can help the user to browse a detection dataset (both images and bounding box annotations)
+   visually, or save the image to a designated directory. More detials can refer to
+   [useful_tools](../useful_tools.md)
diff --git a/docs/tutorials/pytorch2onnx.md b/docs/tutorials/pytorch2onnx.md
index c76bc603..36ec0000 100644
--- a/docs/tutorials/pytorch2onnx.md
+++ b/docs/tutorials/pytorch2onnx.md
@@ -59,6 +59,7 @@ python tools/deployment/pytorch2onnx.py \
 - `--verify`: Determines whether to verify the correctness of an exported model. If not specified, it will be set to `False`.
 - `--simplify`: Determines whether to simplify the exported ONNX model. If not specified, it will be set to `False`.
 - `--cfg-options`: Override some settings in the used config file, the key-value pair in `xxx=yyy` format will be merged into config file.
+- `--skip-postprocess`: Determines whether export model without post process. If not specified, it will be set to `False`. Notice: This is an experimental option. Only work for some single stage models. Users need to implement the post-process by themselves. We do not guarantee the correctness of the exported model.
 
 Example:
 
diff --git a/docs/useful_tools.md b/docs/useful_tools.md
index 2306e26f..a9472402 100644
--- a/docs/useful_tools.md
+++ b/docs/useful_tools.md
@@ -188,7 +188,7 @@ python tools/deployment/mmdet2torchserve.py ${CONFIG_FILE} ${CHECKPOINT_FILE} \
 --model-name ${MODEL_NAME}
 ```
 
-***Note**: ${MODEL_STORE} needs to be an absolute path to a folder.
+**Note**: ${MODEL_STORE} needs to be an absolute path to a folder.
 
 ### 2. Build `mmdet-serve` docker image
 
@@ -227,35 +227,55 @@ You should obtain a respose similar to:
 ```json
 [
   {
-    "dog": [
-      402.9117736816406,
-      124.19664001464844,
-      571.7910766601562,
-      292.6463623046875
+    "class_name": "dog",
+    "bbox": [
+      294.63409423828125,
+      203.99111938476562,
+      417.048583984375,
+      281.62744140625
     ],
-    "score": 0.9561963081359863
+    "score": 0.9987992644309998
   },
   {
-    "dog": [
-      293.90057373046875,
-      196.2908477783203,
-      417.4869079589844,
-      286.2522277832031
+    "class_name": "dog",
+    "bbox": [
+      404.26019287109375,
+      126.0080795288086,
+      574.5091552734375,
+      293.6662292480469
     ],
-    "score": 0.9179860353469849
+    "score": 0.9979367256164551
   },
   {
-    "dog": [
-      202.178466796875,
-      86.3709487915039,
-      311.9863586425781,
-      276.28411865234375
+    "class_name": "dog",
+    "bbox": [
+      197.2144775390625,
+      93.3067855834961,
+      307.8505554199219,
+      276.7560119628906
     ],
-    "score": 0.8933767080307007
+    "score": 0.993338406085968
   }
 ]
 ```
 
+And you can use `test_torchserver.py` to compare result of torchserver and pytorch, and visualize them.
+
+```shell
+python tools/deployment/test_torchserver.py ${IMAGE_FILE} ${CONFIG_FILE} ${CHECKPOINT_FILE} ${MODEL_NAME}
+[--inference-addr ${INFERENCE_ADDR}] [--device ${DEVICE}] [--score-thr ${SCORE_THR}]
+```
+
+Example:
+
+```shell
+python tools/deployment/test_torchserver.py \
+demo/demo.jpg \
+configs/yolo/yolov3_d53_320_273e_coco.py \
+checkpoint/yolov3_d53_320_273e_coco-421362b6.pth \
+yolov3
+```
+
 ## Model Complexity
 
 `tools/analysis_tools/get_flops.py` is a script adapted from [flops-counter.pytorch](https://github.com/sovrasov/flops-counter.pytorch) to compute the FLOPs and params of a given model.
@@ -382,3 +402,48 @@ python tools/analysis_tools/eval_metric.py ${CONFIG} ${PKL_RESULTS} [-h] [--form
 ```shell
 python tools/misc/print_config.py ${CONFIG} [-h] [--options ${OPTIONS [OPTIONS...]}]
 ```
+
+## Hyper-parameter Optimization
+
+### YOLO Anchor Optimization
+
+`tools/analysis_tools/optimize_anchors.py` provides two method to optimize YOLO anchors.
+
+One is k-means anchor cluster which refers from [darknet](https://github.com/AlexeyAB/darknet/blob/master/src/detector.c#L1421).
+
+
+```shell
+python tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm k-means --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir ${OUTPUT_DIR}
+```
+
+Another is using differential evolution to optimize anchors.
+
+```shell
+python tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm differential_evolution --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir ${OUTPUT_DIR}
+```
+
+E.g.,
+
+```shell
+python tools/analysis_tools/optimize_anchors.py configs/yolo/yolov3_d53_320_273e_coco.py --algorithm differential_evolution --input-shape 608 608 --device cuda --output-dir work_dirs
+```
+
+You will get:
+```
+loading annotations into memory...
+Done (t=9.70s)
+creating index...
+index created!
+2021-07-19 19:37:20,951 - mmdet - INFO - Collecting bboxes from annotation...
+[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 117266/117266, 15874.5 task/s, elapsed: 7s, ETA:     0s
+
+2021-07-19 19:37:28,753 - mmdet - INFO - Collected 849902 bboxes.
+differential_evolution step 1: f(x)= 0.506055
+differential_evolution step 2: f(x)= 0.506055
+......
+
+differential_evolution step 489: f(x)= 0.386625
+2021-07-19 19:46:40,775 - mmdet - INFO Anchor evolution finish. Average IOU: 0.6133754253387451
+2021-07-19 19:46:40,776 - mmdet - INFO Anchor differential evolution result:[[10, 12], [15, 30], [32, 22], [29, 59], [61, 46], [57, 116], [112, 89], [154, 198], [349, 336]]
+2021-07-19 19:46:40,798 - mmdet - INFO Result saved in work_dirs/anchor_optimize_result.json
+```
diff --git a/docs_zh-CN/1_exist_data_model.md b/docs_zh-CN/1_exist_data_model.md
index 2f87b7ca..a63dfe2b 100644
--- a/docs_zh-CN/1_exist_data_model.md
+++ b/docs_zh-CN/1_exist_data_model.md
@@ -189,7 +189,7 @@ mmdetection
 │   │   ├── VOC2012
 ```
 
-有些模型需要额外的 COCO-stuff 数据集，比如 HTC，DetectoRS 和 SCNet，你可以下载并解压它们到 `coco` 文件夹下。文件夹会是如下结构：
+有些模型需要额外的 [COCO-stuff](http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip) 数据集，比如 HTC，DetectoRS 和 SCNet，你可以下载并解压它们到 `coco` 文件夹下。文件夹会是如下结构：
 
 ```plain
 mmdetection
@@ -202,6 +202,22 @@ mmdetection
 │   │   ├── stuffthingmaps
 ```
 
+PanopticFPN 等全景分割模型需要额外的 [COCO Panoptic](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip) 数据集，你可以下载并解压它们到 `coco/annotations` 文件夹下。文件夹会是如下结构：
+
+```text
+mmdetection
+├── data
+│   ├── coco
+│   │   ├── annotations
+│   │   │   ├── panoptic_train2017.json
+│   │   │   ├── panoptic_train2017
+│   │   │   ├── panoptic_val2017.json
+│   │   │   ├── panoptic_val2017
+│   │   ├── train2017
+│   │   ├── val2017
+│   │   ├── test2017
+```
+
 Cityscape 数据集的标注格式需要转换，以与 COCO 数据集标注格式保持一致，使用 `tools/dataset_converters/cityscapes.py` 来完成转换：
 
 ```shell
diff --git a/docs_zh-CN/_static/css/readthedocs.css b/docs_zh-CN/_static/css/readthedocs.css
new file mode 100644
index 00000000..57ed0ad0
--- /dev/null
+++ b/docs_zh-CN/_static/css/readthedocs.css
@@ -0,0 +1,6 @@
+.header-logo {
+    background-image: url("../image/mmdet-logo.png");
+    background-size: 156px 40px;
+    height: 40px;
+    width: 156px;
+}
diff --git a/docs_zh-CN/_static/image/mmdet-logo.png b/docs_zh-CN/_static/image/mmdet-logo.png
new file mode 100644
index 00000000..58e2b5e6
Binary files /dev/null and b/docs_zh-CN/_static/image/mmdet-logo.png differ
diff --git a/docs_zh-CN/api.rst b/docs_zh-CN/api.rst
index ab8ee89d..c75a467e 100644
--- a/docs_zh-CN/api.rst
+++ b/docs_zh-CN/api.rst
@@ -1,6 +1,3 @@
-API Reference
-=================
-
 mmdet.apis
 --------------
 .. automodule:: mmdet.apis
diff --git a/docs_zh-CN/conf.py b/docs_zh-CN/conf.py
index 61a886c1..69a7dd5b 100644
--- a/docs_zh-CN/conf.py
+++ b/docs_zh-CN/conf.py
@@ -14,6 +14,8 @@ import os
 import subprocess
 import sys
 
+import pytorch_sphinx_theme
+
 sys.path.insert(0, os.path.abspath('..'))
 
 # -- Project information -----------------------------------------------------
@@ -44,6 +46,7 @@ extensions = [
     'sphinx.ext.viewcode',
     'recommonmark',
     'sphinx_markdown_tables',
+    'sphinx_copybutton',
 ]
 
 autodoc_mock_imports = [
@@ -74,15 +77,100 @@ exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']
 # The theme to use for HTML and HTML Help pages.  See the documentation for
 # a list of builtin themes.
 #
-html_theme = 'sphinx_rtd_theme'
+# html_theme = 'sphinx_rtd_theme'
+html_theme = 'pytorch_sphinx_theme'
+html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]
+
+html_theme_options = {
+    'menu': [
+        {
+            'name': 'GitHub',
+            'url': 'https://github.com/open-mmlab/mmcv'
+        },
+        {
+            'name':
+            '算法库',
+            'children': [
+                {
+                    'name': 'MMAction2',
+                    'url': 'https://github.com/open-mmlab/mmaction2',
+                },
+                {
+                    'name': 'MMClassification',
+                    'url': 'https://github.com/open-mmlab/mmclassification',
+                },
+                {
+                    'name': 'MMDetection',
+                    'url': 'https://github.com/open-mmlab/mmdetection',
+                },
+                {
+                    'name': 'MMDetection3D',
+                    'url': 'https://github.com/open-mmlab/mmdetection3d',
+                },
+                {
+                    'name': 'MMEditing',
+                    'url': 'https://github.com/open-mmlab/mmediting',
+                },
+                {
+                    'name': 'MMGeneration',
+                    'url': 'https://github.com/open-mmlab/mmgeneration',
+                },
+                {
+                    'name': 'MMOCR',
+                    'url': 'https://github.com/open-mmlab/mmocr',
+                },
+                {
+                    'name': 'MMPose',
+                    'url': 'https://github.com/open-mmlab/mmpose',
+                },
+                {
+                    'name': 'MMSegmentation',
+                    'url': 'https://github.com/open-mmlab/mmsegmentation',
+                },
+                {
+                    'name': 'MMTracking',
+                    'url': 'https://github.com/open-mmlab/mmtracking',
+                },
+            ]
+        },
+        {
+            'name':
+            'OpenMMLab',
+            'children': [
+                {
+                    'name': '官网',
+                    'url': 'https://openmmlab.com/'
+                },
+                {
+                    'name': 'GitHub',
+                    'url': 'https://github.com/open-mmlab/'
+                },
+                {
+                    'name': '推特',
+                    'url': 'https://twitter.com/OpenMMLab'
+                },
+                {
+                    'name': '知乎',
+                    'url': 'https://zhihu.com/people/openmmlab'
+                },
+            ]
+        },
+    ]
+}
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
 html_static_path = ['_static']
+html_css_files = ['css/readthedocs.css']
 
 language = 'zh_CN'
 
+# -- Extension configuration -------------------------------------------------
+# Ignore >>> when copying code
+copybutton_prompt_text = r'>>> |\.\.\. '
+copybutton_prompt_is_regexp = True
+
 
 def builder_inited_handler(app):
     subprocess.run(['./stat.py'])
diff --git a/docs_zh-CN/conventions.md b/docs_zh-CN/conventions.md
index 72b132bf..cae9206f 100644
--- a/docs_zh-CN/conventions.md
+++ b/docs_zh-CN/conventions.md
@@ -1 +1,74 @@
 # 默认约定
+
+如果你想把 MMDetection 修改为自己的项目，请遵循下面的约定。
+
+## 损失
+
+在 MMDetection 中，`model(**data)` 的返回值是一个字典，包含着所有的损失和评价指标，他们将会由 `model(**data)` 返回。
+
+例如，在 bbox head 中，
+
+```python
+class BBoxHead(nn.Module):
+    ...
+    def loss(self, ...):
+        losses = dict()
+        # 分类损失
+        losses['loss_cls'] = self.loss_cls(...)
+        # 分类准确率
+        losses['acc'] = accuracy(...)
+        # 边界框损失
+        losses['loss_bbox'] = self.loss_bbox(...)
+        return losses
+```
+
+`'bbox_head.loss()'` 在模型 forward 阶段会被调用。返回的字典中包含了 `'loss_bbox'`,`'loss_cls'`,`'acc'`。只有 `'loss_bbox'`, `'loss_cls'` 会被用于反向传播，`'acc'` 只会被作为评价指标来监控训练过程。
+
+我们默认，只有那些键的名称中包含 `'loss'` 的值会被用于反向传播。这个行为可以通过修改 `BaseDetector.train_step()` 来改变。
+
+## 空 proposals
+
+在 MMDetection 中，我们为两阶段方法中空 proposals 的情况增加了特殊处理和单元测试。我们同时需要处理整个 batch 和单一图片中空 proposals 的情况。例如，在 CascadeRoIHead 中，
+
+```python
+# 简单的测试
+...
+
+# 在整个 batch中 都没有 proposals
+if rois.shape[0] == 0:
+    bbox_results = [[
+        np.zeros((0, 5), dtype=np.float32)
+        for _ in range(self.bbox_head[-1].num_classes)
+    ]] * num_imgs
+    if self.with_mask:
+        mask_classes = self.mask_head[-1].num_classes
+        segm_results = [[[] for _ in range(mask_classes)]
+                        for _ in range(num_imgs)]
+        results = list(zip(bbox_results, segm_results))
+    else:
+        results = bbox_results
+    return results
+...
+
+# 在单张图片中没有 proposals
+for i in range(self.num_stages):
+    ...
+    if i < self.num_stages - 1:
+          for j in range(num_imgs):
+                   # 处理空 proposals
+                   if rois[j].shape[0] > 0:
+                       bbox_label = cls_score[j][:, :-1].argmax(dim=1)
+                       refine_roi = self.bbox_head[i].regress_by_class(
+                            rois[j], bbox_label[j], bbox_pred[j], img_metas[j])
+                       refine_roi_list.append(refine_roi)
+```
+如果你有自定义的 `RoIHead`, 你可以参考上面的方法来处理空 proposals 的情况。
+
+## 全景分割数据集
+
+在 MMDetection 中，我们支持了 COCO 全景分割数据集 `CocoPanopticDataset`。对于它的实现，我们在这里声明一些默认约定。
+
+1. 在 mmdet<=2.16.0 时，语义分割标注中的前景和背景标签范围与 MMDetection 中的默认规定有所不同。标签 `0` 代表 `VOID` 标签。
+从 mmdet=2.17.0 开始，为了和框的类别标注保持一致，语义分割标注的类别标签也改为从 `0` 开始，标签 `255` 代表 `VOID` 类。
+为了达成这一目标，我们在流程 `Pad` 里支持了设置 `seg` 的填充值的功能。
+2. 在评估中，全景分割结果必须是一个与原图大小相同的图。结果图中每个像素的值有如此形式：`instance_id * INSTANCE_OFFSET + category_id`。
diff --git a/docs_zh-CN/get_started.md b/docs_zh-CN/get_started.md
index 96b746c1..34b865c3 100644
--- a/docs_zh-CN/get_started.md
+++ b/docs_zh-CN/get_started.md
@@ -131,6 +131,8 @@ MIM 能够自动地安装 OpenMMLab 的项目以及对应的依赖包。
     pip install git+https://github.com/cocodataset/panopticapi.git
     # 安装 LVIS 数据集依赖
     pip install git+https://github.com/lvis-dataset/lvis-api.git
+    # 安装 albumentations 依赖
+    pip install albumentations>=0.3.2 --no-binary imgaug,albumentations
     ```
 
 **注意：**
@@ -141,6 +143,8 @@ MIM 能够自动地安装 OpenMMLab 的项目以及对应的依赖包。
 
 (3) 一些安装依赖是可以选择的。例如只需要安装最低运行要求的版本，则可以使用 `pip install -v -e .` 命令。如果希望使用可选择的像 `albumentations` 和 `imagecorruptions` 这种依赖项，可以使用 `pip install -r requirements/optional.txt ` 进行手动安装，或者在使用 `pip` 时指定所需的附加功能（例如 `pip install -v -e .[optional]`），支持附加功能的有效键值包括  `all`、`tests`、`build` 以及 `optional` 。
 
+(4) 如果希望使用 `albumentations`，我们建议使用 `pip install albumentations>=0.3.2 --no-binary imgaug,albumentations` 进行安装。 如果简单地使用 `pip install albumentations>=0.3.2` 进行安装，则会同时安装 `opencv-python-headless`（即便已经安装了 `opencv-python` 也会再次安装）。我们不允许同时安装 `opencv-python` 和 `opencv-python-headless`，因为这样可能会导致一些问题。更多细节请参考[官方文档](https://albumentations.ai/docs/getting_started/installation/#note-on-opencv-dependencies)。
+
 ### 只在 CPU 安装
 
 我们的代码能够建立在只使用 CPU 的环境（CUDA 不可用）。
diff --git a/docs_zh-CN/index.rst b/docs_zh-CN/index.rst
index 3e3aeb56..4fa30c5e 100644
--- a/docs_zh-CN/index.rst
+++ b/docs_zh-CN/index.rst
@@ -41,6 +41,7 @@ Welcome to MMDetection's documentation!
    switch_language.md
 
 .. toctree::
+   :maxdepth: 1
    :caption: 接口文档（英文）
 
    api.rst
diff --git a/docs_zh-CN/model_zoo.md b/docs_zh-CN/model_zoo.md
index a0438a53..d75737dd 100644
--- a/docs_zh-CN/model_zoo.md
+++ b/docs_zh-CN/model_zoo.md
@@ -1,3 +1,319 @@
 # 模型库
 
 ## 镜像地址
+
+从 MMDetection V2.0 起，我们只通过阿里云维护模型库。V1.x 版本的模型已经弃用。
+
+## 共同设置
+
+- 所有模型都是在 `coco_2017_train` 上训练，在 `coco_2017_val` 上测试。
+- 我们使用分布式训练。
+- 所有 pytorch-style 的 ImageNet 预训练主干网络来自 PyTorch 的模型库，caffe-style 的预训练主干网络来自 detectron2 最新开源的模型。
+- 为了与其他代码库公平比较，文档中所写的 GPU 内存是8个 GPU 的 `torch.cuda.max_memory_allocated()` 的最大值，此值通常小于 nvidia-smi 显示的值。
+- 我们以网络 foward 和后处理的时间加和作为推理时间，不包含数据加载时间。所有结果通过 [benchmark.py](https://github.com/open-mmlab/mmdetection/blob/master/tools/analysis_tools/benchmark.py) 脚本计算所得。该脚本会计算推理 2000 张图像的平均时间。
+
+## ImageNet 预训练模型
+
+通过 ImageNet 分类任务预训练的主干网络进行初始化是很常见的操作。所有预训练模型的链接都可以在 [open_mmlab](https://github.com/open-mmlab/mmcv/blob/master/mmcv/model_zoo/open_mmlab.json) 中找到。根据 `img_norm_cfg` 和原始权重，我们可以将所有 ImageNet 预训练模型分为以下几种情况：
+
+- TorchVision：torchvision 模型权重，包含 ResNet50, ResNet101。`img_norm_cfg` 为 `dict(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)`。
+- Pycls：[pycls](https://github.com/facebookresearch/pycls) 模型权重，包含 RegNetX。`img_norm_cfg` 为 `dict(
+    mean=[103.530, 116.280, 123.675], std=[57.375, 57.12, 58.395], to_rgb=False)`。
+- MSRA styles：[MSRA](https://github.com/KaimingHe/deep-residual-networks) 模型权重，包含 ResNet50_Caffe，ResNet101_Caffe。`img_norm_cfg` 为 `dict(
+    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)`。
+- Caffe2 styles：现阶段只包含 ResNext101_32x8d。`img_norm_cfg` 为 `dict(mean=[103.530, 116.280, 123.675], std=[57.375, 57.120, 58.395], to_rgb=False)`。
+- Other styles: SSD 的 `img_norm_cfg` 为 `dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)`，YOLOv3 的 `img_norm_cfg` 为 `dict(mean=[0, 0, 0], std=[255., 255., 255.], to_rgb=True)`。
+
+MMdetection 常用到的主干网络细节如下表所示：
+
+| 模型             | 来源        | 链接                                                                                                                                                                                                | 描述                                                                                                                                                                                                                       |
+| ---------------- | ----------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
+| ResNet50         | TorchVision | [torchvision 中的 ResNet-50](https://download.pytorch.org/models/resnet50-19c8e357.pth)                                                                                                             | 来自 [torchvision 中的 ResNet-50](https://download.pytorch.org/models/resnet50-19c8e357.pth)。                                                                                                                             |
+| ResNet101        | TorchVision | [torchvision 中的 ResNet-101](https://download.pytorch.org/models/resnet101-5d3b4d8f.pth)                                                                                                           | 来自 [torchvision 中的 ResNet-101](https://download.pytorch.org/models/resnet101-5d3b4d8f.pth)。                                                                                                                           |
+| RegNetX          | Pycls       | [RegNetX_3.2gf](https://download.openmmlab.com/pretrain/third_party/regnetx_3.2gf-c2599b0f.pth)，[RegNetX_800mf](https://download.openmmlab.com/pretrain/third_party/regnetx_800mf-1f4be4c7.pth) 等 | 来自 [pycls](https://github.com/facebookresearch/pycls)。                                                                                                                                                                  |
+| ResNet50_Caffe   | MSRA        | [MSRA 中的 ResNet-50](https://download.openmmlab.com/pretrain/third_party/resnet50_caffe-788b5fa3.pth)                                                                                              | 由 [Detectron2 中的 R-50.pkl](https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MSRA/R-50.pkl) 转化的副本。原始权重文件来自 [MSRA 中的原始 ResNet-50](https://github.com/KaimingHe/deep-residual-networks)。    |
+| ResNet101_Caffe  | MSRA        | [MSRA 中的 ResNet-101](https://download.openmmlab.com/pretrain/third_party/resnet101_caffe-3ad79236.pth)                                                                                            | 由 [Detectron2 中的 R-101.pkl](https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MSRA/R-101.pkl) 转化的副本。原始权重文件来自 [MSRA 中的原始 ResNet-101](https://github.com/KaimingHe/deep-residual-networks)。 |
+| ResNext101_32x8d | Caffe2      | [Caffe2 ResNext101_32x8d](https://download.openmmlab.com/pretrain/third_party/resnext101_32x8d-1516f1aa.pth)                                                                                        | 由 [Detectron2 中的 X-101-32x8d.pkl](https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/FAIR/X-101-32x8d.pkl) 转化的副本。原始 ResNeXt-101-32x8d 由 FB 使用 Caffe2 训练。                                        |
+
+## Baselines
+
+### RPN
+
+请参考 [RPN](https://github.com/open-mmlab/mmdetection/blob/master/configs/rpn)。
+
+### Faster R-CNN
+
+请参考 [Faster R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/faster_rcnn)。
+
+### Mask R-CNN
+
+请参考 [Mask R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/mask_rcnn)。
+
+### Fast R-CNN (使用提前计算的 proposals)
+
+请参考 [Fast R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/fast_rcnn)。
+
+### RetinaNet
+
+请参考 [RetinaNet](https://github.com/open-mmlab/mmdetection/blob/master/configs/retinanet)。
+
+### Cascade R-CNN and Cascade Mask R-CNN
+
+请参考 [Cascade R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/cascade_rcnn)。
+
+### Hybrid Task Cascade (HTC)
+
+请参考 [HTC](https://github.com/open-mmlab/mmdetection/blob/master/configs/htc)。
+
+### SSD
+
+请参考 [SSD](https://github.com/open-mmlab/mmdetection/blob/master/configs/ssd)。
+
+### Group Normalization (GN)
+
+请参考 [Group Normalization](https://github.com/open-mmlab/mmdetection/blob/master/configs/gn)。
+
+### Weight Standardization
+
+请参考 [Weight Standardization](https://github.com/open-mmlab/mmdetection/blob/master/configs/gn+ws)。
+
+### Deformable Convolution v2
+
+请参考 [Deformable Convolutional Networks](https://github.com/open-mmlab/mmdetection/blob/master/configs/dcn)。
+
+### CARAFE: Content-Aware ReAssembly of FEatures
+
+请参考 [CARAFE](https://github.com/open-mmlab/mmdetection/blob/master/configs/carafe)。
+
+### Instaboost
+
+请参考 [Instaboost](https://github.com/open-mmlab/mmdetection/blob/master/configs/instaboost)。
+
+### Libra R-CNN
+
+请参考 [Libra R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/libra_rcnn)。
+
+### Guided Anchoring
+
+请参考 [Guided Anchoring](https://github.com/open-mmlab/mmdetection/blob/master/configs/guided_anchoring)。
+
+### FCOS
+
+请参考 [FCOS](https://github.com/open-mmlab/mmdetection/blob/master/configs/fcos)。
+
+### FoveaBox
+
+请参考 [FoveaBox](https://github.com/open-mmlab/mmdetection/blob/master/configs/foveabox)。
+
+### RepPoints
+
+请参考 [RepPoints](https://github.com/open-mmlab/mmdetection/blob/master/configs/reppoints)。
+
+### FreeAnchor
+
+请参考 [FreeAnchor](https://github.com/open-mmlab/mmdetection/blob/master/configs/free_anchor)。
+
+### Grid R-CNN (plus)
+
+请参考 [Grid R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/grid_rcnn)。
+
+### GHM
+
+请参考 [GHM](https://github.com/open-mmlab/mmdetection/blob/master/configs/ghm)。
+
+### GCNet
+
+请参考 [GCNet](https://github.com/open-mmlab/mmdetection/blob/master/configs/gcnet)。
+
+### HRNet
+
+请参考 [HRNet](https://github.com/open-mmlab/mmdetection/blob/master/configs/hrnet)。
+
+### Mask Scoring R-CNN
+
+请参考 [Mask Scoring R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/ms_rcnn)。
+
+### Train from Scratch
+
+请参考 [Rethinking ImageNet Pre-training](https://github.com/open-mmlab/mmdetection/blob/master/configs/scratch)。
+
+### NAS-FPN
+
+请参考 [NAS-FPN](https://github.com/open-mmlab/mmdetection/blob/master/configs/nas_fpn)。
+
+### ATSS
+
+请参考 [ATSS](https://github.com/open-mmlab/mmdetection/blob/master/configs/atss)。
+
+### FSAF
+
+请参考 [FSAF](https://github.com/open-mmlab/mmdetection/blob/master/configs/fsaf)。
+
+### RegNetX
+
+请参考 [RegNet](https://github.com/open-mmlab/mmdetection/blob/master/configs/regnet)。
+
+### Res2Net
+
+请参考 [Res2Net](https://github.com/open-mmlab/mmdetection/blob/master/configs/res2net)。
+
+### GRoIE
+
+请参考 [GRoIE](https://github.com/open-mmlab/mmdetection/blob/master/configs/groie)。
+
+### Dynamic R-CNN
+
+请参考 [Dynamic R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/dynamic_rcnn)。
+
+### PointRend
+
+请参考 [PointRend](https://github.com/open-mmlab/mmdetection/blob/master/configs/point_rend)。
+
+### DetectoRS
+
+请参考 [DetectoRS](https://github.com/open-mmlab/mmdetection/blob/master/configs/detectors)。
+
+### Generalized Focal Loss
+
+请参考 [Generalized Focal Loss](https://github.com/open-mmlab/mmdetection/blob/master/configs/gfl)。
+
+### CornerNet
+
+请参考 [CornerNet](https://github.com/open-mmlab/mmdetection/blob/master/configs/cornernet)。
+
+### YOLOv3
+
+请参考 [YOLOv3](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolo)。
+
+### PAA
+
+请参考 [PAA](https://github.com/open-mmlab/mmdetection/blob/master/configs/paa)。
+
+### SABL
+
+请参考 [SABL](https://github.com/open-mmlab/mmdetection/blob/master/configs/sabl)。
+
+### CentripetalNet
+
+请参考 [CentripetalNet](https://github.com/open-mmlab/mmdetection/blob/master/configs/centripetalnet)。
+
+### ResNeSt
+
+请参考 [ResNeSt](https://github.com/open-mmlab/mmdetection/blob/master/configs/resnest)。
+
+### DETR
+
+请参考 [DETR](https://github.com/open-mmlab/mmdetection/blob/master/configs/detr)。
+
+### Deformable DETR
+
+请参考 [Deformable DETR](https://github.com/open-mmlab/mmdetection/blob/master/configs/deformable_detr)。
+
+### AutoAssign
+
+请参考 [AutoAssign](https://github.com/open-mmlab/mmdetection/blob/master/configs/autoassign)。
+
+### YOLOF
+
+请参考 [YOLOF](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolof)。
+
+### Seesaw Loss
+
+请参考 [Seesaw Loss](https://github.com/open-mmlab/mmdetection/blob/master/configs/seesaw_loss)。
+
+### CenterNet
+
+请参考 [CenterNet](https://github.com/open-mmlab/mmdetection/blob/master/configs/centernet)。
+
+### Other datasets
+
+我们还在 [PASCAL VOC](https://github.com/open-mmlab/mmdetection/blob/master/configs/pascal_voc)，[Cityscapes](https://github.com/open-mmlab/mmdetection/blob/master/configs/cityscapes) 和 [WIDER FACE](https://github.com/open-mmlab/mmdetection/blob/master/configs/wider_face) 上对一些方法进行了基准测试。
+
+### Pre-trained Models
+
+我们还通过多尺度训练和更长的训练策略来训练用 ResNet-50 和 [RegNetX-3.2G](https://github.com/open-mmlab/mmdetection/blob/master/configs/regnet) 作为主干网络的 [Faster R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/faster_rcnn) 和 [Mask R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/mask_rcnn)。这些模型可以作为下游任务的预训练模型。
+
+## 速度基准
+
+### 训练速度基准
+
+我们提供 [analyze_logs.py](https://github.com/open-mmlab/mmdetection/blob/master/tools/analysis_tools/analyze_logs.py) 来得到训练中每一次迭代的平均时间。示例请参考 [Log Analysis](https://mmdetection.readthedocs.io/en/latest/useful_tools.html#log-analysis)。
+
+我们与其他流行框架的 Mask R-CNN 训练速度进行比较（数据是从 [detectron2](https://github.com/facebookresearch/detectron2/blob/master/docs/notes/benchmarks.md/) 复制而来）。在 mmdetection 中，我们使用 [mask_rcnn_r50_caffe_fpn_poly_1x_coco_v1.py](https://github.com/open-mmlab/mmdetection/blob/master/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_poly_1x_coco_v1.py) 进行基准测试。它与 detectron2 的 [mask_rcnn_R_50_FPN_noaug_1x.yaml](https://github.com/facebookresearch/detectron2/blob/master/configs/Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x.yaml) 设置完全一样。同时，我们还提供了[模型权重](https://download.openmmlab.com/mmdetection/v2.0/benchmark/mask_rcnn_r50_caffe_fpn_poly_1x_coco_no_aug/mask_rcnn_r50_caffe_fpn_poly_1x_coco_no_aug_compare_20200518-10127928.pth)和[训练 log](https://download.openmmlab.com/mmdetection/v2.0/benchmark/mask_rcnn_r50_caffe_fpn_poly_1x_coco_no_aug/mask_rcnn_r50_caffe_fpn_poly_1x_coco_no_aug_20200518_105755.log.json) 作为参考。为了跳过 GPU 预热时间，吞吐量按照100-500次迭代之间的平均吞吐量来计算。
+
+| 框架                                                                                   | 吞吐量 (img/s) |
+| -------------------------------------------------------------------------------------- | ------------------ |
+| [Detectron2](https://github.com/facebookresearch/detectron2)                           | 62                 |
+| [MMDetection](https://github.com/open-mmlab/mmdetection)                               | 61                 |
+| [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/)          | 53                 |
+| [tensorpack](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) | 50                 |
+| [simpledet](https://github.com/TuSimple/simpledet/)                                    | 39                 |
+| [Detectron](https://github.com/facebookresearch/Detectron)                             | 19                 |
+| [matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN/)                       | 14                 |
+
+### 推理时间基准
+
+我们提供 [benchmark.py](https://github.com/open-mmlab/mmdetection/blob/master/tools/analysis_tools/benchmark.py) 对推理时间进行基准测试。此脚本将推理 2000 张图片并计算忽略前 5 次推理的平均推理时间。可以通过设置 `LOG-INTERVAL` 来改变 log 输出间隔（默认为 50）。
+
+```shell
+python toools/benchmark.py ${CONFIG} ${CHECKPOINT} [--log-interval $[LOG-INTERVAL]] [--fuse-conv-bn]
+```
+
+模型库中，所有模型在基准测量推理时间时都没设置 `fuse-conv-bn`, 此设置可以使推理时间更短。
+
+## 与 Detectron2 对比
+
+我们在速度和精度方面对 mmdetection 和 [Detectron2](https://github.com/facebookresearch/detectron2.git) 进行对比。对比所使用的 detectron2 的 commit id 为 [185c27e](https://github.com/facebookresearch/detectron2/tree/185c27e4b4d2d4c68b5627b3765420c6d7f5a659)(30/4/2020)。
+为了公平对比，我们所有的实验都在同一机器下进行。
+
+### 硬件
+
+- 8 NVIDIA Tesla V100 (32G) GPUs
+- Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz
+
+### 软件环境
+
+- Python 3.7
+- PyTorch 1.4
+- CUDA 10.1
+- CUDNN 7.6.03
+- NCCL 2.4.08
+
+### 精度
+
+| 模型                                                                                                                                   | 训练策略 | Detectron2                                                                                                                             | mmdetection | 下载                                                                                                                                                                                                                                                                                                                                                                 |
+| -------------------------------------------------------------------------------------------------------------------------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------- | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
+| [Faster R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py) | 1x       | [37.9](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml)                 | 38.0        | [model](https://download.openmmlab.com/mmdetection/v2.0/benchmark/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco-5324cff8.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/benchmark/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco_20200429_234554.log.json)             |
+| [Mask R-CNN](https://github.com/open-mmlab/mmdetection/blob/master/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py)  | 1x       | [38.6 & 35.2](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml) | 38.8 & 35.4 | [model](https://download.openmmlab.com/mmdetection/v2.0/benchmark/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco-dbecf295.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/benchmark/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco_20200430_054239.log.json) |
+| [Retinanet](https://github.com/open-mmlab/mmdetection/blob/master/configs/retinanet/retinanet_r50_caffe_fpn_mstrain_1x_coco.py)        | 1x       | [36.5](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-Detection/retinanet_R_50_FPN_1x.yaml)                   | 37.0        | [model](https://download.openmmlab.com/mmdetection/v2.0/benchmark/retinanet_r50_caffe_fpn_mstrain_1x_coco/retinanet_r50_caffe_fpn_mstrain_1x_coco-586977a0.pth) &#124; [log](https://download.openmmlab.com/mmdetection/v2.0/benchmark/retinanet_r50_caffe_fpn_mstrain_1x_coco/retinanet_r50_caffe_fpn_mstrain_1x_coco_20200430_014748.log.json)                     |
+
+### 训练速度
+
+训练速度使用 s/iter 来度量。结果越低越好。
+
+| 模型         | Detectron2 | mmdetection |
+| ------------ | ---------- | ----------- |
+| Faster R-CNN | 0.210      | 0.216       |
+| Mask R-CNN   | 0.261      | 0.265       |
+| Retinanet    | 0.200      | 0.205       |
+
+### 推理速度
+
+推理速度通过单张 GPU 下的 fps(img/s) 来度量，越高越好。
+为了与 Detectron2 保持一致，我们所写的推理时间除去了数据加载时间。
+对于 Mask RCNN，我们去除了后处理中 RLE 编码的时间。
+我们在括号中给出了官方给出的速度。由于硬件差异，官方给出的速度会比我们所测试得到的速度快一些。
+
+| 模型         | Detectron2  | mmdetection |
+| ------------ | ----------- | ----------- |
+| Faster R-CNN | 25.6 (26.3) | 22.2        |
+| Mask R-CNN   | 22.5 (23.3) | 19.6        |
+| Retinanet    | 17.8 (18.2) | 20.6        |
+
+### 训练内存
+
+| 模型         | Detectron2 | mmdetection |
+| ------------ | ---------- | ----------- |
+| Faster R-CNN | 3.0        | 3.8         |
+| Mask R-CNN   | 3.4        | 3.9         |
+| Retinanet    | 3.9        | 3.4         |
diff --git a/docs_zh-CN/tutorials/customize_dataset.md b/docs_zh-CN/tutorials/customize_dataset.md
index 597a5e16..b5373202 100644
--- a/docs_zh-CN/tutorials/customize_dataset.md
+++ b/docs_zh-CN/tutorials/customize_dataset.md
@@ -1 +1,457 @@
 # 教程 2: 自定义数据集
+
+## 支持新的数据格式
+
+为了支持新的数据格式，可以选择将数据转换成现成的格式（COCO 或者 PASCAL）或将其转换成中间格式。当然也可以选择以离线的形式（在训练之前使用脚本转换）或者在线的形式（实现一个新的 dataset 在训练中进行转换）来转换数据。
+
+在 MMDetection 中，建议将数据转换成 COCO 格式并以离线的方式进行，因此在完成数据转换后只需修改配置文件中的标注数据的路径和类别即可。
+
+### 将新的数据格式转换为现有的数据格式
+
+最简单的方法就是将你的数据集转换成现有的数据格式（COCO 或者 PASCAL VOC）
+
+COCO 格式的 json 标注文件有如下必要的字段：
+
+```python
+'images': [
+    {
+        'file_name': 'COCO_val2014_000000001268.jpg',
+        'height': 427,
+        'width': 640,
+        'id': 1268
+    },
+    ...
+],
+
+'annotations': [
+    {
+        'segmentation': [[192.81,
+            247.09,
+            ...
+            219.03,
+            249.06]],  # 如果有 mask 标签
+        'area': 1035.749,
+        'iscrowd': 0,
+        'image_id': 1268,
+        'bbox': [192.81, 224.8, 74.73, 33.43],
+        'category_id': 16,
+        'id': 42986
+    },
+    ...
+],
+
+'categories': [
+    {'id': 0, 'name': 'car'},
+ ]
+```
+
+在 json 文件中有三个必要的键：
+
+- `images`: 包含多个图片以及它们的信息的数组，例如 `file_name`、`height`、`width` 和 `id`。
+- `annotations`: 包含多个实例标注信息的数组。
+- `categories`: 包含多个类别名字和 ID 的数组。
+
+在数据预处理之后，使用现有的数据格式来训练自定义的新数据集有如下两步（以 COCO 为例）：
+
+1. 为自定义数据集修改配置文件。
+2. 检查自定义数据集的标注。
+
+这里我们举一个例子来展示上面的两个步骤，这个例子使用包括 5 个类别的 COCO 格式的数据集来训练一个现有的 Cascade Mask R-CNN R50-FPN 检测器
+
+#### 1. 为自定义数据集修改配置文件
+
+配置文件的修改涉及两个方面：
+
+1. `data` 部分。需要在 `data.train`、`data.val` 和 `data.test` 中添加 `classes`。
+2. `model` 部分中的 `num_classes`。需要将默认值（COCO 数据集中为 80）修改为自定义数据集中的类别数。
+
+`configs/my_custom_config.py` 内容如下：
+
+```python
+
+# 新的配置来自基础的配置以更好地说明需要修改的地方
+_base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py'
+
+# 1. 数据集设定
+dataset_type = 'CocoDataset'
+classes = ('a', 'b', 'c', 'd', 'e')
+data = dict(
+    samples_per_gpu=2,
+    workers_per_gpu=2,
+    train=dict(
+        type=dataset_type,
+        # 将类别名字添加至 `classes` 字段中
+        classes=classes,
+        ann_file='path/to/your/train/annotation_data',
+        img_prefix='path/to/your/train/image_data'),
+    val=dict(
+        type=dataset_type,
+        # 将类别名字添加至 `classes` 字段中
+        classes=classes,
+        ann_file='path/to/your/val/annotation_data',
+        img_prefix='path/to/your/val/image_data'),
+    test=dict(
+        type=dataset_type,
+        # 将类别名字添加至 `classes` 字段中
+        classes=classes,
+        ann_file='path/to/your/test/annotation_data',
+        img_prefix='path/to/your/test/image_data'))
+
+# 2. 模型设置
+
+# 将所有的 `num_classes` 默认值修改为5（原来为80）
+model = dict(
+    roi_head=dict(
+        bbox_head=[
+            dict(
+                type='Shared2FCBBoxHead',
+                # 将所有的 `num_classes` 默认值修改为 5（原来为 80）
+                num_classes=5),
+            dict(
+                type='Shared2FCBBoxHead',
+                # 将所有的 `num_classes` 默认值修改为 5（原来为 80）
+                num_classes=5),
+            dict(
+                type='Shared2FCBBoxHead',
+                # 将所有的 `num_classes` 默认值修改为 5（原来为 80）
+                num_classes=5)],
+    # 将所有的 `num_classes` 默认值修改为 5（原来为 80）
+    mask_head=dict(num_classes=5)))
+```
+
+#### 2. 检查自定义数据集的标注
+
+假设你自己的数据集是 COCO 格式，那么需要保证数据的标注没有问题：
+
+1. 标注文件中 `categories` 的长度要与配置中的 `classes` 元组长度相匹配，它们都表示有几类。（如例子中有 5 个类别）
+2. 配置文件中 `classes` 字段应与标注文件里 `categories` 下的 `name` 有相同的元素且顺序一致。MMDetection 会自动将 `categories` 中不连续的 `id` 映射成连续的索引，因此 `categories` 下的 `name`的字符串顺序会影响标签的索引。同时，配置文件中的 `classes` 的字符串顺序也会影响到预测框可视化时的标签。
+3. `annotations` 中的 `category_id` 必须是有效的值。比如所有 `category_id` 的值都应该属于 `categories` 中的 `id`。
+
+下面是一个有效标注的例子：
+
+```python
+
+'annotations': [
+    {
+        'segmentation': [[192.81,
+            247.09,
+            ...
+            219.03,
+            249.06]],  #如果有 mask 标签。
+        'area': 1035.749,
+        'iscrowd': 0,
+        'image_id': 1268,
+        'bbox': [192.81, 224.8, 74.73, 33.43],
+        'category_id': 16,
+        'id': 42986
+    },
+    ...
+],
+
+# MMDetection 会自动将 `categories` 中不连续的 `id` 映射成连续的索引。
+'categories': [
+    {'id': 1, 'name': 'a'}, {'id': 3, 'name': 'b'}, {'id': 4, 'name': 'c'}, {'id': 16, 'name': 'd'}, {'id': 17, 'name': 'e'},
+ ]
+```
+
+我们使用这种方式来支持 CityScapes 数据集。脚本在[cityscapes.py](https://github.com/open-mmlab/mmdetection/blob/master/tools/dataset_converters/cityscapes.py) 并且我们提供了微调的[configs](https://github.com/open-mmlab/mmdetection/blob/master/configs/cityscapes).
+
+**注意**
+
+1. 对于实例分割数据集, **MMDetection 目前只支持评估 COCO 格式的 mask AP**.
+2. 推荐训练之前进行离线转换，这样就可以继续使用 `CocoDataset` 且只需修改标注文件的路径以及训练的种类。
+
+### 调整新的数据格式为中间格式
+
+如果不想将标注格式转换为 COCO 或者 PASCAL 格式也是可行的。实际上，我们定义了一种简单的标注格式并且与所有现有的数据格式兼容，也能进行离线或者在线转换。
+
+数据集的标注是包含多个字典（dict）的列表，每个字典（dict）都与一张图片对应。测试时需要用到 `filename`（相对路径）、`width` 和 `height` 三个字段；训练时则额外需要 `ann`。`ann` 也是至少包含了两个字段的字典：`bboxes` 和 `labels`，它们都是 numpy array。有些数据集可能会提供如：crowd/difficult/ignored bboxes 标注，那么我们使用 `bboxes_ignore` 以及 `labels_ignore` 来包含它们。
+
+下面给出一个例子。
+
+```python
+
+[
+    {
+        'filename': 'a.jpg',
+        'width': 1280,
+        'height': 720,
+        'ann': {
+            'bboxes': <np.ndarray, float32> (n, 4),
+            'labels': <np.ndarray, int64> (n, ),
+            'bboxes_ignore': <np.ndarray, float32> (k, 4),
+            'labels_ignore': <np.ndarray, int64> (k, ) （可选字段）
+        }
+    },
+    ...
+]
+```
+
+有两种方法处理自定义数据。
+
+- 在线转换（online conversion）
+
+  可以新写一个继承自 `CustomDataset` 的 Dataset 类，并重写 `load_annotations(self, ann_file)` 以及 `get_ann_info(self, idx)` 这两个方法，正如[CocoDataset](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/coco.py)与[VOCDataset](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/voc.py).
+
+- 离线转换（offline conversion）
+
+  可以将标注格式转换为上述的任意格式并将其保存为 pickle 或者 json 文件，例如[pascal_voc.py](https://github.com/open-mmlab/mmdetection/blob/master/tools/dataset_converters/pascal_voc.py)。
+  然后使用`CustomDataset`。
+
+### 自定义数据集的例子：
+
+假设文本文件中表示的是一种全新的标注格式。边界框的标注信息保存在 `annotation.txt` 中，内容如下：
+
+```
+#
+000001.jpg
+1280 720
+2
+10 20 40 60 1
+20 40 50 60 2
+#
+000002.jpg
+1280 720
+3
+50 20 40 60 2
+20 40 30 45 2
+30 40 50 60 3
+```
+
+我们可以在 `mmdet/datasets/my_dataset.py` 中创建一个新的 dataset 用以加载数据。
+
+```python
+import mmcv
+import numpy as np
+
+from .builder import DATASETS
+from .custom import CustomDataset
+
+
+@DATASETS.register_module()
+class MyDataset(CustomDataset):
+
+    CLASSES = ('person', 'bicycle', 'car', 'motorcycle')
+
+    def load_annotations(self, ann_file):
+        ann_list = mmcv.list_from_file(ann_file)
+
+        data_infos = []
+        for i, ann_line in enumerate(ann_list):
+            if ann_line != '#':
+                continue
+
+            img_shape = ann_list[i + 2].split(' ')
+            width = int(img_shape[0])
+            height = int(img_shape[1])
+            bbox_number = int(ann_list[i + 3])
+
+            anns = ann_line.split(' ')
+            bboxes = []
+            labels = []
+            for anns in ann_list[i + 4:i + 4 + bbox_number]:
+                bboxes.append([float(ann) for ann in anns[:4]])
+                labels.append(int(anns[4]))
+
+            data_infos.append(
+                dict(
+                    filename=ann_list[i + 1],
+                    width=width,
+                    height=height,
+                    ann=dict(
+                        bboxes=np.array(bboxes).astype(np.float32),
+                        labels=np.array(labels).astype(np.int64))
+                ))
+
+        return data_infos
+
+    def get_ann_info(self, idx):
+        return self.data_infos[idx]['ann']
+
+```
+
+配置文件中，可以使用 `MyDataset` 进行如下修改
+
+```python
+dataset_A_train = dict(
+    type='MyDataset',
+    ann_file = 'image_list.txt',
+    pipeline=train_pipeline
+)
+```
+
+## 使用 dataset 包装器自定义数据集
+
+MMDetection 也支持非常多的数据集包装器（wrapper）来混合数据集或在训练时修改数据集的分布。
+最近 MMDetection 支持如下三种数据集包装：
+
+- `RepeatDataset`：将整个数据集简单地重复。
+- `ClassBalancedDataset`：以类别均衡的方式重复数据集。
+- `ConcatDataset`：合并数据集。
+
+### 重复数据集（Repeat dataset）
+
+使用 `RepeatDataset` 包装器来重复数据集。例如，假设原始数据集为 `Dataset_A`，重复它过后，其配置如下：
+
+
+```python
+dataset_A_train = dict(
+        type='RepeatDataset',
+        times=N,
+        dataset=dict(  # Dataset_A 的原始配置信息
+            type='Dataset_A',
+            ...
+            pipeline=train_pipeline
+        )
+    )
+```
+
+### 类别均衡数据集（Class balanced dataset）
+
+使用 `ClassBalancedDataset` 作为包装器在类别的出现的频率上重复数据集。数据集需要实例化 `self.get_cat_ids(idx)` 函数以支持 `ClassBalancedDataset`。
+比如，以 `oversample_thr=1e-3` 来重复数据集 `Dataset_A`，其配置如下：
+
+```python
+dataset_A_train = dict(
+        type='ClassBalancedDataset',
+        oversample_thr=1e-3,
+        dataset=dict(  # Dataset_A 的原始配置信息
+            type='Dataset_A',
+            ...
+            pipeline=train_pipeline
+        )
+    )
+```
+
+更多细节请参考[源码](../../mmdet/datasets/dataset_wrappers.py)。
+
+### 合并数据集（Concatenate dataset）
+
+合并数据集有三种方法：
+
+1. 如果要合并的数据集类型一致但有多个的标注文件，那么可以使用如下配置将其合并。
+
+    ```python
+    dataset_A_train = dict(
+        type='Dataset_A',
+        ann_file = ['anno_file_1', 'anno_file_2'],
+        pipeline=train_pipeline
+    )
+    ```
+
+    如果合并的数据集适用于测试或者评估，那么这种方式支持每个数据集分开进行评估。如果想要将合并的数据集作为整体用于评估，那么可以像如下一样设置 `separate_eval=False`。
+
+    ```python
+    dataset_A_train = dict(
+        type='Dataset_A',
+        ann_file = ['anno_file_1', 'anno_file_2'],
+        separate_eval=False,
+        pipeline=train_pipeline
+    )
+    ```
+
+2. 如果想要合并的是不同数据集，那么可以使用如下配置。
+
+    ```python
+    dataset_A_val = dict()
+    dataset_B_val = dict()
+
+    data = dict(
+        imgs_per_gpu=2,
+        workers_per_gpu=2,
+        train=dataset_A_train,
+        val=dict(
+            type='ConcatDataset',
+            datasets=[dataset_A_val, dataset_B_val],
+            separate_eval=False))
+    ```
+
+    只需设置 `separate_eval=False`，用户就可以将所有的数据集作为一个整体来评估。
+
+**注意**
+
+1. 在做评估时，`separate_eval=False` 选项是假设数据集使用了 `self.data_infos`。因此COCO数据集不支持此项操作，因为COCO数据集在做评估时并不是所有都依赖 `self.data_infos`。组合不同类型的数据集并将其作为一个整体来评估，这种做法没有得到测试，也不建议这样做。
+
+2. 因为不支持评估 `ClassBalancedDataset` 和 `RepeatDataset`，所以也不支持评估它们的组合。
+
+一个更复杂的例子则是分别将 `Dataset_A` 和 `Dataset_B` 重复N和M次，然后进行如下合并。
+
+```python
+dataset_A_train = dict(
+    type='RepeatDataset',
+    times=N,
+    dataset=dict(
+        type='Dataset_A',
+        ...
+        pipeline=train_pipeline
+    )
+)
+dataset_A_val = dict(
+    ...
+    pipeline=test_pipeline
+)
+dataset_A_test = dict(
+    ...
+    pipeline=test_pipeline
+)
+dataset_B_train = dict(
+    type='RepeatDataset',
+    times=M,
+    dataset=dict(
+        type='Dataset_B',
+        ...
+        pipeline=train_pipeline
+    )
+)
+data = dict(
+    imgs_per_gpu=2,
+    workers_per_gpu=2,
+    train = [
+        dataset_A_train,
+        dataset_B_train
+    ],
+    val = dataset_A_val,
+    test = dataset_A_test
+)
+
+```
+
+## 修改数据集的类别
+
+根据现有数据集的类型，我们可以修改它们的类别名称来训练其标注的子集。
+例如，如果只想训练当前数据集中的三个类别，那么就可以修改数据集的类别元组。
+数据集就会自动屏蔽掉其他类别的真实框。
+
+```python
+classes = ('person', 'bicycle', 'car')
+data = dict(
+    train=dict(classes=classes),
+    val=dict(classes=classes),
+    test=dict(classes=classes))
+```
+
+MMDetection V2.0 也支持从文件中读取类别名称，这种方式在实际应用中很常见。
+假设存在文件 `classes.txt`，其包含了如下的类别名称。
+
+```
+person
+bicycle
+car
+```
+
+用户可以将类别设置成文件路径，数据集就会自动将其加载并转换成一个列表。
+
+```python
+classes = 'path/to/classes.txt'
+data = dict(
+    train=dict(classes=classes),
+    val=dict(classes=classes),
+    test=dict(classes=classes))
+```
+
+**注意**
+
+- 在 MMDetection v2.5.0 之前，如果类别为集合时数据集将自动过滤掉不包含 GT 的图片，且没办法通过修改配置将其关闭。这是一种不可取的行为而且会引起混淆，因为当类别不是集合时数据集只有在 `filter_empty_gt=True` 以及 `test_mode=False` 的情况下才会过滤掉不包含 GT 的图片。在 MMDetection v2.5.0 之后，我们将图片的过滤以及类别的修改进行解耦，如，数据集只有在 `filter_empty_gt=True` 和 `test_mode=False` 的情况下才会过滤掉不包含 GT 的图片，无论类别是否为集合。设置类别只会影响用于训练的标注类别，用户可以自行决定是否过滤不包含 GT 的图片。
+- 因为中间格式只有框的标签并不包含类别的名字，所以使用 `CustomDataset` 时用户不能通过修改配置来过滤不含 GT 的图片。但是可以通过离线的方式来解决。
+- 当设置数据集中的 `classes` 时，记得修改 `num_classes`。从 v2.9.0 (PR#4508) 之后，我们实现了[NumClassCheckHook](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/utils.py)来检查类别数是否一致。
+- 我们在未来将会重构设置数据集类别以及数据集过滤的特性，使其更加地方便用户使用。
diff --git a/docs_zh-CN/tutorials/customize_losses.md b/docs_zh-CN/tutorials/customize_losses.md
index 4a2a8426..932a4445 100644
--- a/docs_zh-CN/tutorials/customize_losses.md
+++ b/docs_zh-CN/tutorials/customize_losses.md
@@ -1 +1,106 @@
 # 教程 6: 自定义损失函数
+
+MMDetection 为用户提供了不同的损失函数。但是默认的配置可能无法适应不同的数据和模型，所以用户可能会希望修改某一个损失函数来适应新的情况。
+
+本教程首先详细的解释计算损失的过程然后给出一些关于如何修改每一个步骤的指导。对损失的修改可以被分为微调和加权。
+
+
+## 一个损失的计算过程
+
+给定输入（包括预测和目标，以及权重），损失函数会把输入的张量映射到最后的损失标量。映射过程可以分为下面四个步骤：
+
+1. 通过损失核函数获取**元素**或者**样本**损失。
+
+2. 通过权重张量来给损失**逐元素**权重。
+
+3. 把损失张量归纳为一个**标量**。
+
+4. 用一个**张量**给当前损失一个权重。
+
+
+## 微调损失
+
+微调一个损失主要与步骤 1，3，4 有关，大部分的修改可以在配置文件中指定。这里我们用 [Focal Loss (FL)](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/losses/focal_loss.py) 作为例子。
+下面的代码分别是构建 FL 的方法和它的配置文件，他们是一一对应的。
+
+```python
+@LOSSES.register_module()
+class FocalLoss(nn.Module):
+
+    def __init__(self,
+                 use_sigmoid=True,
+                 gamma=2.0,
+                 alpha=0.25,
+                 reduction='mean',
+                 loss_weight=1.0):
+```
+
+```python
+loss_cls=dict(
+    type='FocalLoss',
+    use_sigmoid=True,
+    gamma=2.0,
+    alpha=0.25,
+    loss_weight=1.0)
+```
+
+### 微调超参数（步骤1）
+
+`gamma` 和 `beta` 是 Focal Loss 中的两个超参数。如果我们想把 `gamma` 的值设为 1.5，把 `alpha` 的值设为 0.5，我们可以在配置文件中按照如下指定：
+
+```python
+loss_cls=dict(
+    type='FocalLoss',
+    use_sigmoid=True,
+    gamma=1.5,
+    alpha=0.5,
+    loss_weight=1.0)
+```
+
+### 微调归纳方式（步骤3）
+
+Focal Loss 默认的归纳方式是 `mean`。如果我们想把归纳方式从 `mean` 改成 `sum`，我们可以在配置文件中按照如下指定：
+
+```python
+loss_cls=dict(
+    type='FocalLoss',
+    use_sigmoid=True,
+    gamma=2.0,
+    alpha=0.25,
+    loss_weight=1.0,
+    reduction='sum')
+```
+
+### 微调损失权重（步骤4）
+
+这里的损失权重是一个标量，他用来控制多任务学习中不同损失的重要程度，例如，分类损失和回归损失。如果我们想把分类损失的权重设为 0.5，我们可以在配置文件中如下指定：
+
+```python
+loss_cls=dict(
+    type='FocalLoss',
+    use_sigmoid=True,
+    gamma=2.0,
+    alpha=0.25,
+    loss_weight=0.5)
+```
+
+## 加权损失（步骤2）
+
+加权损失就是我们逐元素修改损失权重。更具体来说，我们给损失张量乘以一个与他有相同形状的权重张量。所以，损失中不同的元素可以被赋予不同的比例，所以这里叫做逐元素。损失的权重在不同模型中变化很大，而且与上下文相关，但是总的来说主要有两种损失权重：分类损失的 `label_weights` 和边界框的 `bbox_weights`。你可以在相应的头中的 `get_target` 方法中找到他们。这里我们使用 [ATSSHead](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/atss_head.py#L530) 作为一个例子。它继承了 [AnchorHead](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/anchor_head.py)，但是我们重写它的
+`get_targets` 方法来产生不同的 `label_weights` 和 `bbox_weights`。
+
+```
+class ATSSHead(AnchorHead):
+
+    ...
+
+    def get_targets(self,
+                    anchor_list,
+                    valid_flag_list,
+                    gt_bboxes_list,
+                    img_metas,
+                    gt_bboxes_ignore_list=None,
+                    gt_labels_list=None,
+                    label_channels=1,
+                    unmap_outputs=True):
+```
diff --git a/docs_zh-CN/tutorials/data_pipeline.md b/docs_zh-CN/tutorials/data_pipeline.md
index 852cde90..9305bc53 100644
--- a/docs_zh-CN/tutorials/data_pipeline.md
+++ b/docs_zh-CN/tutorials/data_pipeline.md
@@ -139,28 +139,35 @@ test_pipeline = [
 
 ## 拓展和使用自定义的流程
 
-1. 在任意文件里写一个新的流程，例如 `my_pipeline.py`，它以一个字典作为输入并且输出一个字典：
+1. 在任意文件里写一个新的流程，例如在 `my_pipeline.py`，它以一个字典作为输入并且输出一个字典：
 
     ```python
+    import random
     from mmdet.datasets import PIPELINES
 
+
     @PIPELINES.register_module()
     class MyTransform:
+        """Add your transform
+
+        Args:
+            p (float): Probability of shifts. Default 0.5.
+        """
+
+        def __init__(self, p=0.5):
+            self.p = p
 
         def __call__(self, results):
-            results['dummy'] = True
+            if random.random() > self.p:
+                results['dummy'] = True
             return results
     ```
 
-2. 导入这个新类，确保程序启动时它会被注册进 PIPELINES：
+2. 在配置文件里调用并使用你写的数据处理流程，需要确保你的训练脚本能够正确导入新增模块：
 
     ```python
-    from .my_pipeline import MyTransform
-    ```
+    custom_imports = dict(imports=['path.to.my_pipeline'], allow_failed_imports=False)
 
-3. 在配置文件里使用它：
-
-    ```python
     img_norm_cfg = dict(
         mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
     train_pipeline = [
@@ -170,8 +177,14 @@ test_pipeline = [
         dict(type='RandomFlip', flip_ratio=0.5),
         dict(type='Normalize', **img_norm_cfg),
         dict(type='Pad', size_divisor=32),
-        dict(type='MyTransform'),
+        dict(type='MyTransform', p=0.2),
         dict(type='DefaultFormatBundle'),
         dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
     ]
     ```
+
+3. 可视化数据增强处理流程的结果
+
+   如果想要可视化数据增强处理流程的结果，可以使用 `tools/misc/browse_dataset.py` 直观
+   地浏览检测数据集（图像和标注信息），或将图像保存到指定目录。
+   使用方法请参考[日志分析](../useful_tools.md)
diff --git a/faster_rcnn_inference.ipynb b/faster_rcnn_inference.ipynb
deleted file mode 100644
index 65594715..00000000
--- a/faster_rcnn_inference.ipynb
+++ /dev/null
@@ -1,288 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "id": "3fd75793",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import mmcv\n",
-    "from mmcv import Config\n",
-    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
-    "                            replace_ImageToTensor)\n",
-    "from mmdet.models import build_detector\n",
-    "from mmdet.apis import single_gpu_test\n",
-    "from mmcv.runner import load_checkpoint\n",
-    "import os\n",
-    "from mmcv.parallel import MMDataParallel\n",
-    "import pandas as pd\n",
-    "from pandas import DataFrame\n",
-    "from pycocotools.coco import COCO\n",
-    "import numpy as np"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "id": "edda58ea",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "classes = (\"UNKNOWN\", \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\", \n",
-    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\")\n",
-    "\n",
-    "# config file 들고오기\n",
-    "cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py')\n",
-    "\n",
-    "root='/home/data/data/'\n",
-    "\n",
-    "epoch = 'latest'\n",
-    "\n",
-    "# dataset config 수정\n",
-    "cfg.data.test.classes = classes\n",
-    "cfg.data.test.img_prefix = root\n",
-    "cfg.data.test.ann_file = root + 'test.json'\n",
-    "cfg.data.test.pipeline[1]['img_scale'] = (512,512) # Resize\n",
-    "\n",
-    "cfg.data.samples_per_gpu = 4\n",
-    "\n",
-    "cfg.seed=2020\n",
-    "cfg.gpu_ids = [0]\n",
-    "cfg.work_dir = './work_dirs/faster_rcnn_r50_fpn_1x_trash'\n",
-    "\n",
-    "cfg.model.roi_head.bbox_head.num_classes = 11\n",
-    "\n",
-    "cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n",
-    "cfg.model.train_cfg = None\n",
-    "\n",
-    "# checkpoint path\n",
-    "checkpoint_path = os.path.join(cfg.work_dir, f'{epoch}.pth')"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "id": "b086a8cf",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "loading annotations into memory...\n",
-      "Done (t=0.01s)\n",
-      "creating index...\n",
-      "index created!\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/hyelin/mmdetection/mmdet/datasets/api_wrappers/coco_api.py:21: UserWarning: mmpycocotools is deprecated. Please install official pycocotools by \"pip install pycocotools\"\n",
-      "  UserWarning)\n"
-     ]
-    }
-   ],
-   "source": [
-    "dataset = build_dataset(cfg.data.test)\n",
-    "data_loader = build_dataloader(\n",
-    "        dataset,\n",
-    "        samples_per_gpu=1,\n",
-    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
-    "        dist=False,\n",
-    "        shuffle=False)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "id": "83b3eae6",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Use load_from_local loader\n"
-     ]
-    }
-   ],
-   "source": [
-    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
-    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
-    "\n",
-    "model.CLASSES = dataset.CLASSES\n",
-    "model = MMDataParallel(model.cuda(), device_ids=[0])"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 7,
-   "id": "c9f5c2bb",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[                                 ] 2/837, 2.0 task/s, elapsed: 1s, ETA:   412s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/hyelin/mmdetection/mmdet/core/anchor/anchor_generator.py:323: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n",
-      "  warnings.warn('``grid_anchors`` would be deprecated soon. '\n",
-      "/home/hyelin/mmdetection/mmdet/core/anchor/anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n",
-      "  '``single_level_grid_anchors`` would be deprecated soon. '\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 31.4 task/s, elapsed: 27s, ETA:     0s"
-     ]
-    }
-   ],
-   "source": [
-    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 8,
-   "id": "5672a0ff",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "loading annotations into memory...\n",
-      "Done (t=0.01s)\n",
-      "creating index...\n",
-      "index created!\n"
-     ]
-    },
-    {
-     "data": {
-      "text/html": [
-       "<div>\n",
-       "<style scoped>\n",
-       "    .dataframe tbody tr th:only-of-type {\n",
-       "        vertical-align: middle;\n",
-       "    }\n",
-       "\n",
-       "    .dataframe tbody tr th {\n",
-       "        vertical-align: top;\n",
-       "    }\n",
-       "\n",
-       "    .dataframe thead th {\n",
-       "        text-align: right;\n",
-       "    }\n",
-       "</style>\n",
-       "<table border=\"1\" class=\"dataframe\">\n",
-       "  <thead>\n",
-       "    <tr style=\"text-align: right;\">\n",
-       "      <th></th>\n",
-       "      <th>PredictionString</th>\n",
-       "      <th>image_id</th>\n",
-       "    </tr>\n",
-       "  </thead>\n",
-       "  <tbody>\n",
-       "    <tr>\n",
-       "      <th>0</th>\n",
-       "      <td>1 0.13906147 400.45792 396.2644 433.47427 416....</td>\n",
-       "      <td>batch_01_vt/0021.jpg</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>1</th>\n",
-       "      <td>2 0.28541917 86.7869 19.328773 180.50931 221.5...</td>\n",
-       "      <td>batch_01_vt/0028.jpg</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2</th>\n",
-       "      <td>1 0.20373663 234.55782 458.92645 304.07123 511...</td>\n",
-       "      <td>batch_01_vt/0031.jpg</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>3</th>\n",
-       "      <td>1 0.067051776 341.9012 108.89167 377.85495 167...</td>\n",
-       "      <td>batch_01_vt/0032.jpg</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>4</th>\n",
-       "      <td>1 0.097199455 155.30357 339.61398 384.83374 48...</td>\n",
-       "      <td>batch_01_vt/0070.jpg</td>\n",
-       "    </tr>\n",
-       "  </tbody>\n",
-       "</table>\n",
-       "</div>"
-      ],
-      "text/plain": [
-       "                                    PredictionString              image_id\n",
-       "0  1 0.13906147 400.45792 396.2644 433.47427 416....  batch_01_vt/0021.jpg\n",
-       "1  2 0.28541917 86.7869 19.328773 180.50931 221.5...  batch_01_vt/0028.jpg\n",
-       "2  1 0.20373663 234.55782 458.92645 304.07123 511...  batch_01_vt/0031.jpg\n",
-       "3  1 0.067051776 341.9012 108.89167 377.85495 167...  batch_01_vt/0032.jpg\n",
-       "4  1 0.097199455 155.30357 339.61398 384.83374 48...  batch_01_vt/0070.jpg"
-      ]
-     },
-     "execution_count": 8,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "prediction_strings = []\n",
-    "file_names = []\n",
-    "coco = COCO(cfg.data.test.ann_file)\n",
-    "imag_ids = coco.getImgIds()\n",
-    "\n",
-    "class_num = 11\n",
-    "for i, out in enumerate(output):\n",
-    "    prediction_string = ''\n",
-    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
-    "    for j in range(class_num):\n",
-    "        for o in out[j]:\n",
-    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
-    "                o[2]) + ' ' + str(o[3]) + ' '\n",
-    "        \n",
-    "    prediction_strings.append(prediction_string)\n",
-    "    file_names.append(image_info['file_name'])\n",
-    "\n",
-    "\n",
-    "submission = pd.DataFrame()\n",
-    "submission['PredictionString'] = prediction_strings\n",
-    "submission['image_id'] = file_names\n",
-    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}.csv'), index=None)\n",
-    "submission.head()"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.7.11"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 5
-}
diff --git a/faster_rcnn_train.ipynb b/faster_rcnn_train.ipynb
deleted file mode 100644
index 21039b1f..00000000
--- a/faster_rcnn_train.ipynb
+++ /dev/null
@@ -1,928 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "from mmcv import Config\n",
-    "from mmdet.datasets import build_dataset\n",
-    "from mmdet.models import build_detector\n",
-    "from mmdet.apis import train_detector\n",
-    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
-    "                            replace_ImageToTensor)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "classes = (\"UNKNOWN\", \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\", \n",
-    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\")\n",
-    "\n",
-    "# config file 들고오기\n",
-    "cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py')\n",
-    "\n",
-    "root='/home/data/data/'\n",
-    "\n",
-    "# dataset config 수정\n",
-    "cfg.data.train.classes = classes\n",
-    "cfg.data.train.img_prefix = root\n",
-    "cfg.data.train.ann_file = root + 'train.json'\n",
-    "cfg.data.train.pipeline[2]['img_scale'] = (512,512) # Resize\n",
-    "\n",
-    "cfg.data.val.classes = classes\n",
-    "cfg.data.val.img_prefix = root\n",
-    "cfg.data.val.ann_file = root + 'val.json'\n",
-    "cfg.data.val.pipeline[1]['img_scale'] = (512,512) # Resize\n",
-    "\n",
-    "cfg.data.test.classes = classes\n",
-    "cfg.data.test.img_prefix = root\n",
-    "cfg.data.test.ann_file = root + 'test.json'\n",
-    "cfg.data.test.pipeline[1]['img_scale'] = (512,512) # Resize\n",
-    "\n",
-    "cfg.data.samples_per_gpu = 4\n",
-    "\n",
-    "cfg.seed = 2020\n",
-    "cfg.gpu_ids = [0]\n",
-    "cfg.work_dir = './work_dirs/faster_rcnn_r50_fpn_1x_trash'\n",
-    "\n",
-    "cfg.model.roi_head.bbox_head.num_classes = 11\n",
-    "\n",
-    "cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/hyelin/mmdetection/mmdet/core/anchor/builder.py:16: UserWarning: ``build_anchor_generator`` would be deprecated soon, please use ``build_prior_generator`` \n",
-      "  '``build_anchor_generator`` would be deprecated soon, please use '\n"
-     ]
-    }
-   ],
-   "source": [
-    "model = build_detector(cfg.model)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/hyelin/mmdetection/mmdet/datasets/api_wrappers/coco_api.py:21: UserWarning: mmpycocotools is deprecated. Please install official pycocotools by \"pip install pycocotools\"\n",
-      "  UserWarning)\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "loading annotations into memory...\n",
-      "Done (t=3.94s)\n",
-      "creating index...\n",
-      "index created!\n"
-     ]
-    }
-   ],
-   "source": [
-    "datasets = [build_dataset(cfg.data.train)]"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "loading annotations into memory...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:22:57,552 - mmdet - INFO - Start running, host: root@boostcamp2-teacher005, work_dir: /home/hyelin/mmdetection/wordk_dirs/faster_rcnn_r50_fpn_1x_trash\n",
-      "2021-08-15 12:22:57,553 - mmdet - INFO - Hooks will be executed in the following order:\n",
-      "before_run:\n",
-      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
-      "(NORMAL      ) CheckpointHook                     \n",
-      "(NORMAL      ) EvalHook                           \n",
-      "(VERY_LOW    ) TextLoggerHook                     \n",
-      " -------------------- \n",
-      "before_train_epoch:\n",
-      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
-      "(NORMAL      ) EvalHook                           \n",
-      "(NORMAL      ) NumClassCheckHook                  \n",
-      "(LOW         ) IterTimerHook                      \n",
-      "(VERY_LOW    ) TextLoggerHook                     \n",
-      " -------------------- \n",
-      "before_train_iter:\n",
-      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
-      "(NORMAL      ) EvalHook                           \n",
-      "(LOW         ) IterTimerHook                      \n",
-      " -------------------- \n",
-      "after_train_iter:\n",
-      "(ABOVE_NORMAL) OptimizerHook                      \n",
-      "(NORMAL      ) CheckpointHook                     \n",
-      "(NORMAL      ) EvalHook                           \n",
-      "(LOW         ) IterTimerHook                      \n",
-      "(VERY_LOW    ) TextLoggerHook                     \n",
-      " -------------------- \n",
-      "after_train_epoch:\n",
-      "(NORMAL      ) CheckpointHook                     \n",
-      "(NORMAL      ) EvalHook                           \n",
-      "(VERY_LOW    ) TextLoggerHook                     \n",
-      " -------------------- \n",
-      "before_val_epoch:\n",
-      "(NORMAL      ) NumClassCheckHook                  \n",
-      "(LOW         ) IterTimerHook                      \n",
-      "(VERY_LOW    ) TextLoggerHook                     \n",
-      " -------------------- \n",
-      "before_val_iter:\n",
-      "(LOW         ) IterTimerHook                      \n",
-      " -------------------- \n",
-      "after_val_iter:\n",
-      "(LOW         ) IterTimerHook                      \n",
-      " -------------------- \n",
-      "after_val_epoch:\n",
-      "(VERY_LOW    ) TextLoggerHook                     \n",
-      " -------------------- \n",
-      "2021-08-15 12:22:57,554 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Done (t=1.29s)\n",
-      "creating index...\n",
-      "index created!\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/hyelin/mmdetection/mmdet/core/anchor/anchor_generator.py:323: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n",
-      "  warnings.warn('``grid_anchors`` would be deprecated soon. '\n",
-      "/home/hyelin/mmdetection/mmdet/core/anchor/anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n",
-      "  '``single_level_grid_anchors`` would be deprecated soon. '\n",
-      "2021-08-15 12:23:10,397 - mmdet - INFO - Epoch [1][50/655]\tlr: 1.978e-03, eta: 0:33:25, time: 0.257, data_time: 0.049, memory: 2099, loss_rpn_cls: 0.4915, loss_rpn_bbox: 0.0926, loss_cls: 1.1324, acc: 85.9189, loss_bbox: 0.1610, loss: 1.8774, grad_norm: 5.3722\n",
-      "2021-08-15 12:23:21,090 - mmdet - INFO - Epoch [1][100/655]\tlr: 3.976e-03, eta: 0:30:26, time: 0.214, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.3372, loss_rpn_bbox: 0.0877, loss_cls: 0.4305, acc: 92.8984, loss_bbox: 0.2415, loss: 1.0969, grad_norm: 1.8828\n",
-      "2021-08-15 12:23:31,514 - mmdet - INFO - Epoch [1][150/655]\tlr: 5.974e-03, eta: 0:29:05, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.3199, loss_rpn_bbox: 0.0904, loss_cls: 0.4163, acc: 91.5469, loss_bbox: 0.3014, loss: 1.1279, grad_norm: 1.5767\n",
-      "2021-08-15 12:23:42,022 - mmdet - INFO - Epoch [1][200/655]\tlr: 7.972e-03, eta: 0:28:22, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.3219, loss_rpn_bbox: 0.0955, loss_cls: 0.4332, acc: 91.7207, loss_bbox: 0.2907, loss: 1.1413, grad_norm: 2.1307\n",
-      "2021-08-15 12:23:52,383 - mmdet - INFO - Epoch [1][250/655]\tlr: 9.970e-03, eta: 0:27:48, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.3324, loss_rpn_bbox: 0.0966, loss_cls: 0.4738, acc: 91.2236, loss_bbox: 0.3126, loss: 1.2153, grad_norm: 2.0484\n",
-      "2021-08-15 12:24:02,915 - mmdet - INFO - Epoch [1][300/655]\tlr: 1.197e-02, eta: 0:27:26, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2739, loss_rpn_bbox: 0.0804, loss_cls: 0.4509, acc: 90.8164, loss_bbox: 0.3383, loss: 1.1435, grad_norm: 1.6490\n",
-      "2021-08-15 12:24:13,507 - mmdet - INFO - Epoch [1][350/655]\tlr: 1.397e-02, eta: 0:27:09, time: 0.212, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2618, loss_rpn_bbox: 0.0775, loss_cls: 0.4191, acc: 91.1230, loss_bbox: 0.3256, loss: 1.0840, grad_norm: 1.4355\n",
-      "2021-08-15 12:24:24,011 - mmdet - INFO - Epoch [1][400/655]\tlr: 1.596e-02, eta: 0:26:52, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2930, loss_rpn_bbox: 0.0900, loss_cls: 0.4739, acc: 90.0449, loss_bbox: 0.3592, loss: 1.2161, grad_norm: 1.5670\n",
-      "2021-08-15 12:24:34,438 - mmdet - INFO - Epoch [1][450/655]\tlr: 1.796e-02, eta: 0:26:35, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2907, loss_rpn_bbox: 0.0938, loss_cls: 0.4717, acc: 90.2520, loss_bbox: 0.3498, loss: 1.2060, grad_norm: 1.4118\n",
-      "2021-08-15 12:24:44,908 - mmdet - INFO - Epoch [1][500/655]\tlr: 1.996e-02, eta: 0:26:20, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2780, loss_rpn_bbox: 0.0903, loss_cls: 0.4460, acc: 91.0635, loss_bbox: 0.3177, loss: 1.1320, grad_norm: 1.4641\n",
-      "2021-08-15 12:24:55,288 - mmdet - INFO - Epoch [1][550/655]\tlr: 2.000e-02, eta: 0:26:04, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2518, loss_rpn_bbox: 0.0742, loss_cls: 0.4582, acc: 91.3008, loss_bbox: 0.3158, loss: 1.1001, grad_norm: 1.4061\n",
-      "2021-08-15 12:25:05,950 - mmdet - INFO - Epoch [1][600/655]\tlr: 2.000e-02, eta: 0:25:53, time: 0.213, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2728, loss_rpn_bbox: 0.0824, loss_cls: 0.4356, acc: 91.1230, loss_bbox: 0.3172, loss: 1.1080, grad_norm: 1.1786\n",
-      "2021-08-15 12:25:16,355 - mmdet - INFO - Epoch [1][650/655]\tlr: 2.000e-02, eta: 0:25:39, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2593, loss_rpn_bbox: 0.0768, loss_cls: 0.4479, acc: 90.8857, loss_bbox: 0.3251, loss: 1.1092, grad_norm: 1.2416\n",
-      "2021-08-15 12:25:17,409 - mmdet - INFO - Saving checkpoint at 1 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 36.0 task/s, elapsed: 18s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:25:36,750 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.02s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=2.10s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:25:39,805 - mmdet - INFO - Epoch(val) [1][655]\tbbox_mAP: 0.0010, bbox_mAP_50: 0.0040, bbox_mAP_75: 0.0010, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0040, bbox_mAP_l: 0.0020, bbox_mAP_copypaste: 0.001 0.004 0.001 0.000 0.004 0.002\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=0.48s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.004\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.001\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.004\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.002\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.012\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.012\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.003\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.028\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:25:52,457 - mmdet - INFO - Epoch [2][50/655]\tlr: 2.000e-02, eta: 0:25:36, time: 0.253, data_time: 0.051, memory: 2100, loss_rpn_cls: 0.2491, loss_rpn_bbox: 0.0721, loss_cls: 0.4294, acc: 90.7871, loss_bbox: 0.3276, loss: 1.0782, grad_norm: 1.0733\n",
-      "2021-08-15 12:26:03,266 - mmdet - INFO - Epoch [2][100/655]\tlr: 2.000e-02, eta: 0:25:26, time: 0.216, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2929, loss_rpn_bbox: 0.0908, loss_cls: 0.4321, acc: 90.8018, loss_bbox: 0.3179, loss: 1.1337, grad_norm: 1.1091\n",
-      "2021-08-15 12:26:13,900 - mmdet - INFO - Epoch [2][150/655]\tlr: 2.000e-02, eta: 0:25:15, time: 0.213, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2540, loss_rpn_bbox: 0.0760, loss_cls: 0.3992, acc: 91.4326, loss_bbox: 0.2992, loss: 1.0284, grad_norm: 0.9320\n",
-      "2021-08-15 12:26:24,446 - mmdet - INFO - Epoch [2][200/655]\tlr: 2.000e-02, eta: 0:25:02, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2551, loss_rpn_bbox: 0.0817, loss_cls: 0.4561, acc: 90.0215, loss_bbox: 0.3519, loss: 1.1448, grad_norm: 1.0173\n",
-      "2021-08-15 12:26:34,870 - mmdet - INFO - Epoch [2][250/655]\tlr: 2.000e-02, eta: 0:24:49, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2631, loss_rpn_bbox: 0.0809, loss_cls: 0.4223, acc: 90.8779, loss_bbox: 0.3109, loss: 1.0773, grad_norm: 1.0821\n",
-      "2021-08-15 12:26:45,548 - mmdet - INFO - Epoch [2][300/655]\tlr: 2.000e-02, eta: 0:24:38, time: 0.214, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2673, loss_rpn_bbox: 0.0855, loss_cls: 0.4415, acc: 90.2881, loss_bbox: 0.3329, loss: 1.1271, grad_norm: 1.0146\n",
-      "2021-08-15 12:26:56,109 - mmdet - INFO - Epoch [2][350/655]\tlr: 2.000e-02, eta: 0:24:27, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2957, loss_rpn_bbox: 0.0982, loss_cls: 0.4744, acc: 89.4629, loss_bbox: 0.3578, loss: 1.2260, grad_norm: 1.1126\n",
-      "2021-08-15 12:27:06,414 - mmdet - INFO - Epoch [2][400/655]\tlr: 2.000e-02, eta: 0:24:13, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2413, loss_rpn_bbox: 0.0816, loss_cls: 0.4059, acc: 90.9932, loss_bbox: 0.3067, loss: 1.0355, grad_norm: 0.9855\n",
-      "2021-08-15 12:27:16,915 - mmdet - INFO - Epoch [2][450/655]\tlr: 2.000e-02, eta: 0:24:02, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2582, loss_rpn_bbox: 0.0800, loss_cls: 0.4451, acc: 90.0312, loss_bbox: 0.3450, loss: 1.1283, grad_norm: 1.1152\n",
-      "2021-08-15 12:27:27,464 - mmdet - INFO - Epoch [2][500/655]\tlr: 2.000e-02, eta: 0:23:50, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2426, loss_rpn_bbox: 0.0750, loss_cls: 0.4201, acc: 90.2998, loss_bbox: 0.3338, loss: 1.0715, grad_norm: 1.0147\n",
-      "2021-08-15 12:27:37,933 - mmdet - INFO - Epoch [2][550/655]\tlr: 2.000e-02, eta: 0:23:38, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2523, loss_rpn_bbox: 0.0859, loss_cls: 0.4158, acc: 90.5068, loss_bbox: 0.3196, loss: 1.0736, grad_norm: 1.0101\n",
-      "2021-08-15 12:27:48,488 - mmdet - INFO - Epoch [2][600/655]\tlr: 2.000e-02, eta: 0:23:27, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2599, loss_rpn_bbox: 0.0809, loss_cls: 0.4016, acc: 91.0186, loss_bbox: 0.2997, loss: 1.0421, grad_norm: 0.9795\n",
-      "2021-08-15 12:27:58,948 - mmdet - INFO - Epoch [2][650/655]\tlr: 2.000e-02, eta: 0:23:16, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2337, loss_rpn_bbox: 0.0765, loss_cls: 0.4136, acc: 90.3105, loss_bbox: 0.3327, loss: 1.0565, grad_norm: 1.0360\n",
-      "2021-08-15 12:28:00,001 - mmdet - INFO - Saving checkpoint at 2 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 35.9 task/s, elapsed: 18s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:28:19,711 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.41s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=3.83s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:28:25,444 - mmdet - INFO - Epoch(val) [2][655]\tbbox_mAP: 0.0060, bbox_mAP_50: 0.0170, bbox_mAP_75: 0.0020, bbox_mAP_s: 0.0010, bbox_mAP_m: 0.0100, bbox_mAP_l: 0.0130, bbox_mAP_copypaste: 0.006 0.017 0.002 0.001 0.010 0.013\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=0.94s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.017\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.002\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.001\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.013\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.037\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.037\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.037\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.004\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.021\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.091\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:28:38,314 - mmdet - INFO - Epoch [3][50/655]\tlr: 2.000e-02, eta: 0:23:09, time: 0.257, data_time: 0.050, memory: 2100, loss_rpn_cls: 0.2212, loss_rpn_bbox: 0.0704, loss_cls: 0.4017, acc: 90.6592, loss_bbox: 0.3147, loss: 1.0080, grad_norm: 0.9837\n",
-      "2021-08-15 12:28:49,096 - mmdet - INFO - Epoch [3][100/655]\tlr: 2.000e-02, eta: 0:22:59, time: 0.216, data_time: 0.007, memory: 2100, loss_rpn_cls: 0.2440, loss_rpn_bbox: 0.0852, loss_cls: 0.3866, acc: 91.4482, loss_bbox: 0.2816, loss: 0.9974, grad_norm: 1.0874\n",
-      "2021-08-15 12:28:59,767 - mmdet - INFO - Epoch [3][150/655]\tlr: 2.000e-02, eta: 0:22:48, time: 0.213, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2400, loss_rpn_bbox: 0.0758, loss_cls: 0.3992, acc: 91.1113, loss_bbox: 0.2946, loss: 1.0096, grad_norm: 1.0958\n",
-      "2021-08-15 12:29:10,337 - mmdet - INFO - Epoch [3][200/655]\tlr: 2.000e-02, eta: 0:22:37, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2700, loss_rpn_bbox: 0.0867, loss_cls: 0.4214, acc: 90.2832, loss_bbox: 0.3203, loss: 1.0985, grad_norm: 1.0635\n",
-      "2021-08-15 12:29:20,880 - mmdet - INFO - Epoch [3][250/655]\tlr: 2.000e-02, eta: 0:22:26, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2710, loss_rpn_bbox: 0.0893, loss_cls: 0.4403, acc: 90.0273, loss_bbox: 0.3282, loss: 1.1289, grad_norm: 1.1275\n",
-      "2021-08-15 12:29:31,292 - mmdet - INFO - Epoch [3][300/655]\tlr: 2.000e-02, eta: 0:22:14, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2415, loss_rpn_bbox: 0.0791, loss_cls: 0.4141, acc: 90.1338, loss_bbox: 0.3252, loss: 1.0599, grad_norm: 1.0115\n",
-      "2021-08-15 12:29:41,656 - mmdet - INFO - Epoch [3][350/655]\tlr: 2.000e-02, eta: 0:22:02, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2166, loss_rpn_bbox: 0.0753, loss_cls: 0.4296, acc: 90.0576, loss_bbox: 0.3390, loss: 1.0604, grad_norm: 1.0494\n",
-      "2021-08-15 12:29:52,029 - mmdet - INFO - Epoch [3][400/655]\tlr: 2.000e-02, eta: 0:21:50, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2357, loss_rpn_bbox: 0.0803, loss_cls: 0.4308, acc: 89.8389, loss_bbox: 0.3360, loss: 1.0827, grad_norm: 1.1510\n",
-      "2021-08-15 12:30:02,340 - mmdet - INFO - Epoch [3][450/655]\tlr: 2.000e-02, eta: 0:21:39, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2045, loss_rpn_bbox: 0.0664, loss_cls: 0.4068, acc: 90.3789, loss_bbox: 0.3225, loss: 1.0003, grad_norm: 1.0686\n",
-      "2021-08-15 12:30:12,777 - mmdet - INFO - Epoch [3][500/655]\tlr: 2.000e-02, eta: 0:21:27, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2478, loss_rpn_bbox: 0.0878, loss_cls: 0.4230, acc: 89.7793, loss_bbox: 0.3400, loss: 1.0986, grad_norm: 1.2217\n",
-      "2021-08-15 12:30:23,079 - mmdet - INFO - Epoch [3][550/655]\tlr: 2.000e-02, eta: 0:21:15, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2371, loss_rpn_bbox: 0.0811, loss_cls: 0.4266, acc: 89.7646, loss_bbox: 0.3362, loss: 1.0810, grad_norm: 1.1980\n",
-      "2021-08-15 12:30:33,428 - mmdet - INFO - Epoch [3][600/655]\tlr: 2.000e-02, eta: 0:21:04, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2360, loss_rpn_bbox: 0.0807, loss_cls: 0.4007, acc: 90.7793, loss_bbox: 0.3065, loss: 1.0240, grad_norm: 1.0440\n",
-      "2021-08-15 12:30:43,973 - mmdet - INFO - Epoch [3][650/655]\tlr: 2.000e-02, eta: 0:20:53, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2070, loss_rpn_bbox: 0.0788, loss_cls: 0.3993, acc: 90.1523, loss_bbox: 0.3194, loss: 1.0045, grad_norm: 1.0460\n",
-      "2021-08-15 12:30:45,016 - mmdet - INFO - Saving checkpoint at 3 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 35.9 task/s, elapsed: 18s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:31:05,102 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.05s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=4.29s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:31:10,986 - mmdet - INFO - Epoch(val) [3][655]\tbbox_mAP: 0.0100, bbox_mAP_50: 0.0290, bbox_mAP_75: 0.0050, bbox_mAP_s: 0.0020, bbox_mAP_m: 0.0150, bbox_mAP_l: 0.0240, bbox_mAP_copypaste: 0.010 0.029 0.005 0.002 0.015 0.024\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=0.96s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.010\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.029\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.005\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.002\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.015\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.024\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.048\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.048\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.048\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.002\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.030\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.121\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:31:23,828 - mmdet - INFO - Epoch [4][50/655]\tlr: 2.000e-02, eta: 0:20:45, time: 0.257, data_time: 0.049, memory: 2100, loss_rpn_cls: 0.1979, loss_rpn_bbox: 0.0651, loss_cls: 0.3694, acc: 91.2715, loss_bbox: 0.2876, loss: 0.9199, grad_norm: 1.0870\n",
-      "2021-08-15 12:31:34,343 - mmdet - INFO - Epoch [4][100/655]\tlr: 2.000e-02, eta: 0:20:34, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2357, loss_rpn_bbox: 0.0819, loss_cls: 0.4408, acc: 89.4990, loss_bbox: 0.3406, loss: 1.0990, grad_norm: 1.2515\n",
-      "2021-08-15 12:31:44,787 - mmdet - INFO - Epoch [4][150/655]\tlr: 2.000e-02, eta: 0:20:22, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2300, loss_rpn_bbox: 0.0794, loss_cls: 0.4352, acc: 89.8506, loss_bbox: 0.3307, loss: 1.0752, grad_norm: 1.1525\n",
-      "2021-08-15 12:31:55,202 - mmdet - INFO - Epoch [4][200/655]\tlr: 2.000e-02, eta: 0:20:11, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2072, loss_rpn_bbox: 0.0803, loss_cls: 0.3975, acc: 90.3838, loss_bbox: 0.3132, loss: 0.9983, grad_norm: 1.1998\n",
-      "2021-08-15 12:32:05,716 - mmdet - INFO - Epoch [4][250/655]\tlr: 2.000e-02, eta: 0:20:00, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2126, loss_rpn_bbox: 0.0680, loss_cls: 0.3947, acc: 90.4795, loss_bbox: 0.3121, loss: 0.9875, grad_norm: 1.2379\n",
-      "2021-08-15 12:32:16,096 - mmdet - INFO - Epoch [4][300/655]\tlr: 2.000e-02, eta: 0:19:49, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2081, loss_rpn_bbox: 0.0766, loss_cls: 0.4146, acc: 89.9180, loss_bbox: 0.3345, loss: 1.0337, grad_norm: 1.2099\n",
-      "2021-08-15 12:32:26,530 - mmdet - INFO - Epoch [4][350/655]\tlr: 2.000e-02, eta: 0:19:38, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2251, loss_rpn_bbox: 0.0811, loss_cls: 0.4105, acc: 89.8135, loss_bbox: 0.3306, loss: 1.0472, grad_norm: 1.1802\n",
-      "2021-08-15 12:32:36,939 - mmdet - INFO - Epoch [4][400/655]\tlr: 2.000e-02, eta: 0:19:27, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2229, loss_rpn_bbox: 0.0909, loss_cls: 0.4328, acc: 89.5078, loss_bbox: 0.3389, loss: 1.0854, grad_norm: 1.1720\n",
-      "2021-08-15 12:32:47,685 - mmdet - INFO - Epoch [4][450/655]\tlr: 2.000e-02, eta: 0:19:16, time: 0.215, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1892, loss_rpn_bbox: 0.0695, loss_cls: 0.3923, acc: 90.8574, loss_bbox: 0.2970, loss: 0.9479, grad_norm: 1.2079\n",
-      "2021-08-15 12:32:58,062 - mmdet - INFO - Epoch [4][500/655]\tlr: 2.000e-02, eta: 0:19:05, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2205, loss_rpn_bbox: 0.0816, loss_cls: 0.4022, acc: 90.0371, loss_bbox: 0.3168, loss: 1.0211, grad_norm: 1.2620\n",
-      "2021-08-15 12:33:08,494 - mmdet - INFO - Epoch [4][550/655]\tlr: 2.000e-02, eta: 0:18:54, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1940, loss_rpn_bbox: 0.0765, loss_cls: 0.4050, acc: 89.9902, loss_bbox: 0.3256, loss: 1.0010, grad_norm: 1.2772\n",
-      "2021-08-15 12:33:18,836 - mmdet - INFO - Epoch [4][600/655]\tlr: 2.000e-02, eta: 0:18:43, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1752, loss_rpn_bbox: 0.0672, loss_cls: 0.3983, acc: 90.1270, loss_bbox: 0.3232, loss: 0.9640, grad_norm: 1.1387\n",
-      "2021-08-15 12:33:29,133 - mmdet - INFO - Epoch [4][650/655]\tlr: 2.000e-02, eta: 0:18:32, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2251, loss_rpn_bbox: 0.0894, loss_cls: 0.4194, acc: 89.5029, loss_bbox: 0.3347, loss: 1.0686, grad_norm: 1.2404\n",
-      "2021-08-15 12:33:30,204 - mmdet - INFO - Saving checkpoint at 4 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 35.8 task/s, elapsed: 18s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:33:50,536 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.44s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=5.38s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:33:57,799 - mmdet - INFO - Epoch(val) [4][655]\tbbox_mAP: 0.0140, bbox_mAP_50: 0.0390, bbox_mAP_75: 0.0050, bbox_mAP_s: 0.0020, bbox_mAP_m: 0.0160, bbox_mAP_l: 0.0330, bbox_mAP_copypaste: 0.014 0.039 0.005 0.002 0.016 0.033\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=1.20s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.014\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.039\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.005\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.002\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.016\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.064\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.064\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.004\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.041\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.158\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:34:10,564 - mmdet - INFO - Epoch [5][50/655]\tlr: 2.000e-02, eta: 0:18:22, time: 0.255, data_time: 0.050, memory: 2100, loss_rpn_cls: 0.2167, loss_rpn_bbox: 0.0838, loss_cls: 0.4377, acc: 89.0830, loss_bbox: 0.3514, loss: 1.0896, grad_norm: 1.3470\n",
-      "2021-08-15 12:34:21,008 - mmdet - INFO - Epoch [5][100/655]\tlr: 2.000e-02, eta: 0:18:11, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2217, loss_rpn_bbox: 0.0859, loss_cls: 0.4292, acc: 89.0332, loss_bbox: 0.3500, loss: 1.0869, grad_norm: 1.1860\n",
-      "2021-08-15 12:34:31,484 - mmdet - INFO - Epoch [5][150/655]\tlr: 2.000e-02, eta: 0:18:00, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2055, loss_rpn_bbox: 0.0878, loss_cls: 0.4264, acc: 89.2695, loss_bbox: 0.3484, loss: 1.0681, grad_norm: 1.1925\n",
-      "2021-08-15 12:34:42,153 - mmdet - INFO - Epoch [5][200/655]\tlr: 2.000e-02, eta: 0:17:50, time: 0.213, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1773, loss_rpn_bbox: 0.0733, loss_cls: 0.4037, acc: 89.7539, loss_bbox: 0.3293, loss: 0.9836, grad_norm: 1.1549\n",
-      "2021-08-15 12:34:52,628 - mmdet - INFO - Epoch [5][250/655]\tlr: 2.000e-02, eta: 0:17:39, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1962, loss_rpn_bbox: 0.0810, loss_cls: 0.3918, acc: 89.8242, loss_bbox: 0.3264, loss: 0.9954, grad_norm: 1.3233\n",
-      "2021-08-15 12:35:03,417 - mmdet - INFO - Epoch [5][300/655]\tlr: 2.000e-02, eta: 0:17:29, time: 0.216, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1800, loss_rpn_bbox: 0.0703, loss_cls: 0.3939, acc: 90.3691, loss_bbox: 0.3126, loss: 0.9568, grad_norm: 1.1577\n",
-      "2021-08-15 12:35:14,497 - mmdet - INFO - Epoch [5][350/655]\tlr: 2.000e-02, eta: 0:17:19, time: 0.222, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1759, loss_rpn_bbox: 0.0702, loss_cls: 0.3950, acc: 89.9053, loss_bbox: 0.3249, loss: 0.9658, grad_norm: 1.2485\n",
-      "2021-08-15 12:35:25,075 - mmdet - INFO - Epoch [5][400/655]\tlr: 2.000e-02, eta: 0:17:08, time: 0.212, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1777, loss_rpn_bbox: 0.0737, loss_cls: 0.3878, acc: 90.3838, loss_bbox: 0.3063, loss: 0.9455, grad_norm: 1.2288\n",
-      "2021-08-15 12:35:35,487 - mmdet - INFO - Epoch [5][450/655]\tlr: 2.000e-02, eta: 0:16:57, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1829, loss_rpn_bbox: 0.0696, loss_cls: 0.4152, acc: 89.9258, loss_bbox: 0.3233, loss: 0.9910, grad_norm: 1.2149\n",
-      "2021-08-15 12:35:45,702 - mmdet - INFO - Epoch [5][500/655]\tlr: 2.000e-02, eta: 0:16:46, time: 0.204, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1832, loss_rpn_bbox: 0.0690, loss_cls: 0.3792, acc: 90.6475, loss_bbox: 0.2958, loss: 0.9273, grad_norm: 1.2567\n",
-      "2021-08-15 12:35:56,117 - mmdet - INFO - Epoch [5][550/655]\tlr: 2.000e-02, eta: 0:16:35, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1989, loss_rpn_bbox: 0.0791, loss_cls: 0.4043, acc: 89.7500, loss_bbox: 0.3276, loss: 1.0099, grad_norm: 1.3055\n",
-      "2021-08-15 12:36:06,529 - mmdet - INFO - Epoch [5][600/655]\tlr: 2.000e-02, eta: 0:16:24, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1822, loss_rpn_bbox: 0.0764, loss_cls: 0.4006, acc: 90.0137, loss_bbox: 0.3143, loss: 0.9735, grad_norm: 1.2480\n",
-      "2021-08-15 12:36:17,173 - mmdet - INFO - Epoch [5][650/655]\tlr: 2.000e-02, eta: 0:16:14, time: 0.213, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1890, loss_rpn_bbox: 0.0803, loss_cls: 0.4153, acc: 89.6689, loss_bbox: 0.3290, loss: 1.0136, grad_norm: 1.2516\n",
-      "2021-08-15 12:36:18,229 - mmdet - INFO - Saving checkpoint at 5 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 35.3 task/s, elapsed: 19s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:36:38,106 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.03s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=3.52s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:36:42,923 - mmdet - INFO - Epoch(val) [5][655]\tbbox_mAP: 0.0170, bbox_mAP_50: 0.0440, bbox_mAP_75: 0.0100, bbox_mAP_s: 0.0030, bbox_mAP_m: 0.0120, bbox_mAP_l: 0.0420, bbox_mAP_copypaste: 0.017 0.044 0.010 0.003 0.012 0.042\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=0.72s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.017\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.044\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.010\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.003\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.012\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.042\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.068\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.068\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.068\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.008\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.037\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.180\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:36:55,580 - mmdet - INFO - Epoch [6][50/655]\tlr: 2.000e-02, eta: 0:16:03, time: 0.253, data_time: 0.050, memory: 2100, loss_rpn_cls: 0.1764, loss_rpn_bbox: 0.0716, loss_cls: 0.3662, acc: 90.7295, loss_bbox: 0.2954, loss: 0.9096, grad_norm: 1.2196\n",
-      "2021-08-15 12:37:06,213 - mmdet - INFO - Epoch [6][100/655]\tlr: 2.000e-02, eta: 0:15:53, time: 0.213, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1900, loss_rpn_bbox: 0.0803, loss_cls: 0.4092, acc: 89.6982, loss_bbox: 0.3214, loss: 1.0008, grad_norm: 1.2967\n",
-      "2021-08-15 12:37:16,624 - mmdet - INFO - Epoch [6][150/655]\tlr: 2.000e-02, eta: 0:15:42, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1820, loss_rpn_bbox: 0.0769, loss_cls: 0.4246, acc: 89.2686, loss_bbox: 0.3393, loss: 1.0227, grad_norm: 1.2178\n",
-      "2021-08-15 12:37:27,063 - mmdet - INFO - Epoch [6][200/655]\tlr: 2.000e-02, eta: 0:15:31, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.2108, loss_rpn_bbox: 0.0823, loss_cls: 0.4092, acc: 89.6904, loss_bbox: 0.3251, loss: 1.0274, grad_norm: 1.2745\n",
-      "2021-08-15 12:37:37,248 - mmdet - INFO - Epoch [6][250/655]\tlr: 2.000e-02, eta: 0:15:20, time: 0.204, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1689, loss_rpn_bbox: 0.0683, loss_cls: 0.3800, acc: 90.1377, loss_bbox: 0.3107, loss: 0.9279, grad_norm: 1.1747\n",
-      "2021-08-15 12:37:47,602 - mmdet - INFO - Epoch [6][300/655]\tlr: 2.000e-02, eta: 0:15:09, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1769, loss_rpn_bbox: 0.0722, loss_cls: 0.3857, acc: 90.2354, loss_bbox: 0.3138, loss: 0.9486, grad_norm: 1.1976\n",
-      "2021-08-15 12:37:58,038 - mmdet - INFO - Epoch [6][350/655]\tlr: 2.000e-02, eta: 0:14:58, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1728, loss_rpn_bbox: 0.0704, loss_cls: 0.4087, acc: 89.3232, loss_bbox: 0.3373, loss: 0.9893, grad_norm: 1.1844\n",
-      "2021-08-15 12:38:08,484 - mmdet - INFO - Epoch [6][400/655]\tlr: 2.000e-02, eta: 0:14:47, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1772, loss_rpn_bbox: 0.0726, loss_cls: 0.4040, acc: 89.6475, loss_bbox: 0.3304, loss: 0.9843, grad_norm: 1.2683\n",
-      "2021-08-15 12:38:18,861 - mmdet - INFO - Epoch [6][450/655]\tlr: 2.000e-02, eta: 0:14:36, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1891, loss_rpn_bbox: 0.0816, loss_cls: 0.4266, acc: 88.5732, loss_bbox: 0.3679, loss: 1.0652, grad_norm: 1.3088\n",
-      "2021-08-15 12:38:29,210 - mmdet - INFO - Epoch [6][500/655]\tlr: 2.000e-02, eta: 0:14:25, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1742, loss_rpn_bbox: 0.0718, loss_cls: 0.3745, acc: 90.2529, loss_bbox: 0.3088, loss: 0.9293, grad_norm: 1.2998\n",
-      "2021-08-15 12:38:39,751 - mmdet - INFO - Epoch [6][550/655]\tlr: 2.000e-02, eta: 0:14:15, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1854, loss_rpn_bbox: 0.0781, loss_cls: 0.3935, acc: 89.7334, loss_bbox: 0.3256, loss: 0.9826, grad_norm: 1.3119\n",
-      "2021-08-15 12:38:50,212 - mmdet - INFO - Epoch [6][600/655]\tlr: 2.000e-02, eta: 0:14:04, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1706, loss_rpn_bbox: 0.0777, loss_cls: 0.4022, acc: 89.6094, loss_bbox: 0.3321, loss: 0.9827, grad_norm: 1.2767\n",
-      "2021-08-15 12:39:00,681 - mmdet - INFO - Epoch [6][650/655]\tlr: 2.000e-02, eta: 0:13:53, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1758, loss_rpn_bbox: 0.0763, loss_cls: 0.4204, acc: 89.1045, loss_bbox: 0.3477, loss: 1.0203, grad_norm: 1.2833\n",
-      "2021-08-15 12:39:01,725 - mmdet - INFO - Saving checkpoint at 6 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 35.4 task/s, elapsed: 19s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:39:22,260 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.43s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=5.60s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:39:29,719 - mmdet - INFO - Epoch(val) [6][655]\tbbox_mAP: 0.0230, bbox_mAP_50: 0.0590, bbox_mAP_75: 0.0130, bbox_mAP_s: 0.0040, bbox_mAP_m: 0.0230, bbox_mAP_l: 0.0510, bbox_mAP_copypaste: 0.023 0.059 0.013 0.004 0.023 0.051\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=1.18s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.023\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.059\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.013\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.004\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.023\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.051\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.098\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.098\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.098\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.012\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.073\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.224\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:39:42,489 - mmdet - INFO - Epoch [7][50/655]\tlr: 2.000e-02, eta: 0:13:43, time: 0.255, data_time: 0.050, memory: 2100, loss_rpn_cls: 0.1824, loss_rpn_bbox: 0.0785, loss_cls: 0.4128, acc: 88.9062, loss_bbox: 0.3480, loss: 1.0217, grad_norm: 1.4073\n",
-      "2021-08-15 12:39:53,505 - mmdet - INFO - Epoch [7][100/655]\tlr: 2.000e-02, eta: 0:13:33, time: 0.220, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1575, loss_rpn_bbox: 0.0738, loss_cls: 0.3784, acc: 89.9590, loss_bbox: 0.3167, loss: 0.9264, grad_norm: 1.2300\n",
-      "2021-08-15 12:40:04,286 - mmdet - INFO - Epoch [7][150/655]\tlr: 2.000e-02, eta: 0:13:22, time: 0.216, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1681, loss_rpn_bbox: 0.0705, loss_cls: 0.4195, acc: 89.2256, loss_bbox: 0.3365, loss: 0.9946, grad_norm: 1.3451\n",
-      "2021-08-15 12:40:14,674 - mmdet - INFO - Epoch [7][200/655]\tlr: 2.000e-02, eta: 0:13:11, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1654, loss_rpn_bbox: 0.0796, loss_cls: 0.4078, acc: 89.7646, loss_bbox: 0.3178, loss: 0.9706, grad_norm: 1.3185\n",
-      "2021-08-15 12:40:24,947 - mmdet - INFO - Epoch [7][250/655]\tlr: 2.000e-02, eta: 0:13:00, time: 0.205, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1688, loss_rpn_bbox: 0.0734, loss_cls: 0.4052, acc: 89.2773, loss_bbox: 0.3460, loss: 0.9934, grad_norm: 1.2930\n",
-      "2021-08-15 12:40:35,177 - mmdet - INFO - Epoch [7][300/655]\tlr: 2.000e-02, eta: 0:12:49, time: 0.205, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1899, loss_rpn_bbox: 0.0739, loss_cls: 0.4108, acc: 89.3877, loss_bbox: 0.3289, loss: 1.0035, grad_norm: 1.3047\n",
-      "2021-08-15 12:40:45,714 - mmdet - INFO - Epoch [7][350/655]\tlr: 2.000e-02, eta: 0:12:39, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1706, loss_rpn_bbox: 0.0735, loss_cls: 0.4026, acc: 89.2744, loss_bbox: 0.3458, loss: 0.9925, grad_norm: 1.2226\n",
-      "2021-08-15 12:40:56,158 - mmdet - INFO - Epoch [7][400/655]\tlr: 2.000e-02, eta: 0:12:28, time: 0.209, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1622, loss_rpn_bbox: 0.0706, loss_cls: 0.3918, acc: 89.8623, loss_bbox: 0.3179, loss: 0.9426, grad_norm: 1.2848\n",
-      "2021-08-15 12:41:06,484 - mmdet - INFO - Epoch [7][450/655]\tlr: 2.000e-02, eta: 0:12:17, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1826, loss_rpn_bbox: 0.0734, loss_cls: 0.4053, acc: 89.4922, loss_bbox: 0.3268, loss: 0.9881, grad_norm: 1.3136\n",
-      "2021-08-15 12:41:17,257 - mmdet - INFO - Epoch [7][500/655]\tlr: 2.000e-02, eta: 0:12:07, time: 0.215, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1679, loss_rpn_bbox: 0.0808, loss_cls: 0.4174, acc: 89.2402, loss_bbox: 0.3374, loss: 1.0035, grad_norm: 1.1958\n",
-      "2021-08-15 12:41:27,627 - mmdet - INFO - Epoch [7][550/655]\tlr: 2.000e-02, eta: 0:11:56, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1541, loss_rpn_bbox: 0.0608, loss_cls: 0.3746, acc: 90.1348, loss_bbox: 0.3173, loss: 0.9069, grad_norm: 1.2779\n",
-      "2021-08-15 12:41:37,987 - mmdet - INFO - Epoch [7][600/655]\tlr: 2.000e-02, eta: 0:11:45, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1693, loss_rpn_bbox: 0.0775, loss_cls: 0.3972, acc: 89.5859, loss_bbox: 0.3334, loss: 0.9774, grad_norm: 1.3799\n",
-      "2021-08-15 12:41:48,325 - mmdet - INFO - Epoch [7][650/655]\tlr: 2.000e-02, eta: 0:11:34, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1699, loss_rpn_bbox: 0.0758, loss_cls: 0.4036, acc: 89.4531, loss_bbox: 0.3344, loss: 0.9838, grad_norm: 1.3035\n",
-      "2021-08-15 12:41:49,367 - mmdet - INFO - Saving checkpoint at 7 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 35.8 task/s, elapsed: 18s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:42:09,135 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.42s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=4.62s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:42:15,750 - mmdet - INFO - Epoch(val) [7][655]\tbbox_mAP: 0.0240, bbox_mAP_50: 0.0590, bbox_mAP_75: 0.0140, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0250, bbox_mAP_l: 0.0540, bbox_mAP_copypaste: 0.024 0.059 0.014 0.005 0.025 0.054\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=0.99s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.059\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.014\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.005\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.025\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.054\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.090\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.090\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.090\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.010\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.056\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.215\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:42:28,600 - mmdet - INFO - Epoch [8][50/655]\tlr: 2.000e-02, eta: 0:11:24, time: 0.257, data_time: 0.050, memory: 2100, loss_rpn_cls: 0.1525, loss_rpn_bbox: 0.0679, loss_cls: 0.3790, acc: 89.9443, loss_bbox: 0.3186, loss: 0.9180, grad_norm: 1.2161\n",
-      "2021-08-15 12:42:39,209 - mmdet - INFO - Epoch [8][100/655]\tlr: 2.000e-02, eta: 0:11:13, time: 0.212, data_time: 0.007, memory: 2100, loss_rpn_cls: 0.1775, loss_rpn_bbox: 0.0774, loss_cls: 0.4101, acc: 89.4512, loss_bbox: 0.3345, loss: 0.9995, grad_norm: 1.3599\n",
-      "2021-08-15 12:42:49,599 - mmdet - INFO - Epoch [8][150/655]\tlr: 2.000e-02, eta: 0:11:02, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1766, loss_rpn_bbox: 0.0667, loss_cls: 0.3919, acc: 89.7764, loss_bbox: 0.3194, loss: 0.9546, grad_norm: 1.3244\n",
-      "2021-08-15 12:42:59,835 - mmdet - INFO - Epoch [8][200/655]\tlr: 2.000e-02, eta: 0:10:51, time: 0.205, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1535, loss_rpn_bbox: 0.0652, loss_cls: 0.3619, acc: 90.4434, loss_bbox: 0.2956, loss: 0.8762, grad_norm: 1.1629\n",
-      "2021-08-15 12:43:10,183 - mmdet - INFO - Epoch [8][250/655]\tlr: 2.000e-02, eta: 0:10:41, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1700, loss_rpn_bbox: 0.0789, loss_cls: 0.4156, acc: 88.7012, loss_bbox: 0.3511, loss: 1.0156, grad_norm: 1.3627\n",
-      "2021-08-15 12:43:20,483 - mmdet - INFO - Epoch [8][300/655]\tlr: 2.000e-02, eta: 0:10:30, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1682, loss_rpn_bbox: 0.0731, loss_cls: 0.4041, acc: 89.6113, loss_bbox: 0.3214, loss: 0.9668, grad_norm: 1.3513\n",
-      "2021-08-15 12:43:30,767 - mmdet - INFO - Epoch [8][350/655]\tlr: 2.000e-02, eta: 0:10:19, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1629, loss_rpn_bbox: 0.0755, loss_cls: 0.4089, acc: 89.1250, loss_bbox: 0.3447, loss: 0.9920, grad_norm: 1.2481\n",
-      "2021-08-15 12:43:41,266 - mmdet - INFO - Epoch [8][400/655]\tlr: 2.000e-02, eta: 0:10:08, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1468, loss_rpn_bbox: 0.0641, loss_cls: 0.3929, acc: 89.6182, loss_bbox: 0.3266, loss: 0.9304, grad_norm: 1.2996\n",
-      "2021-08-15 12:43:51,603 - mmdet - INFO - Epoch [8][450/655]\tlr: 2.000e-02, eta: 0:09:58, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1497, loss_rpn_bbox: 0.0635, loss_cls: 0.4138, acc: 89.2119, loss_bbox: 0.3420, loss: 0.9689, grad_norm: 1.3387\n",
-      "2021-08-15 12:44:01,915 - mmdet - INFO - Epoch [8][500/655]\tlr: 2.000e-02, eta: 0:09:47, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1646, loss_rpn_bbox: 0.0770, loss_cls: 0.4143, acc: 89.2354, loss_bbox: 0.3332, loss: 0.9892, grad_norm: 1.2746\n",
-      "2021-08-15 12:44:12,412 - mmdet - INFO - Epoch [8][550/655]\tlr: 2.000e-02, eta: 0:09:36, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1658, loss_rpn_bbox: 0.0750, loss_cls: 0.3906, acc: 89.6396, loss_bbox: 0.3234, loss: 0.9548, grad_norm: 1.2955\n",
-      "2021-08-15 12:44:22,829 - mmdet - INFO - Epoch [8][600/655]\tlr: 2.000e-02, eta: 0:09:26, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1854, loss_rpn_bbox: 0.0919, loss_cls: 0.4187, acc: 88.8721, loss_bbox: 0.3460, loss: 1.0420, grad_norm: 1.3690\n",
-      "2021-08-15 12:44:33,195 - mmdet - INFO - Epoch [8][650/655]\tlr: 2.000e-02, eta: 0:09:15, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1464, loss_rpn_bbox: 0.0712, loss_cls: 0.4026, acc: 89.3408, loss_bbox: 0.3401, loss: 0.9604, grad_norm: 1.2474\n",
-      "2021-08-15 12:44:34,231 - mmdet - INFO - Saving checkpoint at 8 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 35.3 task/s, elapsed: 19s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:44:54,216 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.04s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=4.24s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:44:59,814 - mmdet - INFO - Epoch(val) [8][655]\tbbox_mAP: 0.0290, bbox_mAP_50: 0.0690, bbox_mAP_75: 0.0220, bbox_mAP_s: 0.0080, bbox_mAP_m: 0.0220, bbox_mAP_l: 0.0670, bbox_mAP_copypaste: 0.029 0.069 0.022 0.008 0.022 0.067\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=0.81s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.069\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.008\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.022\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.067\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.093\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.093\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.093\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.017\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.065\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.209\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:45:12,379 - mmdet - INFO - Epoch [9][50/655]\tlr: 2.000e-03, eta: 0:09:04, time: 0.251, data_time: 0.049, memory: 2100, loss_rpn_cls: 0.1385, loss_rpn_bbox: 0.0695, loss_cls: 0.4095, acc: 89.0859, loss_bbox: 0.3427, loss: 0.9601, grad_norm: 1.2280\n",
-      "2021-08-15 12:45:22,595 - mmdet - INFO - Epoch [9][100/655]\tlr: 2.000e-03, eta: 0:08:53, time: 0.204, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1238, loss_rpn_bbox: 0.0602, loss_cls: 0.3552, acc: 90.3457, loss_bbox: 0.3017, loss: 0.8409, grad_norm: 1.1521\n",
-      "2021-08-15 12:45:32,755 - mmdet - INFO - Epoch [9][150/655]\tlr: 2.000e-03, eta: 0:08:42, time: 0.203, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1252, loss_rpn_bbox: 0.0605, loss_cls: 0.3833, acc: 89.7236, loss_bbox: 0.3230, loss: 0.8920, grad_norm: 1.2295\n",
-      "2021-08-15 12:45:43,059 - mmdet - INFO - Epoch [9][200/655]\tlr: 2.000e-03, eta: 0:08:32, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1460, loss_rpn_bbox: 0.0674, loss_cls: 0.3720, acc: 89.9189, loss_bbox: 0.3225, loss: 0.9080, grad_norm: 1.1879\n",
-      "2021-08-15 12:45:53,657 - mmdet - INFO - Epoch [9][250/655]\tlr: 2.000e-03, eta: 0:08:21, time: 0.212, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1310, loss_rpn_bbox: 0.0631, loss_cls: 0.3917, acc: 89.5107, loss_bbox: 0.3315, loss: 0.9174, grad_norm: 1.1909\n",
-      "2021-08-15 12:46:03,936 - mmdet - INFO - Epoch [9][300/655]\tlr: 2.000e-03, eta: 0:08:10, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1345, loss_rpn_bbox: 0.0638, loss_cls: 0.3882, acc: 89.2002, loss_bbox: 0.3467, loss: 0.9332, grad_norm: 1.2446\n",
-      "2021-08-15 12:46:14,258 - mmdet - INFO - Epoch [9][350/655]\tlr: 2.000e-03, eta: 0:08:00, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1309, loss_rpn_bbox: 0.0607, loss_cls: 0.3675, acc: 89.5566, loss_bbox: 0.3331, loss: 0.8923, grad_norm: 1.2135\n",
-      "2021-08-15 12:46:24,666 - mmdet - INFO - Epoch [9][400/655]\tlr: 2.000e-03, eta: 0:07:49, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1434, loss_rpn_bbox: 0.0705, loss_cls: 0.4057, acc: 88.6504, loss_bbox: 0.3620, loss: 0.9816, grad_norm: 1.3854\n",
-      "2021-08-15 12:46:34,968 - mmdet - INFO - Epoch [9][450/655]\tlr: 2.000e-03, eta: 0:07:38, time: 0.206, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1409, loss_rpn_bbox: 0.0676, loss_cls: 0.3907, acc: 89.3086, loss_bbox: 0.3340, loss: 0.9332, grad_norm: 1.2583\n",
-      "2021-08-15 12:46:45,201 - mmdet - INFO - Epoch [9][500/655]\tlr: 2.000e-03, eta: 0:07:28, time: 0.205, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1399, loss_rpn_bbox: 0.0610, loss_cls: 0.3897, acc: 89.3135, loss_bbox: 0.3407, loss: 0.9313, grad_norm: 1.2559\n",
-      "2021-08-15 12:46:55,476 - mmdet - INFO - Epoch [9][550/655]\tlr: 2.000e-03, eta: 0:07:17, time: 0.205, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1421, loss_rpn_bbox: 0.0706, loss_cls: 0.4083, acc: 88.9502, loss_bbox: 0.3505, loss: 0.9715, grad_norm: 1.2802\n",
-      "2021-08-15 12:47:05,861 - mmdet - INFO - Epoch [9][600/655]\tlr: 2.000e-03, eta: 0:07:06, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1452, loss_rpn_bbox: 0.0753, loss_cls: 0.4054, acc: 88.5693, loss_bbox: 0.3696, loss: 0.9956, grad_norm: 1.3317\n",
-      "2021-08-15 12:47:16,341 - mmdet - INFO - Epoch [9][650/655]\tlr: 2.000e-03, eta: 0:06:56, time: 0.210, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1499, loss_rpn_bbox: 0.0770, loss_cls: 0.4107, acc: 88.8984, loss_bbox: 0.3545, loss: 0.9922, grad_norm: 1.3678\n",
-      "2021-08-15 12:47:17,395 - mmdet - INFO - Saving checkpoint at 9 epochs\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 655/655, 36.2 task/s, elapsed: 18s, ETA:     0s"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:47:37,356 - mmdet - INFO - Evaluating bbox...\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Loading and preparing results...\n",
-      "DONE (t=0.05s)\n",
-      "creating index...\n",
-      "index created!\n",
-      "Running per image evaluation...\n",
-      "Evaluate annotation type *bbox*\n",
-      "DONE (t=5.07s).\n",
-      "Accumulating evaluation results...\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:47:44,032 - mmdet - INFO - Epoch(val) [9][655]\tbbox_mAP: 0.0340, bbox_mAP_50: 0.0790, bbox_mAP_75: 0.0250, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0300, bbox_mAP_l: 0.0750, bbox_mAP_copypaste: 0.034 0.079 0.025 0.005 0.030 0.075\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "DONE (t=1.00s).\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
-      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.079\n",
-      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.005\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.030\n",
-      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.075\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.113\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.113\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.018\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.087\n",
-      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.244\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "2021-08-15 12:47:56,642 - mmdet - INFO - Epoch [10][50/655]\tlr: 2.000e-03, eta: 0:06:44, time: 0.252, data_time: 0.050, memory: 2100, loss_rpn_cls: 0.1400, loss_rpn_bbox: 0.0678, loss_cls: 0.3866, acc: 89.4336, loss_bbox: 0.3299, loss: 0.9243, grad_norm: 1.3180\n",
-      "2021-08-15 12:48:07,053 - mmdet - INFO - Epoch [10][100/655]\tlr: 2.000e-03, eta: 0:06:34, time: 0.208, data_time: 0.007, memory: 2100, loss_rpn_cls: 0.1264, loss_rpn_bbox: 0.0633, loss_cls: 0.3801, acc: 89.4619, loss_bbox: 0.3351, loss: 0.9049, grad_norm: 1.2889\n",
-      "2021-08-15 12:48:17,467 - mmdet - INFO - Epoch [10][150/655]\tlr: 2.000e-03, eta: 0:06:23, time: 0.208, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1497, loss_rpn_bbox: 0.0717, loss_cls: 0.3983, acc: 88.9912, loss_bbox: 0.3449, loss: 0.9647, grad_norm: 1.3868\n",
-      "2021-08-15 12:48:27,810 - mmdet - INFO - Epoch [10][200/655]\tlr: 2.000e-03, eta: 0:06:12, time: 0.207, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1257, loss_rpn_bbox: 0.0632, loss_cls: 0.3774, acc: 89.3281, loss_bbox: 0.3383, loss: 0.9047, grad_norm: 1.2777\n",
-      "2021-08-15 12:48:38,486 - mmdet - INFO - Epoch [10][250/655]\tlr: 2.000e-03, eta: 0:06:02, time: 0.214, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1359, loss_rpn_bbox: 0.0661, loss_cls: 0.3720, acc: 89.6338, loss_bbox: 0.3267, loss: 0.9008, grad_norm: 1.3602\n",
-      "2021-08-15 12:48:49,013 - mmdet - INFO - Epoch [10][300/655]\tlr: 2.000e-03, eta: 0:05:51, time: 0.211, data_time: 0.006, memory: 2100, loss_rpn_cls: 0.1309, loss_rpn_bbox: 0.0641, loss_cls: 0.3640, acc: 89.9521, loss_bbox: 0.3156, loss: 0.8746, grad_norm: 1.2649\n"
-     ]
-    },
-    {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "\u001b[0;32m/tmp/ipykernel_57974/991485170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/apis/train.py\u001b[0m in \u001b[0;36mtrain_detector\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
-      "\u001b[0;32m~/anaconda3/envs/hyelin/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mepoch_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/hyelin/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/hyelin/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun_iter\u001b[0;34m(self, data_batch, train_mode, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             outputs = self.model.train_step(data_batch, self.optimizer,\n\u001b[0;32m---> 30\u001b[0;31m                                             **kwargs)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/hyelin/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/detectors/base.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data, optimizer)\u001b[0m\n\u001b[1;32m    235\u001b[0m                   \u001b[0maveraging\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/hyelin/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/hyelin/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                                 'method of nn.Module')\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp16_enabled'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mold_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# get the arg spec of the decorated method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/detectors/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/detectors/two_stage.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore, gt_masks, proposals, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mgt_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 proposal_cfg=proposal_cfg)\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpn_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/dense_heads/base_dense_head.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, x, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore, proposal_cfg, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgt_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproposal_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/dense_heads/rpn_head.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, cls_scores, bbox_preds, gt_bboxes, img_metas, gt_bboxes_ignore)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             gt_bboxes_ignore=gt_bboxes_ignore)\n\u001b[0m\u001b[1;32m     75\u001b[0m         return dict(\n\u001b[1;32m     76\u001b[0m             loss_rpn_cls=losses['loss_cls'], loss_rpn_bbox=losses['loss_bbox'])\n",
-      "\u001b[0;32m~/anaconda3/envs/hyelin/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m                                 'method of nn.Module')\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp16_enabled'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mold_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;31m# get the arg spec of the decorated method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0margs_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/dense_heads/anchor_head.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mgt_bboxes_ignore_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mgt_labels_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             label_channels=label_channels)\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls_reg_targets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/dense_heads/anchor_head.py\u001b[0m in \u001b[0;36mget_targets\u001b[0;34m(self, anchor_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list, gt_labels_list, label_channels, unmap_outputs, return_sampling_results)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mlabel_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             unmap_outputs=unmap_outputs)\n\u001b[0m\u001b[1;32m    346\u001b[0m         (all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n\u001b[1;32m    347\u001b[0m          pos_inds_list, neg_inds_list, sampling_results_list) = results[:7]\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/core/utils/misc.py\u001b[0m in \u001b[0;36mmulti_apply\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmap_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/models/dense_heads/anchor_head.py\u001b[0m in \u001b[0;36m_get_targets_single\u001b[0;34m(self, flat_anchors, valid_flags, gt_bboxes, gt_bboxes_ignore, gt_labels, img_meta, label_channels, unmap_outputs)\u001b[0m\n\u001b[1;32m    219\u001b[0m             None if self.sampling else gt_labels)\n\u001b[1;32m    220\u001b[0m         sampling_result = self.sampler.sample(assign_result, anchors,\n\u001b[0;32m--> 221\u001b[0;31m                                               gt_bboxes)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mnum_valid_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/core/bbox/samplers/base_sampler.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, assign_result, bboxes, gt_bboxes, gt_labels, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mnum_expected_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_upper_bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         neg_inds = self.neg_sampler._sample_neg(\n\u001b[0;32m---> 96\u001b[0;31m             assign_result, num_expected_neg, bboxes=bboxes, **kwargs)\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mneg_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_inds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/core/bbox/samplers/random_sampler.py\u001b[0m in \u001b[0;36m_sample_neg\u001b[0;34m(self, assign_result, num_expected, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mneg_inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_expected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
-      "\u001b[0;32m/home/hyelin/mmdetection/mmdet/core/bbox/samplers/random_sampler.py\u001b[0m in \u001b[0;36mrandom_choice\u001b[0;34m(self, gallery, num)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# when PyTorch fixes the abnormal return of torch.randperm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# See: https://github.com/open-mmlab/mmdetection/pull/5014\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgallery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgallery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mrand_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgallery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
-     ]
-    }
-   ],
-   "source": [
-    "train_detector(model, datasets[0], cfg, distributed=False, validate=True)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "interpreter": {
-   "hash": "980dec4bdc0f65d3f181e5891661df87e8769cde5e79cd54bc145a7f830b2685"
-  },
-  "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.7.11"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
diff --git a/mmdet/__init__.py b/mmdet/__init__.py
index 646ee84e..8c1f2d04 100644
--- a/mmdet/__init__.py
+++ b/mmdet/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 
 from .version import __version__, short_version
diff --git a/mmdet/apis/__init__.py b/mmdet/apis/__init__.py
index 1d8035b7..4a8987d1 100644
--- a/mmdet/apis/__init__.py
+++ b/mmdet/apis/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .inference import (async_inference_detector, inference_detector,
                         init_detector, show_result_pyplot)
 from .test import multi_gpu_test, single_gpu_test
diff --git a/mmdet/apis/inference.py b/mmdet/apis/inference.py
index c257c7f2..6b4b0096 100644
--- a/mmdet/apis/inference.py
+++ b/mmdet/apis/inference.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import mmcv
diff --git a/mmdet/apis/test.py b/mmdet/apis/test.py
index e54b1b8c..1060292c 100644
--- a/mmdet/apis/test.py
+++ b/mmdet/apis/test.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import pickle
 import shutil
diff --git a/mmdet/apis/train.py b/mmdet/apis/train.py
index 3dc943c3..402605ad 100644
--- a/mmdet/apis/train.py
+++ b/mmdet/apis/train.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import random
 import warnings
 
@@ -147,7 +148,10 @@ def train_detector(model,
         eval_cfg = cfg.get('evaluation', {})
         eval_cfg['by_epoch'] = cfg.runner['type'] != 'IterBasedRunner'
         eval_hook = DistEvalHook if distributed else EvalHook
-        runner.register_hook(eval_hook(val_dataloader, **eval_cfg))
+        # In this PR (https://github.com/open-mmlab/mmcv/pull/1193), the
+        # priority of IterTimerHook has been modified from 'NORMAL' to 'LOW'.
+        runner.register_hook(
+            eval_hook(val_dataloader, **eval_cfg), priority='LOW')
 
     # user-defined hooks
     if cfg.get('custom_hooks', None):
diff --git a/mmdet/core/__init__.py b/mmdet/core/__init__.py
index 47852700..7eca58cf 100644
--- a/mmdet/core/__init__.py
+++ b/mmdet/core/__init__.py
@@ -1,5 +1,7 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .anchor import *  # noqa: F401, F403
 from .bbox import *  # noqa: F401, F403
+from .data_structures import *  # noqa: F401, F403
 from .evaluation import *  # noqa: F401, F403
 from .hook import *  # noqa: F401, F403
 from .mask import *  # noqa: F401, F403
diff --git a/mmdet/core/anchor/__init__.py b/mmdet/core/anchor/__init__.py
index f14dc174..fcc7e4af 100644
--- a/mmdet/core/anchor/__init__.py
+++ b/mmdet/core/anchor/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .anchor_generator import (AnchorGenerator, LegacyAnchorGenerator,
                                YOLOAnchorGenerator)
 from .builder import (ANCHOR_GENERATORS, PRIOR_GENERATORS,
diff --git a/mmdet/core/anchor/anchor_generator.py b/mmdet/core/anchor/anchor_generator.py
index 596cfa0b..7cdf9516 100644
--- a/mmdet/core/anchor/anchor_generator.py
+++ b/mmdet/core/anchor/anchor_generator.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import mmcv
diff --git a/mmdet/core/anchor/builder.py b/mmdet/core/anchor/builder.py
index d53a6242..ddb25ad3 100644
--- a/mmdet/core/anchor/builder.py
+++ b/mmdet/core/anchor/builder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 from mmcv.utils import Registry, build_from_cfg
diff --git a/mmdet/core/anchor/point_generator.py b/mmdet/core/anchor/point_generator.py
index 7b11a855..e100854f 100644
--- a/mmdet/core/anchor/point_generator.py
+++ b/mmdet/core/anchor/point_generator.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 from torch.nn.modules.utils import _pair
diff --git a/mmdet/core/anchor/utils.py b/mmdet/core/anchor/utils.py
index ab9b53f3..c2f20247 100644
--- a/mmdet/core/anchor/utils.py
+++ b/mmdet/core/anchor/utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 
diff --git a/mmdet/core/bbox/__init__.py b/mmdet/core/bbox/__init__.py
index a3537297..a7b803a3 100644
--- a/mmdet/core/bbox/__init__.py
+++ b/mmdet/core/bbox/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .assigners import (AssignResult, BaseAssigner, CenterRegionAssigner,
                         MaxIoUAssigner, RegionAssigner)
 from .builder import build_assigner, build_bbox_coder, build_sampler
diff --git a/mmdet/core/bbox/assigners/__init__.py b/mmdet/core/bbox/assigners/__init__.py
index 7bb5e8af..2fef4be8 100644
--- a/mmdet/core/bbox/assigners/__init__.py
+++ b/mmdet/core/bbox/assigners/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .approx_max_iou_assigner import ApproxMaxIoUAssigner
 from .assign_result import AssignResult
 from .atss_assigner import ATSSAssigner
diff --git a/mmdet/core/bbox/assigners/approx_max_iou_assigner.py b/mmdet/core/bbox/assigners/approx_max_iou_assigner.py
index 6d07656d..304d09c3 100644
--- a/mmdet/core/bbox/assigners/approx_max_iou_assigner.py
+++ b/mmdet/core/bbox/assigners/approx_max_iou_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/assigners/assign_result.py b/mmdet/core/bbox/assigners/assign_result.py
index 6a16ca20..eda0a01d 100644
--- a/mmdet/core/bbox/assigners/assign_result.py
+++ b/mmdet/core/bbox/assigners/assign_result.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.utils import util_mixins
diff --git a/mmdet/core/bbox/assigners/atss_assigner.py b/mmdet/core/bbox/assigners/atss_assigner.py
index d4fe9d0e..7b195303 100644
--- a/mmdet/core/bbox/assigners/atss_assigner.py
+++ b/mmdet/core/bbox/assigners/atss_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/assigners/base_assigner.py b/mmdet/core/bbox/assigners/base_assigner.py
index 1ff0160d..3c2d597a 100644
--- a/mmdet/core/bbox/assigners/base_assigner.py
+++ b/mmdet/core/bbox/assigners/base_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 
 
diff --git a/mmdet/core/bbox/assigners/center_region_assigner.py b/mmdet/core/bbox/assigners/center_region_assigner.py
index 488e3b61..86e78597 100644
--- a/mmdet/core/bbox/assigners/center_region_assigner.py
+++ b/mmdet/core/bbox/assigners/center_region_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/assigners/grid_assigner.py b/mmdet/core/bbox/assigners/grid_assigner.py
index 7390ea63..63078d6c 100644
--- a/mmdet/core/bbox/assigners/grid_assigner.py
+++ b/mmdet/core/bbox/assigners/grid_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/assigners/hungarian_assigner.py b/mmdet/core/bbox/assigners/hungarian_assigner.py
index e10cc14a..4105fb5c 100644
--- a/mmdet/core/bbox/assigners/hungarian_assigner.py
+++ b/mmdet/core/bbox/assigners/hungarian_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/assigners/max_iou_assigner.py b/mmdet/core/bbox/assigners/max_iou_assigner.py
index 5cf4c4b4..bfaa96ef 100644
--- a/mmdet/core/bbox/assigners/max_iou_assigner.py
+++ b/mmdet/core/bbox/assigners/max_iou_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/assigners/point_assigner.py b/mmdet/core/bbox/assigners/point_assigner.py
index fb8f5e4e..b0dc2246 100644
--- a/mmdet/core/bbox/assigners/point_assigner.py
+++ b/mmdet/core/bbox/assigners/point_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/assigners/region_assigner.py b/mmdet/core/bbox/assigners/region_assigner.py
index a4aadef4..1833b894 100644
--- a/mmdet/core/bbox/assigners/region_assigner.py
+++ b/mmdet/core/bbox/assigners/region_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import anchor_inside_flags
diff --git a/mmdet/core/bbox/assigners/sim_ota_assigner.py b/mmdet/core/bbox/assigners/sim_ota_assigner.py
index ebb86855..4d00f53a 100644
--- a/mmdet/core/bbox/assigners/sim_ota_assigner.py
+++ b/mmdet/core/bbox/assigners/sim_ota_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch
diff --git a/mmdet/core/bbox/assigners/uniform_assigner.py b/mmdet/core/bbox/assigners/uniform_assigner.py
index 1d606dee..70294fc4 100644
--- a/mmdet/core/bbox/assigners/uniform_assigner.py
+++ b/mmdet/core/bbox/assigners/uniform_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_ASSIGNERS
diff --git a/mmdet/core/bbox/builder.py b/mmdet/core/bbox/builder.py
index 682683b6..9cfa055b 100644
--- a/mmdet/core/bbox/builder.py
+++ b/mmdet/core/bbox/builder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.utils import Registry, build_from_cfg
 
 BBOX_ASSIGNERS = Registry('bbox_assigner')
diff --git a/mmdet/core/bbox/coder/__init__.py b/mmdet/core/bbox/coder/__init__.py
index ae455ba8..4c7db000 100644
--- a/mmdet/core/bbox/coder/__init__.py
+++ b/mmdet/core/bbox/coder/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .base_bbox_coder import BaseBBoxCoder
 from .bucketing_bbox_coder import BucketingBBoxCoder
 from .delta_xywh_bbox_coder import DeltaXYWHBBoxCoder
diff --git a/mmdet/core/bbox/coder/base_bbox_coder.py b/mmdet/core/bbox/coder/base_bbox_coder.py
index cf0b34c7..a7ed041a 100644
--- a/mmdet/core/bbox/coder/base_bbox_coder.py
+++ b/mmdet/core/bbox/coder/base_bbox_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 
 
diff --git a/mmdet/core/bbox/coder/bucketing_bbox_coder.py b/mmdet/core/bbox/coder/bucketing_bbox_coder.py
index 92d24b45..1164b350 100644
--- a/mmdet/core/bbox/coder/bucketing_bbox_coder.py
+++ b/mmdet/core/bbox/coder/bucketing_bbox_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import numpy as np
 import torch
diff --git a/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py b/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py
index 98d30906..5688c9d1 100644
--- a/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py
+++ b/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import numpy as np
 import torch
diff --git a/mmdet/core/bbox/coder/legacy_delta_xywh_bbox_coder.py b/mmdet/core/bbox/coder/legacy_delta_xywh_bbox_coder.py
index 190309fd..7fa348b2 100644
--- a/mmdet/core/bbox/coder/legacy_delta_xywh_bbox_coder.py
+++ b/mmdet/core/bbox/coder/legacy_delta_xywh_bbox_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import numpy as np
 import torch
diff --git a/mmdet/core/bbox/coder/pseudo_bbox_coder.py b/mmdet/core/bbox/coder/pseudo_bbox_coder.py
index 1c8346f4..fe71f369 100644
--- a/mmdet/core/bbox/coder/pseudo_bbox_coder.py
+++ b/mmdet/core/bbox/coder/pseudo_bbox_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import BBOX_CODERS
 from .base_bbox_coder import BaseBBoxCoder
 
diff --git a/mmdet/core/bbox/coder/tblr_bbox_coder.py b/mmdet/core/bbox/coder/tblr_bbox_coder.py
index c45c6167..cb420663 100644
--- a/mmdet/core/bbox/coder/tblr_bbox_coder.py
+++ b/mmdet/core/bbox/coder/tblr_bbox_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/mmdet/core/bbox/coder/yolo_bbox_coder.py b/mmdet/core/bbox/coder/yolo_bbox_coder.py
index d6d0e82a..2852eca7 100644
--- a/mmdet/core/bbox/coder/yolo_bbox_coder.py
+++ b/mmdet/core/bbox/coder/yolo_bbox_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
@@ -69,21 +70,14 @@ class YOLOBBoxCoder(BaseBBoxCoder):
         Returns:
             torch.Tensor: Decoded boxes.
         """
-        assert pred_bboxes.size(0) == bboxes.size(0)
         assert pred_bboxes.size(-1) == bboxes.size(-1) == 4
-        x_center = (bboxes[..., 0] + bboxes[..., 2]) * 0.5
-        y_center = (bboxes[..., 1] + bboxes[..., 3]) * 0.5
-        w = bboxes[..., 2] - bboxes[..., 0]
-        h = bboxes[..., 3] - bboxes[..., 1]
-        # Get outputs x, y
-        x_center_pred = (pred_bboxes[..., 0] - 0.5) * stride + x_center
-        y_center_pred = (pred_bboxes[..., 1] - 0.5) * stride + y_center
-        w_pred = torch.exp(pred_bboxes[..., 2]) * w
-        h_pred = torch.exp(pred_bboxes[..., 3]) * h
-
+        xy_centers = (bboxes[..., :2] + bboxes[..., 2:]) * 0.5 + (
+            pred_bboxes[..., :2] - 0.5) * stride
+        whs = (bboxes[..., 2:] -
+               bboxes[..., :2]) * 0.5 * pred_bboxes[..., 2:].exp()
         decoded_bboxes = torch.stack(
-            (x_center_pred - w_pred / 2, y_center_pred - h_pred / 2,
-             x_center_pred + w_pred / 2, y_center_pred + h_pred / 2),
+            (xy_centers[..., 0] - whs[..., 0], xy_centers[..., 1] -
+             whs[..., 1], xy_centers[..., 0] + whs[..., 0],
+             xy_centers[..., 1] + whs[..., 1]),
             dim=-1)
-
         return decoded_bboxes
diff --git a/mmdet/core/bbox/demodata.py b/mmdet/core/bbox/demodata.py
index feecb693..eb24b34b 100644
--- a/mmdet/core/bbox/demodata.py
+++ b/mmdet/core/bbox/demodata.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
diff --git a/mmdet/core/bbox/iou_calculators/__init__.py b/mmdet/core/bbox/iou_calculators/__init__.py
index e71369a5..04ba925b 100644
--- a/mmdet/core/bbox/iou_calculators/__init__.py
+++ b/mmdet/core/bbox/iou_calculators/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .builder import build_iou_calculator
 from .iou2d_calculator import BboxOverlaps2D, bbox_overlaps
 
diff --git a/mmdet/core/bbox/iou_calculators/builder.py b/mmdet/core/bbox/iou_calculators/builder.py
index 09094d7e..378ee269 100644
--- a/mmdet/core/bbox/iou_calculators/builder.py
+++ b/mmdet/core/bbox/iou_calculators/builder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.utils import Registry, build_from_cfg
 
 IOU_CALCULATORS = Registry('IoU calculator')
diff --git a/mmdet/core/bbox/iou_calculators/iou2d_calculator.py b/mmdet/core/bbox/iou_calculators/iou2d_calculator.py
index 3021d0b5..4656d619 100644
--- a/mmdet/core/bbox/iou_calculators/iou2d_calculator.py
+++ b/mmdet/core/bbox/iou_calculators/iou2d_calculator.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from .builder import IOU_CALCULATORS
diff --git a/mmdet/core/bbox/match_costs/__init__.py b/mmdet/core/bbox/match_costs/__init__.py
index add5e0d3..3f79a1ce 100644
--- a/mmdet/core/bbox/match_costs/__init__.py
+++ b/mmdet/core/bbox/match_costs/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .builder import build_match_cost
 from .match_cost import BBoxL1Cost, ClassificationCost, FocalLossCost, IoUCost
 
diff --git a/mmdet/core/bbox/match_costs/builder.py b/mmdet/core/bbox/match_costs/builder.py
index 6894017d..ea086adf 100644
--- a/mmdet/core/bbox/match_costs/builder.py
+++ b/mmdet/core/bbox/match_costs/builder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.utils import Registry, build_from_cfg
 
 MATCH_COST = Registry('Match Cost')
diff --git a/mmdet/core/bbox/match_costs/match_cost.py b/mmdet/core/bbox/match_costs/match_cost.py
index ae852a5d..d5ce4ca9 100644
--- a/mmdet/core/bbox/match_costs/match_cost.py
+++ b/mmdet/core/bbox/match_costs/match_cost.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core.bbox.iou_calculators import bbox_overlaps
diff --git a/mmdet/core/bbox/samplers/__init__.py b/mmdet/core/bbox/samplers/__init__.py
index 0b06303f..b9e83913 100644
--- a/mmdet/core/bbox/samplers/__init__.py
+++ b/mmdet/core/bbox/samplers/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .base_sampler import BaseSampler
 from .combined_sampler import CombinedSampler
 from .instance_balanced_pos_sampler import InstanceBalancedPosSampler
diff --git a/mmdet/core/bbox/samplers/base_sampler.py b/mmdet/core/bbox/samplers/base_sampler.py
index 9ea35def..bd15c7c6 100644
--- a/mmdet/core/bbox/samplers/base_sampler.py
+++ b/mmdet/core/bbox/samplers/base_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 
 import torch
diff --git a/mmdet/core/bbox/samplers/combined_sampler.py b/mmdet/core/bbox/samplers/combined_sampler.py
index 564729f0..4f6d86ff 100644
--- a/mmdet/core/bbox/samplers/combined_sampler.py
+++ b/mmdet/core/bbox/samplers/combined_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import BBOX_SAMPLERS, build_sampler
 from .base_sampler import BaseSampler
 
diff --git a/mmdet/core/bbox/samplers/instance_balanced_pos_sampler.py b/mmdet/core/bbox/samplers/instance_balanced_pos_sampler.py
index c7352984..5e0d9cc0 100644
--- a/mmdet/core/bbox/samplers/instance_balanced_pos_sampler.py
+++ b/mmdet/core/bbox/samplers/instance_balanced_pos_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
diff --git a/mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py b/mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py
index f275e430..56e2874a 100644
--- a/mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py
+++ b/mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
diff --git a/mmdet/core/bbox/samplers/ohem_sampler.py b/mmdet/core/bbox/samplers/ohem_sampler.py
index 8b99f60e..43ca2f4f 100644
--- a/mmdet/core/bbox/samplers/ohem_sampler.py
+++ b/mmdet/core/bbox/samplers/ohem_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_SAMPLERS
diff --git a/mmdet/core/bbox/samplers/pseudo_sampler.py b/mmdet/core/bbox/samplers/pseudo_sampler.py
index 2bd81abc..5a563ea2 100644
--- a/mmdet/core/bbox/samplers/pseudo_sampler.py
+++ b/mmdet/core/bbox/samplers/pseudo_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_SAMPLERS
diff --git a/mmdet/core/bbox/samplers/random_sampler.py b/mmdet/core/bbox/samplers/random_sampler.py
index c23a7a1f..d09207e7 100644
--- a/mmdet/core/bbox/samplers/random_sampler.py
+++ b/mmdet/core/bbox/samplers/random_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from ..builder import BBOX_SAMPLERS
diff --git a/mmdet/core/bbox/samplers/sampling_result.py b/mmdet/core/bbox/samplers/sampling_result.py
index 1ca2d29c..6c03c7bb 100644
--- a/mmdet/core/bbox/samplers/sampling_result.py
+++ b/mmdet/core/bbox/samplers/sampling_result.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.utils import util_mixins
diff --git a/mmdet/core/bbox/samplers/score_hlr_sampler.py b/mmdet/core/bbox/samplers/score_hlr_sampler.py
index 83244ed5..f4be9b8c 100644
--- a/mmdet/core/bbox/samplers/score_hlr_sampler.py
+++ b/mmdet/core/bbox/samplers/score_hlr_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmcv.ops import nms_match
 
diff --git a/mmdet/core/bbox/transforms.py b/mmdet/core/bbox/transforms.py
index fb141f47..cfa66207 100644
--- a/mmdet/core/bbox/transforms.py
+++ b/mmdet/core/bbox/transforms.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
diff --git a/mmdet/core/data_structures/__init__.py b/mmdet/core/data_structures/__init__.py
new file mode 100644
index 00000000..11ab96c5
--- /dev/null
+++ b/mmdet/core/data_structures/__init__.py
@@ -0,0 +1,5 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from .general_data import GeneralData
+from .instance_data import InstanceData
+
+__all__ = ['GeneralData', 'InstanceData']
diff --git a/mmdet/core/data_structures/general_data.py b/mmdet/core/data_structures/general_data.py
new file mode 100644
index 00000000..bcd32201
--- /dev/null
+++ b/mmdet/core/data_structures/general_data.py
@@ -0,0 +1,316 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import copy
+
+import numpy as np
+import torch
+
+from mmdet.utils.util_mixins import NiceRepr
+
+
+class GeneralData(NiceRepr):
+    """A general data structure of OpenMMlab.
+
+    A data structure that stores the meta information,
+    the annotations of the images or the model predictions,
+    which can be used in communication between components.
+
+    The attributes in `GeneralData` are divided into two parts,
+    the `meta_info_fields` and the `data_fields` respectively.
+
+        - `meta_info_fields`: Usually contains the
+          information about the image such as filename,
+          image_shape, pad_shape, etc. All attributes in
+          it are immutable once set,
+          but the user can add new meta information with
+          `set_meta_info` function, all information can be accessed
+          with methods `meta_info_keys`, `meta_info_values`,
+          `meta_info_items`.
+
+        - `data_fields`: Annotations or model predictions are
+          stored. The attributes can be accessed or modified by
+          dict-like or object-like operations, such as
+          `.` , `[]`, `in`, `del`, `pop(str)` `get(str)`, `keys()`,
+          `values()`, `items()`. Users can also apply tensor-like methods
+          to all obj:`torch.Tensor` in the `data_fileds`,
+          such as `.cuda()`, `.cpu()`, `.numpy()`, `device`, `.to()`
+          `.detach()`, `.numpy()`
+
+    Args:
+        meta_info (dict, optional): A dict contains the meta information
+            of single image. such as `img_shape`, `scale_factor`, etc.
+            Default: None.
+        data (dict, optional): A dict contains annotations of single image or
+            model predictions. Default: None.
+
+    Examples:
+        >>> from mmdet.core import GeneralData
+        >>> img_meta = dict(img_shape=(800, 1196, 3), pad_shape=(800, 1216, 3))
+        >>> instance_data = GeneralData(meta_info=img_meta)
+        >>> img_shape in instance_data
+        True
+        >>> instance_data.det_labels = torch.LongTensor([0, 1, 2, 3])
+        >>> instance_data["det_scores"] = torch.Tensor([0.01, 0.1, 0.2, 0.3])
+        >>> print(results)
+        <GeneralData(
+
+          META INFORMATION
+        img_shape: (800, 1196, 3)
+        pad_shape: (800, 1216, 3)
+
+          DATA FIELDS
+        shape of det_labels: torch.Size([4])
+        shape of det_scores: torch.Size([4])
+
+        ) at 0x7f84acd10f90>
+        >>> instance_data.det_scores
+        tensor([0.0100, 0.1000, 0.2000, 0.3000])
+        >>> instance_data.det_labels
+        tensor([0, 1, 2, 3])
+        >>> instance_data['det_labels']
+        tensor([0, 1, 2, 3])
+        >>> 'det_labels' in instance_data
+        True
+        >>> instance_data.img_shape
+        (800, 1196, 3)
+        >>> 'det_scores' in instance_data
+        True
+        >>> del instance_data.det_scores
+        >>> 'det_scores' in instance_data
+        False
+        >>> det_labels = instance_data.pop('det_labels', None)
+        >>> det_labels
+        tensor([0, 1, 2, 3])
+        >>> 'det_labels' in instance_data
+        >>> False
+    """
+
+    def __init__(self, meta_info=None, data=None):
+
+        self._meta_info_fields = set()
+        self._data_fields = set()
+
+        if meta_info is not None:
+            self.set_meta_info(meta_info=meta_info)
+        if data is not None:
+            self.set_data(data)
+
+    def set_meta_info(self, meta_info):
+        """Add meta information.
+
+        Args:
+            meta_info (dict): A dict contains the meta information
+                of image. such as `img_shape`, `scale_factor`, etc.
+                Default: None.
+        """
+        assert isinstance(meta_info,
+                          dict), f'meta should be a `dict` but get {meta_info}'
+        meta = copy.deepcopy(meta_info)
+        for k, v in meta.items():
+            # should be consistent with original meta_info
+            if k in self._meta_info_fields:
+                ori_value = getattr(self, k)
+                if isinstance(ori_value, (torch.Tensor, np.ndarray)):
+                    if (ori_value == v).all():
+                        continue
+                    else:
+                        raise KeyError(
+                            f'img_meta_info {k} has been set as '
+                            f'{getattr(self, k)} before, which is immutable ')
+                elif ori_value == v:
+                    continue
+                else:
+                    raise KeyError(
+                        f'img_meta_info {k} has been set as '
+                        f'{getattr(self, k)} before, which is immutable ')
+            else:
+                self._meta_info_fields.add(k)
+                self.__dict__[k] = v
+
+    def set_data(self, data):
+        """Update a dict to `data_fields`.
+
+        Args:
+            data (dict): A dict contains annotations of image or
+                model predictions. Default: None.
+        """
+        assert isinstance(data,
+                          dict), f'meta should be a `dict` but get {data}'
+        for k, v in data.items():
+            self.__setattr__(k, v)
+
+    def new(self, meta_info=None, data=None):
+        """Return a new results with same image meta information.
+
+        Args:
+            meta_info (dict, optional): A dict contains the meta information
+                of image. such as `img_shape`, `scale_factor`, etc.
+                Default: None.
+            data (dict, optional): A dict contains annotations of image or
+                model predictions. Default: None.
+        """
+        new_data = self.__class__()
+        new_data.set_meta_info(dict(self.meta_info_items()))
+        if meta_info is not None:
+            new_data.set_meta_info(meta_info)
+        if data is not None:
+            new_data.set_data(data)
+        return new_data
+
+    def keys(self):
+        """
+        Returns:
+            list: Contains all keys in data_fields.
+        """
+        return [key for key in self._data_fields]
+
+    def meta_info_keys(self):
+        """
+        Returns:
+            list: Contains all keys in meta_info_fields.
+        """
+        return [key for key in self._meta_info_fields]
+
+    def values(self):
+        """
+        Returns:
+            list: Contains all values in data_fields.
+        """
+        return [getattr(self, k) for k in self.keys()]
+
+    def meta_info_values(self):
+        """
+        Returns:
+            list: Contains all values in meta_info_fields.
+        """
+        return [getattr(self, k) for k in self.meta_info_keys()]
+
+    def items(self):
+        for k in self.keys():
+            yield (k, getattr(self, k))
+
+    def meta_info_items(self):
+        for k in self.meta_info_keys():
+            yield (k, getattr(self, k))
+
+    def __setattr__(self, name, val):
+        if name in ('_meta_info_fields', '_data_fields'):
+            if not hasattr(self, name):
+                super().__setattr__(name, val)
+            else:
+                raise AttributeError(
+                    f'{name} has been used as a '
+                    f'private attribute, which is immutable. ')
+        else:
+            if name in self._meta_info_fields:
+                raise AttributeError(f'`{name}` is used in meta information,'
+                                     f'which is immutable')
+
+            self._data_fields.add(name)
+            super().__setattr__(name, val)
+
+    def __delattr__(self, item):
+
+        if item in ('_meta_info_fields', '_data_fields'):
+            raise AttributeError(f'{item} has been used as a '
+                                 f'private attribute, which is immutable. ')
+
+        if item in self._meta_info_fields:
+            raise KeyError(f'{item} is used in meta information, '
+                           f'which is immutable.')
+        super().__delattr__(item)
+        if item in self._data_fields:
+            self._data_fields.remove(item)
+
+    # dict-like methods
+    __setitem__ = __setattr__
+    __delitem__ = __delattr__
+
+    def __getitem__(self, name):
+        return getattr(self, name)
+
+    def get(self, *args):
+        assert len(args) < 3, '`get` get more than 2 arguments'
+        return self.__dict__.get(*args)
+
+    def pop(self, *args):
+        assert len(args) < 3, '`pop` get more than 2 arguments'
+        name = args[0]
+        if name in self._meta_info_fields:
+            raise KeyError(f'{name} is a key in meta information, '
+                           f'which is immutable')
+
+        if args[0] in self._data_fields:
+            self._data_fields.remove(args[0])
+            return self.__dict__.pop(*args)
+
+        # with default value
+        elif len(args) == 2:
+            return args[1]
+        else:
+            raise KeyError(f'{args[0]}')
+
+    def __contains__(self, item):
+        return item in self._data_fields or \
+                    item in self._meta_info_fields
+
+    # Tensor-like methods
+    def to(self, *args, **kwargs):
+        """Apply same name function to all tensors in data_fields."""
+        new_data = self.new()
+        for k, v in self.items():
+            if hasattr(v, 'to'):
+                v = v.to(*args, **kwargs)
+            new_data[k] = v
+        return new_data
+
+    # Tensor-like methods
+    def cpu(self):
+        """Apply same name function to all tensors in data_fields."""
+        new_data = self.new()
+        for k, v in self.items():
+            if isinstance(v, torch.Tensor):
+                v = v.cpu()
+            new_data[k] = v
+        return new_data
+
+    # Tensor-like methods
+    def cuda(self):
+        """Apply same name function to all tensors in data_fields."""
+        new_data = self.new()
+        for k, v in self.items():
+            if isinstance(v, torch.Tensor):
+                v = v.cuda()
+            new_data[k] = v
+        return new_data
+
+    # Tensor-like methods
+    def detach(self):
+        """Apply same name function to all tensors in data_fields."""
+        new_data = self.new()
+        for k, v in self.items():
+            if isinstance(v, torch.Tensor):
+                v = v.detach()
+            new_data[k] = v
+        return new_data
+
+    # Tensor-like methods
+    def numpy(self):
+        """Apply same name function to all tensors in data_fields."""
+        new_data = self.new()
+        for k, v in self.items():
+            if isinstance(v, torch.Tensor):
+                v = v.detach().cpu().numpy()
+            new_data[k] = v
+        return new_data
+
+    def __nice__(self):
+        repr = '\n \n  META INFORMATION \n'
+        for k, v in self.meta_info_items():
+            repr += f'{k}: {v} \n'
+        repr += '\n   DATA FIELDS \n'
+        for k, v in self.items():
+            if isinstance(v, (torch.Tensor, np.ndarray)):
+                repr += f'shape of {k}: {v.shape} \n'
+            else:
+                repr += f'{k}: {v} \n'
+        return repr + '\n'
diff --git a/mmdet/core/data_structures/instance_data.py b/mmdet/core/data_structures/instance_data.py
new file mode 100644
index 00000000..eef2065c
--- /dev/null
+++ b/mmdet/core/data_structures/instance_data.py
@@ -0,0 +1,188 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import itertools
+
+import numpy as np
+import torch
+
+from .general_data import GeneralData
+
+
+class InstanceData(GeneralData):
+    """Data structure for instance-level annnotations or predictions.
+
+    Subclass of :class:`GeneralData`. All value in `data_fields`
+    should have the same length. This design refer to
+    https://github.com/facebookresearch/detectron2/blob/master/detectron2/structures/instances.py # noqa E501
+
+    Examples:
+        >>> from mmdet.core import InstanceData
+        >>> import numpy as np
+        >>> img_meta = dict(img_shape=(800, 1196, 3), pad_shape=(800, 1216, 3))
+        >>> results = InstanceData(img_meta)
+        >>> img_shape in results
+        True
+        >>> results.det_labels = torch.LongTensor([0, 1, 2, 3])
+        >>> results["det_scores"] = torch.Tensor([0.01, 0.7, 0.6, 0.3])
+        >>> results["det_masks"] = np.ndarray(4, 2, 2)
+        >>> len(results)
+        4
+        >>> print(resutls)
+        <InstanceData(
+
+            META INFORMATION
+        pad_shape: (800, 1216, 3)
+        img_shape: (800, 1196, 3)
+
+            PREDICTIONS
+        shape of det_labels: torch.Size([4])
+        shape of det_masks: (4, 2, 2)
+        shape of det_scores: torch.Size([4])
+
+        ) at 0x7fe26b5ca990>
+        >>> sorted_results = results[results.det_scores.sort().indices]
+        >>> sorted_results.det_scores
+        tensor([0.0100, 0.3000, 0.6000, 0.7000])
+        >>> sorted_results.det_labels
+        tensor([0, 3, 2, 1])
+        >>> print(results[results.scores > 0.5])
+        <InstanceData(
+
+            META INFORMATION
+        pad_shape: (800, 1216, 3)
+        img_shape: (800, 1196, 3)
+
+            PREDICTIONS
+        shape of det_labels: torch.Size([2])
+        shape of det_masks: (2, 2, 2)
+        shape of det_scores: torch.Size([2])
+
+        ) at 0x7fe26b6d7790>
+        >>> results[results.det_scores > 0.5].det_labels
+        tensor([1, 2])
+        >>> results[results.det_scores > 0.5].det_scores
+        tensor([0.7000, 0.6000])
+    """
+
+    def __setattr__(self, name, value):
+
+        if name in ('_meta_info_fields', '_data_fields'):
+            if not hasattr(self, name):
+                super().__setattr__(name, value)
+            else:
+                raise AttributeError(
+                    f'{name} has been used as a '
+                    f'private attribute, which is immutable. ')
+
+        else:
+            assert isinstance(value, (torch.Tensor, np.ndarray, list)), \
+                f'Can set {type(value)}, only support' \
+                f' {(torch.Tensor, np.ndarray, list)}'
+
+            if self._data_fields:
+                assert len(value) == len(self), f'the length of ' \
+                                             f'values {len(value)} is ' \
+                                             f'not consistent with' \
+                                             f' the length ' \
+                                             f'of this :obj:`InstanceData` ' \
+                                             f'{len(self)} '
+            super().__setattr__(name, value)
+
+    def __getitem__(self, item):
+        """
+        Args:
+            item (str, obj:`slice`,
+                obj`torch.LongTensor`, obj:`torch.BoolTensor`):
+                get the corresponding values according to item.
+
+        Returns:
+            obj:`InstanceData`: Corresponding values.
+        """
+        assert len(self), ' This is a empty instance'
+
+        assert isinstance(
+            item, (str, slice, int, torch.LongTensor, torch.BoolTensor))
+
+        if isinstance(item, str):
+            return getattr(self, item)
+
+        if type(item) == int:
+            if item >= len(self) or item < -len(self):
+                raise IndexError(f'Index {item} out of range!')
+            else:
+                # keep the dimension
+                item = slice(item, None, len(self))
+
+        new_data = self.new()
+        if isinstance(item, (torch.Tensor)):
+            assert item.dim() == 1, 'Only support to get the' \
+                                 ' values along the first dimension.'
+            if isinstance(item, torch.BoolTensor):
+                assert len(item) == len(self), f'The shape of the' \
+                                               f' input(BoolTensor)) ' \
+                                               f'{len(item)} ' \
+                                               f' does not match the shape ' \
+                                               f'of the indexed tensor ' \
+                                               f'in results_filed ' \
+                                               f'{len(self)} at ' \
+                                               f'first dimension. '
+
+            for k, v in self.items():
+                if isinstance(v, torch.Tensor):
+                    new_data[k] = v[item]
+                elif isinstance(v, np.ndarray):
+                    new_data[k] = v[item.cpu().numpy()]
+                elif isinstance(v, list):
+                    r_list = []
+                    # convert to indexes from boolTensor
+                    if isinstance(item, torch.BoolTensor):
+                        indexes = torch.nonzero(item).view(-1)
+                    else:
+                        indexes = item
+                    for index in indexes:
+                        r_list.append(v[index])
+                    new_data[k] = r_list
+        else:
+            # item is a slice
+            for k, v in self.items():
+                new_data[k] = v[item]
+        return new_data
+
+    @staticmethod
+    def cat(instances_list):
+        """Concat the predictions of all :obj:`InstanceData` in the list.
+
+        Args:
+            instances_list (list[:obj:`InstanceData`]): A list
+                of :obj:`InstanceData`.
+
+        Returns:
+            obj:`InstanceData`
+        """
+        assert all(
+            isinstance(results, InstanceData) for results in instances_list)
+        assert len(instances_list) > 0
+        if len(instances_list) == 1:
+            return instances_list[0]
+
+        new_data = instances_list[0].new()
+        for k in instances_list[0]._data_fields:
+            values = [results[k] for results in instances_list]
+            v0 = values[0]
+            if isinstance(v0, torch.Tensor):
+                values = torch.cat(values, dim=0)
+            elif isinstance(v0, np.ndarray):
+                values = np.concatenate(values, axis=0)
+            elif isinstance(v0, list):
+                values = list(itertools.chain(*values))
+            else:
+                raise ValueError(
+                    f'Can not concat the {k} which is a {type(v0)}')
+            new_data[k] = values
+        return new_data
+
+    def __len__(self):
+        if len(self._data_fields):
+            for v in self.values():
+                return len(v)
+        else:
+            raise AssertionError('This is an empty `InstanceData`.')
diff --git a/mmdet/core/evaluation/__init__.py b/mmdet/core/evaluation/__init__.py
index d11ef15b..888af619 100644
--- a/mmdet/core/evaluation/__init__.py
+++ b/mmdet/core/evaluation/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .class_names import (cityscapes_classes, coco_classes, dataset_aliases,
                           get_classes, imagenet_det_classes,
                           imagenet_vid_classes, voc_classes)
diff --git a/mmdet/core/evaluation/bbox_overlaps.py b/mmdet/core/evaluation/bbox_overlaps.py
index 93559ea0..5d6eb82f 100644
--- a/mmdet/core/evaluation/bbox_overlaps.py
+++ b/mmdet/core/evaluation/bbox_overlaps.py
@@ -1,21 +1,36 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 
 
-def bbox_overlaps(bboxes1, bboxes2, mode='iou', eps=1e-6):
+def bbox_overlaps(bboxes1,
+                  bboxes2,
+                  mode='iou',
+                  eps=1e-6,
+                  use_legacy_coordinate=False):
     """Calculate the ious between each bbox of bboxes1 and bboxes2.
 
     Args:
-        bboxes1(ndarray): shape (n, 4)
-        bboxes2(ndarray): shape (k, 4)
-        mode(str): iou (intersection over union) or iof (intersection
+        bboxes1 (ndarray): Shape (n, 4)
+        bboxes2 (ndarray): Shape (k, 4)
+        mode (str): IOU (intersection over union) or IOF (intersection
             over foreground)
+        use_legacy_coordinate (bool): Whether to use coordinate system in
+            mmdet v1.x. which means width, height should be
+            calculated as 'x2 - x1 + 1` and 'y2 - y1 + 1' respectively.
+            Note when function is used in `VOCDataset`, it should be
+            True to align with the official implementation
+            `http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar`
+            Default: False.
 
     Returns:
-        ious(ndarray): shape (n, k)
+        ious (ndarray): Shape (n, k)
     """
 
     assert mode in ['iou', 'iof']
-
+    if not use_legacy_coordinate:
+        extra_length = 0.
+    else:
+        extra_length = 1.
     bboxes1 = bboxes1.astype(np.float32)
     bboxes2 = bboxes2.astype(np.float32)
     rows = bboxes1.shape[0]
@@ -28,15 +43,17 @@ def bbox_overlaps(bboxes1, bboxes2, mode='iou', eps=1e-6):
         bboxes1, bboxes2 = bboxes2, bboxes1
         ious = np.zeros((cols, rows), dtype=np.float32)
         exchange = True
-    area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])
-    area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])
+    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + extra_length) * (
+        bboxes1[:, 3] - bboxes1[:, 1] + extra_length)
+    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + extra_length) * (
+        bboxes2[:, 3] - bboxes2[:, 1] + extra_length)
     for i in range(bboxes1.shape[0]):
         x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])
         y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])
         x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])
         y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])
-        overlap = np.maximum(x_end - x_start, 0) * np.maximum(
-            y_end - y_start, 0)
+        overlap = np.maximum(x_end - x_start + extra_length, 0) * np.maximum(
+            y_end - y_start + extra_length, 0)
         if mode == 'iou':
             union = area1[i] + area2 - overlap
         else:
diff --git a/mmdet/core/evaluation/class_names.py b/mmdet/core/evaluation/class_names.py
index c2487c2e..c7122b5c 100644
--- a/mmdet/core/evaluation/class_names.py
+++ b/mmdet/core/evaluation/class_names.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 
 
diff --git a/mmdet/core/evaluation/eval_hooks.py b/mmdet/core/evaluation/eval_hooks.py
index 68dc92f2..9891e82f 100644
--- a/mmdet/core/evaluation/eval_hooks.py
+++ b/mmdet/core/evaluation/eval_hooks.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 
 import torch.distributed as dist
diff --git a/mmdet/core/evaluation/mean_ap.py b/mmdet/core/evaluation/mean_ap.py
index 1d653a35..18bdfb5f 100644
--- a/mmdet/core/evaluation/mean_ap.py
+++ b/mmdet/core/evaluation/mean_ap.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from multiprocessing import Pool
 
 import mmcv
@@ -60,7 +61,8 @@ def tpfp_imagenet(det_bboxes,
                   gt_bboxes,
                   gt_bboxes_ignore=None,
                   default_iou_thr=0.5,
-                  area_ranges=None):
+                  area_ranges=None,
+                  use_legacy_coordinate=False):
     """Check if detected bboxes are true positive or false positive.
 
     Args:
@@ -73,11 +75,21 @@ def tpfp_imagenet(det_bboxes,
             Default: 0.5.
         area_ranges (list[tuple] | None): Range of bbox areas to be evaluated,
             in the format [(min1, max1), (min2, max2), ...]. Default: None.
+        use_legacy_coordinate (bool): Whether to use coordinate system in
+            mmdet v1.x. which means width, height should be
+            calculated as 'x2 - x1 + 1` and 'y2 - y1 + 1' respectively.
+            Default: False.
 
     Returns:
         tuple[np.ndarray]: (tp, fp) whose elements are 0 and 1. The shape of
             each array is (num_scales, m).
     """
+
+    if not use_legacy_coordinate:
+        extra_length = 0.
+    else:
+        extra_length = 1.
+
     # an indicator of ignored gts
     gt_ignore_inds = np.concatenate(
         (np.zeros(gt_bboxes.shape[0], dtype=np.bool),
@@ -98,14 +110,16 @@ def tpfp_imagenet(det_bboxes,
         if area_ranges == [(None, None)]:
             fp[...] = 1
         else:
-            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0]) * (
-                det_bboxes[:, 3] - det_bboxes[:, 1])
+            det_areas = (
+                det_bboxes[:, 2] - det_bboxes[:, 0] + extra_length) * (
+                    det_bboxes[:, 3] - det_bboxes[:, 1] + extra_length)
             for i, (min_area, max_area) in enumerate(area_ranges):
                 fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1
         return tp, fp
-    ious = bbox_overlaps(det_bboxes, gt_bboxes - 1)
-    gt_w = gt_bboxes[:, 2] - gt_bboxes[:, 0]
-    gt_h = gt_bboxes[:, 3] - gt_bboxes[:, 1]
+    ious = bbox_overlaps(
+        det_bboxes, gt_bboxes - 1, use_legacy_coordinate=use_legacy_coordinate)
+    gt_w = gt_bboxes[:, 2] - gt_bboxes[:, 0] + extra_length
+    gt_h = gt_bboxes[:, 3] - gt_bboxes[:, 1] + extra_length
     iou_thrs = np.minimum((gt_w * gt_h) / ((gt_w + 10.0) * (gt_h + 10.0)),
                           default_iou_thr)
     # sort all detections by scores in descending order
@@ -144,7 +158,8 @@ def tpfp_imagenet(det_bboxes,
                 fp[k, i] = 1
             else:
                 bbox = det_bboxes[i, :4]
-                area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
+                area = (bbox[2] - bbox[0] + extra_length) * (
+                    bbox[3] - bbox[1] + extra_length)
                 if area >= min_area and area < max_area:
                     fp[k, i] = 1
     return tp, fp
@@ -154,7 +169,8 @@ def tpfp_default(det_bboxes,
                  gt_bboxes,
                  gt_bboxes_ignore=None,
                  iou_thr=0.5,
-                 area_ranges=None):
+                 area_ranges=None,
+                 use_legacy_coordinate=False):
     """Check if detected bboxes are true positive or false positive.
 
     Args:
@@ -164,13 +180,24 @@ def tpfp_default(det_bboxes,
             of shape (k, 4). Default: None
         iou_thr (float): IoU threshold to be considered as matched.
             Default: 0.5.
-        area_ranges (list[tuple] | None): Range of bbox areas to be evaluated,
-            in the format [(min1, max1), (min2, max2), ...]. Default: None.
+        area_ranges (list[tuple] | None): Range of bbox areas to be
+            evaluated, in the format [(min1, max1), (min2, max2), ...].
+            Default: None.
+        use_legacy_coordinate (bool): Whether to use coordinate system in
+            mmdet v1.x. which means width, height should be
+            calculated as 'x2 - x1 + 1` and 'y2 - y1 + 1' respectively.
+            Default: False.
 
     Returns:
         tuple[np.ndarray]: (tp, fp) whose elements are 0 and 1. The shape of
             each array is (num_scales, m).
     """
+
+    if not use_legacy_coordinate:
+        extra_length = 0.
+    else:
+        extra_length = 1.
+
     # an indicator of ignored gts
     gt_ignore_inds = np.concatenate(
         (np.zeros(gt_bboxes.shape[0], dtype=np.bool),
@@ -194,13 +221,15 @@ def tpfp_default(det_bboxes,
         if area_ranges == [(None, None)]:
             fp[...] = 1
         else:
-            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0]) * (
-                det_bboxes[:, 3] - det_bboxes[:, 1])
+            det_areas = (
+                det_bboxes[:, 2] - det_bboxes[:, 0] + extra_length) * (
+                    det_bboxes[:, 3] - det_bboxes[:, 1] + extra_length)
             for i, (min_area, max_area) in enumerate(area_ranges):
                 fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1
         return tp, fp
 
-    ious = bbox_overlaps(det_bboxes, gt_bboxes)
+    ious = bbox_overlaps(
+        det_bboxes, gt_bboxes, use_legacy_coordinate=use_legacy_coordinate)
     # for each det, the max iou with all gts
     ious_max = ious.max(axis=1)
     # for each det, which gt overlaps most with it
@@ -213,8 +242,8 @@ def tpfp_default(det_bboxes,
         if min_area is None:
             gt_area_ignore = np.zeros_like(gt_ignore_inds, dtype=bool)
         else:
-            gt_areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0]) * (
-                gt_bboxes[:, 3] - gt_bboxes[:, 1])
+            gt_areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0] + extra_length) * (
+                gt_bboxes[:, 3] - gt_bboxes[:, 1] + extra_length)
             gt_area_ignore = (gt_areas < min_area) | (gt_areas >= max_area)
         for i in sort_inds:
             if ious_max[i] >= iou_thr:
@@ -231,7 +260,8 @@ def tpfp_default(det_bboxes,
                 fp[k, i] = 1
             else:
                 bbox = det_bboxes[i, :4]
-                area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
+                area = (bbox[2] - bbox[0] + extra_length) * (
+                    bbox[3] - bbox[1] + extra_length)
                 if area >= min_area and area < max_area:
                     fp[k, i] = 1
     return tp, fp
@@ -271,7 +301,8 @@ def eval_map(det_results,
              dataset=None,
              logger=None,
              tpfp_fn=None,
-             nproc=4):
+             nproc=4,
+             use_legacy_coordinate=False):
     """Evaluate mAP of a dataset.
 
     Args:
@@ -303,11 +334,19 @@ def eval_map(det_results,
             to evaluate tp & fp. Default None.
         nproc (int): Processes used for computing TP and FP.
             Default: 4.
+        use_legacy_coordinate (bool): Whether to use coordinate system in
+            mmdet v1.x. which means width, height should be
+            calculated as 'x2 - x1 + 1` and 'y2 - y1 + 1' respectively.
+            Default: False.
 
     Returns:
         tuple: (mAP, [dict, dict, ...])
     """
     assert len(det_results) == len(annotations)
+    if not use_legacy_coordinate:
+        extra_length = 0.
+    else:
+        extra_length = 1.
 
     num_imgs = len(det_results)
     num_scales = len(scale_ranges) if scale_ranges is not None else 1
@@ -336,7 +375,8 @@ def eval_map(det_results,
             tpfp_fn,
             zip(cls_dets, cls_gts, cls_gts_ignore,
                 [iou_thr for _ in range(num_imgs)],
-                [area_ranges for _ in range(num_imgs)]))
+                [area_ranges for _ in range(num_imgs)],
+                [use_legacy_coordinate for _ in range(num_imgs)]))
         tp, fp = tuple(zip(*tpfp))
         # calculate gt number of each scale
         # ignored gts or gts beyond the specific scale are not counted
@@ -345,8 +385,8 @@ def eval_map(det_results,
             if area_ranges is None:
                 num_gts[0] += bbox.shape[0]
             else:
-                gt_areas = (bbox[:, 2] - bbox[:, 0]) * (
-                    bbox[:, 3] - bbox[:, 1])
+                gt_areas = (bbox[:, 2] - bbox[:, 0] + extra_length) * (
+                    bbox[:, 3] - bbox[:, 1] + extra_length)
                 for k, (min_area, max_area) in enumerate(area_ranges):
                     num_gts[k] += np.sum((gt_areas >= min_area)
                                          & (gt_areas < max_area))
diff --git a/mmdet/core/evaluation/recall.py b/mmdet/core/evaluation/recall.py
index 23ec744f..82b3c909 100644
--- a/mmdet/core/evaluation/recall.py
+++ b/mmdet/core/evaluation/recall.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from collections.abc import Sequence
 
 import numpy as np
@@ -65,7 +66,8 @@ def eval_recalls(gts,
                  proposals,
                  proposal_nums=None,
                  iou_thrs=0.5,
-                 logger=None):
+                 logger=None,
+                 use_legacy_coordinate=False):
     """Calculate recalls.
 
     Args:
@@ -75,6 +77,11 @@ def eval_recalls(gts,
         iou_thrs (float | Sequence[float]): IoU thresholds. Default: 0.5.
         logger (logging.Logger | str | None): The way to print the recall
             summary. See `mmcv.utils.print_log()` for details. Default: None.
+        use_legacy_coordinate (bool): Whether use coordinate system
+            in mmdet v1.x. "1" was added to both height and width
+            which means w, h should be
+            computed as 'x2 - x1 + 1` and 'y2 - y1 + 1'. Default: False.
+
 
     Returns:
         ndarray: recalls of different ious and proposal nums
@@ -82,9 +89,7 @@ def eval_recalls(gts,
 
     img_num = len(gts)
     assert img_num == len(proposals)
-
     proposal_nums, iou_thrs = set_recall_param(proposal_nums, iou_thrs)
-
     all_ious = []
     for i in range(img_num):
         if proposals[i].ndim == 2 and proposals[i].shape[1] == 5:
@@ -97,7 +102,10 @@ def eval_recalls(gts,
         if gts[i] is None or gts[i].shape[0] == 0:
             ious = np.zeros((0, img_proposal.shape[0]), dtype=np.float32)
         else:
-            ious = bbox_overlaps(gts[i], img_proposal[:prop_num, :4])
+            ious = bbox_overlaps(
+                gts[i],
+                img_proposal[:prop_num, :4],
+                use_legacy_coordinate=use_legacy_coordinate)
         all_ious.append(ious)
     all_ious = np.array(all_ious)
     recalls = _recalls(all_ious, proposal_nums, iou_thrs)
diff --git a/mmdet/core/export/__init__.py b/mmdet/core/export/__init__.py
index 91685619..a8179c93 100644
--- a/mmdet/core/export/__init__.py
+++ b/mmdet/core/export/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .onnx_helper import (add_dummy_nms_for_onnx, dynamic_clip_for_onnx,
                           get_k_for_topk)
 from .pytorch2onnx import (build_model_from_cfg,
diff --git a/mmdet/core/export/model_wrappers.py b/mmdet/core/export/model_wrappers.py
index 4f4b35b9..2f62bb03 100644
--- a/mmdet/core/export/model_wrappers.py
+++ b/mmdet/core/export/model_wrappers.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import warnings
 
@@ -34,7 +35,7 @@ class DeployBaseDetector(BaseDetector):
     def train_step(self, data, optimizer):
         raise NotImplementedError('This method is not implemented.')
 
-    def aforward_test(self, *, img, img_metas, **kwargs):
+    def forward_test(self, *, img, img_metas, **kwargs):
         raise NotImplementedError('This method is not implemented.')
 
     def async_simple_test(self, img, img_metas, **kwargs):
diff --git a/mmdet/core/export/onnx_helper.py b/mmdet/core/export/onnx_helper.py
index 4bab842d..733a672f 100644
--- a/mmdet/core/export/onnx_helper.py
+++ b/mmdet/core/export/onnx_helper.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os
 
 import torch
diff --git a/mmdet/core/export/pytorch2onnx.py b/mmdet/core/export/pytorch2onnx.py
index 71725d0f..53906f26 100644
--- a/mmdet/core/export/pytorch2onnx.py
+++ b/mmdet/core/export/pytorch2onnx.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from functools import partial
 
 import mmcv
diff --git a/mmdet/core/hook/__init__.py b/mmdet/core/hook/__init__.py
index 3206e51a..31d69a0d 100644
--- a/mmdet/core/hook/__init__.py
+++ b/mmdet/core/hook/__init__.py
@@ -1,3 +1,5 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from .checkloss_hook import CheckInvalidLossHook
 from .ema import ExpMomentumEMAHook, LinearMomentumEMAHook
 from .sync_norm_hook import SyncNormHook
 from .sync_random_size_hook import SyncRandomSizeHook
@@ -6,5 +8,6 @@ from .yolox_mode_switch_hook import YOLOXModeSwitchHook
 
 __all__ = [
     'SyncRandomSizeHook', 'YOLOXModeSwitchHook', 'SyncNormHook',
-    'ExpMomentumEMAHook', 'LinearMomentumEMAHook', 'YOLOXLrUpdaterHook'
+    'ExpMomentumEMAHook', 'LinearMomentumEMAHook', 'YOLOXLrUpdaterHook',
+    'CheckInvalidLossHook'
 ]
diff --git a/mmdet/core/hook/checkloss_hook.py b/mmdet/core/hook/checkloss_hook.py
new file mode 100644
index 00000000..754e61be
--- /dev/null
+++ b/mmdet/core/hook/checkloss_hook.py
@@ -0,0 +1,24 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import torch
+from mmcv.runner.hooks import HOOKS, Hook
+
+
+@HOOKS.register_module()
+class CheckInvalidLossHook(Hook):
+    """Check invalid loss hook.
+
+    This hook will regularly check whether the loss is valid
+    during training.
+
+    Args:
+        interval (int): Checking interval (every k iterations).
+            Default: 50.
+    """
+
+    def __init__(self, interval=50):
+        self.interval = interval
+
+    def after_train_iter(self, runner):
+        if self.every_n_iters(runner, self.interval):
+            assert torch.isfinite(runner.outputs['loss']), \
+                runner.logger.info('loss become infinite or NaN!')
diff --git a/mmdet/core/hook/ema.py b/mmdet/core/hook/ema.py
index 54b88475..ff7bfbab 100644
--- a/mmdet/core/hook/ema.py
+++ b/mmdet/core/hook/ema.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 from mmcv.parallel import is_module_wrapper
diff --git a/mmdet/core/hook/sync_norm_hook.py b/mmdet/core/hook/sync_norm_hook.py
index f6a6832e..7265620a 100644
--- a/mmdet/core/hook/sync_norm_hook.py
+++ b/mmdet/core/hook/sync_norm_hook.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from collections import OrderedDict
 
 from mmcv.runner import get_dist_info
diff --git a/mmdet/core/hook/sync_random_size_hook.py b/mmdet/core/hook/sync_random_size_hook.py
index 56bd2480..6d7e96c6 100644
--- a/mmdet/core/hook/sync_random_size_hook.py
+++ b/mmdet/core/hook/sync_random_size_hook.py
@@ -1,4 +1,6 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import random
+import warnings
 
 import torch
 from mmcv.runner import get_dist_info
@@ -8,15 +10,22 @@ from torch import distributed as dist
 
 @HOOKS.register_module()
 class SyncRandomSizeHook(Hook):
-    """Change and synchronize the random image size across ranks, currently
-    used in YOLOX.
+    """Change and synchronize the random image size across ranks.
+    SyncRandomSizeHook is deprecated, please use Resize pipeline to achieve
+    similar functions. Such as `dict(type='Resize', img_scale=[(448, 448),
+    (832, 832)], multiscale_mode='range', keep_ratio=True)`.
+
+    Note: Due to the multi-process dataloader, its behavior is different
+    from YOLOX's official implementation, the official is to change the
+    size every fixed iteration interval and what we achieved is a fixed
+    epoch interval.
 
     Args:
         ratio_range (tuple[int]): Random ratio range. It will be multiplied
             by 32, and then change the dataset output image size.
             Default: (14, 26).
         img_scale (tuple[int]): Size of input image. Default: (640, 640).
-        interval (int): The interval of change image size. Default: 10.
+        interval (int): The epoch interval of change image size. Default: 1.
         device (torch.device | str): device for returned tensors.
             Default: 'cuda'.
     """
@@ -24,8 +33,15 @@ class SyncRandomSizeHook(Hook):
     def __init__(self,
                  ratio_range=(14, 26),
                  img_scale=(640, 640),
-                 interval=10,
+                 interval=1,
                  device='cuda'):
+        warnings.warn('DeprecationWarning: SyncRandomSizeHook is deprecated. '
+                      'Please use Resize pipeline to achieve similar '
+                      'functions. Due to the multi-process dataloader, '
+                      'its behavior is different from YOLOX\'s official '
+                      'implementation, the official is to change the size '
+                      'every fixed iteration interval and what we achieved '
+                      'is a fixed epoch interval.')
         self.rank, world_size = get_dist_info()
         self.is_distributed = world_size > 1
         self.ratio_range = ratio_range
@@ -33,9 +49,9 @@ class SyncRandomSizeHook(Hook):
         self.interval = interval
         self.device = device
 
-    def after_train_iter(self, runner):
+    def after_train_epoch(self, runner):
         """Change the dataset output image size."""
-        if self.ratio_range is not None and (runner.iter +
+        if self.ratio_range is not None and (runner.epoch +
                                              1) % self.interval == 0:
             # Due to DDP and DP get the device behavior inconsistent,
             # so we did not get the device from runner.model.
diff --git a/mmdet/core/hook/yolox_lrupdater_hook.py b/mmdet/core/hook/yolox_lrupdater_hook.py
index 48fd51b2..ecb028ed 100644
--- a/mmdet/core/hook/yolox_lrupdater_hook.py
+++ b/mmdet/core/hook/yolox_lrupdater_hook.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.runner.hooks import HOOKS
 from mmcv.runner.hooks.lr_updater import (CosineAnnealingLrUpdaterHook,
                                           annealing_cos)
diff --git a/mmdet/core/hook/yolox_mode_switch_hook.py b/mmdet/core/hook/yolox_mode_switch_hook.py
index ef1fed17..2895eeaf 100644
--- a/mmdet/core/hook/yolox_mode_switch_hook.py
+++ b/mmdet/core/hook/yolox_mode_switch_hook.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.parallel import is_module_wrapper
 from mmcv.runner.hooks import HOOKS, Hook
 
diff --git a/mmdet/core/mask/__init__.py b/mmdet/core/mask/__init__.py
index ab1e88bc..2083af20 100644
--- a/mmdet/core/mask/__init__.py
+++ b/mmdet/core/mask/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .mask_target import mask_target
 from .structures import BaseInstanceMasks, BitmapMasks, PolygonMasks
 from .utils import encode_mask_results, split_combined_polys
diff --git a/mmdet/core/mask/mask_target.py b/mmdet/core/mask/mask_target.py
index cfccd77c..273e7678 100644
--- a/mmdet/core/mask/mask_target.py
+++ b/mmdet/core/mask/mask_target.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 from torch.nn.modules.utils import _pair
diff --git a/mmdet/core/mask/structures.py b/mmdet/core/mask/structures.py
index b03d57a3..8fd30680 100644
--- a/mmdet/core/mask/structures.py
+++ b/mmdet/core/mask/structures.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 
 import cv2
@@ -528,6 +529,21 @@ class BitmapMasks(BaseInstanceMasks):
         self = cls(masks, height=height, width=width)
         return self
 
+    def get_bboxes(self):
+        num_masks = len(self)
+        boxes = np.zeros((num_masks, 4), dtype=np.float32)
+        x_any = self.masks.any(axis=1)
+        y_any = self.masks.any(axis=2)
+        for idx in range(num_masks):
+            x = np.where(x_any[idx, :])[0]
+            y = np.where(y_any[idx, :])[0]
+            if len(x) > 0 and len(y) > 0:
+                # use +1 for x_max and y_max so that the right and bottom
+                # boundary of instance masks are fully included by the box
+                boxes[idx, :] = np.array([x[0], y[0], x[-1] + 1, y[-1] + 1],
+                                         dtype=np.float32)
+        return boxes
+
 
 class PolygonMasks(BaseInstanceMasks):
     """This class represents masks in the form of polygons.
@@ -1019,6 +1035,24 @@ class PolygonMasks(BaseInstanceMasks):
         self = cls(masks, height, width)
         return self
 
+    def get_bboxes(self):
+        num_masks = len(self)
+        boxes = np.zeros((num_masks, 4), dtype=np.float32)
+        for idx, poly_per_obj in enumerate(self.masks):
+            # simply use a number that is big enough for comparison with
+            # coordinates
+            xy_min = np.array([self.width * 2, self.height * 2],
+                              dtype=np.float32)
+            xy_max = np.zeros(2, dtype=np.float32)
+            for p in poly_per_obj:
+                xy = np.array(p).reshape(-1, 2).astype(np.float32)
+                xy_min = np.minimum(xy_min, np.min(xy, axis=0))
+                xy_max = np.maximum(xy_max, np.max(xy, axis=0))
+            boxes[idx, :2] = xy_min
+            boxes[idx, 2:] = xy_max
+
+        return boxes
+
 
 def polygon_to_bitmap(polygons, height, width):
     """Convert masks from the form of polygons to bitmaps.
diff --git a/mmdet/core/mask/utils.py b/mmdet/core/mask/utils.py
index c8820829..8e95f72b 100644
--- a/mmdet/core/mask/utils.py
+++ b/mmdet/core/mask/utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import numpy as np
 import pycocotools.mask as mask_util
diff --git a/mmdet/core/post_processing/__init__.py b/mmdet/core/post_processing/__init__.py
index 880b3f06..00376bd4 100644
--- a/mmdet/core/post_processing/__init__.py
+++ b/mmdet/core/post_processing/__init__.py
@@ -1,8 +1,10 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .bbox_nms import fast_nms, multiclass_nms
+from .matrix_nms import mask_matrix_nms
 from .merge_augs import (merge_aug_bboxes, merge_aug_masks,
                          merge_aug_proposals, merge_aug_scores)
 
 __all__ = [
     'multiclass_nms', 'merge_aug_proposals', 'merge_aug_bboxes',
-    'merge_aug_scores', 'merge_aug_masks', 'fast_nms'
+    'merge_aug_scores', 'merge_aug_masks', 'mask_matrix_nms', 'fast_nms'
 ]
diff --git a/mmdet/core/post_processing/bbox_nms.py b/mmdet/core/post_processing/bbox_nms.py
index e836225f..f16a1e77 100644
--- a/mmdet/core/post_processing/bbox_nms.py
+++ b/mmdet/core/post_processing/bbox_nms.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmcv.ops.nms import batched_nms
 
diff --git a/mmdet/core/post_processing/matrix_nms.py b/mmdet/core/post_processing/matrix_nms.py
new file mode 100644
index 00000000..e2fc5d9e
--- /dev/null
+++ b/mmdet/core/post_processing/matrix_nms.py
@@ -0,0 +1,121 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import torch
+
+
+def mask_matrix_nms(masks,
+                    labels,
+                    scores,
+                    filter_thr=-1,
+                    nms_pre=-1,
+                    max_num=-1,
+                    kernel='gaussian',
+                    sigma=2.0,
+                    mask_area=None):
+    """Matrix NMS for multi-class masks.
+
+    Args:
+        masks (Tensor): Has shape (num_instances, h, w)
+        labels (Tensor): Labels of corresponding masks,
+            has shape (num_instances,).
+        scores (Tensor): Mask scores of corresponding masks,
+            has shape (num_instances).
+        filter_thr (float): Score threshold to filter the masks
+            after matrix nms. Default: -1, which means do not
+            use filter_thr.
+        nms_pre (int): The max number of instances to do the matrix nms.
+            Default: -1, which means do not use nms_pre.
+        max_num (int, optional): If there are more than max_num masks after
+            matrix, only top max_num will be kept. Default: -1, which means
+            do not use max_num.
+        kernel (str): 'linear' or 'gaussian'.
+        sigma (float): std in gaussian method.
+        mask_area (Tensor): The sum of seg_masks.
+
+    Returns:
+        tuple(Tensor): Processed mask results.
+
+            - scores (Tensor): Updated scores, has shape (n,).
+            - labels (Tensor): Remained labels, has shape (n,).
+            - masks (Tensor): Remained masks, has shape (n, w, h).
+            - keep_inds (Tensor): The indexs number of
+              the remaining mask in the input mask, has shape (n,).
+    """
+    assert len(labels) == len(masks) == len(scores)
+    if len(labels) == 0:
+        return scores.new_zeros(0), labels.new_zeros(0), masks.new_zeros(
+            0, *masks.shape[-2:]), labels.new_zeros(0)
+    if mask_area is None:
+        mask_area = masks.sum((1, 2)).float()
+    else:
+        assert len(masks) == len(mask_area)
+
+    # sort and keep top nms_pre
+    scores, sort_inds = torch.sort(scores, descending=True)
+
+    keep_inds = sort_inds
+    if nms_pre > 0 and len(sort_inds) > nms_pre:
+        sort_inds = sort_inds[:nms_pre]
+        keep_inds = keep_inds[:nms_pre]
+        scores = scores[:nms_pre]
+    masks = masks[sort_inds]
+    mask_area = mask_area[sort_inds]
+    labels = labels[sort_inds]
+
+    num_masks = len(labels)
+    flatten_masks = masks.reshape(num_masks, -1).float()
+    # inter.
+    inter_matrix = torch.mm(flatten_masks, flatten_masks.transpose(1, 0))
+    expanded_mask_area = mask_area.expand(num_masks, num_masks)
+    # Upper triangle iou matrix.
+    iou_matrix = (inter_matrix /
+                  (expanded_mask_area + expanded_mask_area.transpose(1, 0) -
+                   inter_matrix)).triu(diagonal=1)
+    # label_specific matrix.
+    expanded_labels = labels.expand(num_masks, num_masks)
+    # Upper triangle label matrix.
+    label_matrix = (expanded_labels == expanded_labels.transpose(
+        1, 0)).triu(diagonal=1)
+
+    # IoU compensation
+    compensate_iou, _ = (iou_matrix * label_matrix).max(0)
+    compensate_iou = compensate_iou.expand(num_masks,
+                                           num_masks).transpose(1, 0)
+
+    # IoU decay
+    decay_iou = iou_matrix * label_matrix
+
+    # Calculate the decay_coefficient
+    if kernel == 'gaussian':
+        decay_matrix = torch.exp(-1 * sigma * (decay_iou**2))
+        compensate_matrix = torch.exp(-1 * sigma * (compensate_iou**2))
+        decay_coefficient, _ = (decay_matrix / compensate_matrix).min(0)
+    elif kernel == 'linear':
+        decay_matrix = (1 - decay_iou) / (1 - compensate_iou)
+        decay_coefficient, _ = decay_matrix.min(0)
+    else:
+        raise NotImplementedError(
+            f'{kernel} kernel is not supported in matrix nms!')
+    # update the score.
+    scores = scores * decay_coefficient
+
+    if filter_thr > 0:
+        keep = scores >= filter_thr
+        keep_inds = keep_inds[keep]
+        if not keep.any():
+            return scores.new_zeros(0), labels.new_zeros(0), masks.new_zeros(
+                0, *masks.shape[-2:]), labels.new_zeros(0)
+        masks = masks[keep]
+        scores = scores[keep]
+        labels = labels[keep]
+
+    # sort and keep top max_num
+    scores, sort_inds = torch.sort(scores, descending=True)
+    keep_inds = keep_inds[sort_inds]
+    if max_num > 0 and len(sort_inds) > max_num:
+        sort_inds = sort_inds[:max_num]
+        keep_inds = keep_inds[:max_num]
+        scores = scores[:max_num]
+    masks = masks[sort_inds]
+    labels = labels[sort_inds]
+
+    return scores, labels, masks, keep_inds
diff --git a/mmdet/core/post_processing/merge_augs.py b/mmdet/core/post_processing/merge_augs.py
index 42e9af35..2ac4603a 100644
--- a/mmdet/core/post_processing/merge_augs.py
+++ b/mmdet/core/post_processing/merge_augs.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import warnings
 
diff --git a/mmdet/core/utils/__init__.py b/mmdet/core/utils/__init__.py
index 4e4672d0..bbd909ff 100644
--- a/mmdet/core/utils/__init__.py
+++ b/mmdet/core/utils/__init__.py
@@ -1,8 +1,11 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .dist_utils import (DistOptimizerHook, all_reduce_dict, allreduce_grads,
                          reduce_mean)
-from .misc import flip_tensor, mask2ndarray, multi_apply, unmap
+from .misc import (center_of_mass, flip_tensor, generate_coordinate,
+                   mask2ndarray, multi_apply, unmap)
 
 __all__ = [
     'allreduce_grads', 'DistOptimizerHook', 'reduce_mean', 'multi_apply',
-    'unmap', 'mask2ndarray', 'flip_tensor', 'all_reduce_dict'
+    'unmap', 'mask2ndarray', 'flip_tensor', 'all_reduce_dict',
+    'center_of_mass', 'generate_coordinate'
 ]
diff --git a/mmdet/core/utils/dist_utils.py b/mmdet/core/utils/dist_utils.py
index 25dded55..d63fef8a 100644
--- a/mmdet/core/utils/dist_utils.py
+++ b/mmdet/core/utils/dist_utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import functools
 import pickle
 import warnings
diff --git a/mmdet/core/utils/misc.py b/mmdet/core/utils/misc.py
index e1f40d8c..36bb6883 100644
--- a/mmdet/core/utils/misc.py
+++ b/mmdet/core/utils/misc.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from functools import partial
 
 import numpy as np
@@ -82,3 +83,46 @@ def flip_tensor(src_tensor, flip_direction):
     else:
         out_tensor = torch.flip(src_tensor, [2, 3])
     return out_tensor
+
+
+def center_of_mass(mask, esp=1e-6):
+    """Calculate the centroid coordinates of the mask.
+
+    Args:
+        mask (Tensor): The mask to be calculated, shape (h, w).
+        esp (float): Avoid dividing by zero. Default: 1e-6.
+
+    Returns:
+        tuple[Tensor]: the coordinates of the center point of the mask.
+
+            - center_h (Tensor): the center point of the height.
+            - center_w (Tensor): the center point of the width.
+    """
+    h, w = mask.shape
+    grid_h = torch.arange(h, device=mask.device)[:, None]
+    grid_w = torch.arange(w, device=mask.device)
+    normalizer = mask.sum().float().clamp(min=esp)
+    center_h = (mask * grid_h).sum() / normalizer
+    center_w = (mask * grid_w).sum() / normalizer
+    return center_h, center_w
+
+
+def generate_coordinate(featmap_sizes, device='cuda'):
+    """Generate the coordinate.
+
+    Args:
+        featmap_sizes (tuple): The feature to be calculated,
+            of shape (N, C, W, H).
+        device (str): The device where the feature will be put on.
+    Returns:
+        coord_feat (Tensor): The coordinate feature, of shape (N, 2, W, H).
+    """
+
+    x_range = torch.linspace(-1, 1, featmap_sizes[-1], device=device)
+    y_range = torch.linspace(-1, 1, featmap_sizes[-2], device=device)
+    y, x = torch.meshgrid(y_range, x_range)
+    y = y.expand([featmap_sizes[0], 1, -1, -1])
+    x = x.expand([featmap_sizes[0], 1, -1, -1])
+    coord_feat = torch.cat([x, y], 1)
+
+    return coord_feat
diff --git a/mmdet/core/visualization/__init__.py b/mmdet/core/visualization/__init__.py
index 4ff995c0..85a5c98c 100644
--- a/mmdet/core/visualization/__init__.py
+++ b/mmdet/core/visualization/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .image import (color_val_matplotlib, imshow_det_bboxes,
                     imshow_gt_det_bboxes)
 
diff --git a/mmdet/core/visualization/image.py b/mmdet/core/visualization/image.py
index d8a4e94e..35104ca8 100644
--- a/mmdet/core/visualization/image.py
+++ b/mmdet/core/visualization/image.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import matplotlib.pyplot as plt
 import mmcv
 import numpy as np
diff --git a/mmdet/datasets/__init__.py b/mmdet/datasets/__init__.py
index 14f8963d..8a0d2e51 100644
--- a/mmdet/datasets/__init__.py
+++ b/mmdet/datasets/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .builder import DATASETS, PIPELINES, build_dataloader, build_dataset
 from .cityscapes import CityscapesDataset
 from .coco import CocoDataset
diff --git a/mmdet/datasets/api_wrappers/__init__.py b/mmdet/datasets/api_wrappers/__init__.py
index 05f95c94..9bf80710 100644
--- a/mmdet/datasets/api_wrappers/__init__.py
+++ b/mmdet/datasets/api_wrappers/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .coco_api import COCO, COCOeval
 
 __all__ = ['COCO', 'COCOeval']
diff --git a/mmdet/datasets/api_wrappers/coco_api.py b/mmdet/datasets/api_wrappers/coco_api.py
index 57077f9b..eef6341e 100644
--- a/mmdet/datasets/api_wrappers/coco_api.py
+++ b/mmdet/datasets/api_wrappers/coco_api.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # This file add snake case alias for coco api
 
 import warnings
diff --git a/mmdet/datasets/builder.py b/mmdet/datasets/builder.py
index 1de68d54..387151f5 100644
--- a/mmdet/datasets/builder.py
+++ b/mmdet/datasets/builder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import platform
 import random
diff --git a/mmdet/datasets/cityscapes.py b/mmdet/datasets/cityscapes.py
index 71eead87..716aa649 100644
--- a/mmdet/datasets/cityscapes.py
+++ b/mmdet/datasets/cityscapes.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # Modified from https://github.com/facebookresearch/detectron2/blob/master/detectron2/data/datasets/cityscapes.py # noqa
 # and https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py # noqa
 
diff --git a/mmdet/datasets/coco.py b/mmdet/datasets/coco.py
index f8a779eb..6c0bd963 100644
--- a/mmdet/datasets/coco.py
+++ b/mmdet/datasets/coco.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import itertools
 import logging
 import os.path as osp
diff --git a/mmdet/datasets/coco_panoptic.py b/mmdet/datasets/coco_panoptic.py
index bf29eb6f..36e95950 100644
--- a/mmdet/datasets/coco_panoptic.py
+++ b/mmdet/datasets/coco_panoptic.py
@@ -1,9 +1,12 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import itertools
 import os
 from collections import defaultdict
 
 import mmcv
 import numpy as np
 from mmcv.utils import print_log
+from terminaltables import AsciiTable
 
 from .api_wrappers import COCO
 from .builder import DATASETS
@@ -44,7 +47,7 @@ class COCOPanoptic(COCO):
                 'pip install git+https://github.com/cocodataset/'
                 'panopticapi.git.')
 
-        super(COCO, self).__init__(annotation_file)
+        super(COCOPanoptic, self).__init__(annotation_file)
 
     def createIndex(self):
         # create index
@@ -55,6 +58,7 @@ class COCOPanoptic(COCO):
         if 'annotations' in self.dataset:
             for ann, img_info in zip(self.dataset['annotations'],
                                      self.dataset['images']):
+                img_info['segm_file'] = ann['file_name']
                 for seg_ann in ann['segments_info']:
                     # to match with instance.json
                     seg_ann['image_id'] = ann['image_id']
@@ -327,7 +331,7 @@ class CocoPanopticDataset(CocoDataset):
     def _pan2json(self, results, outfile_prefix):
         """Convert panoptic results to COCO panoptic json style."""
         label2cat = dict((v, k) for (k, v) in self.cat2label.items())
-        pan_json_results = []
+        pred_annotations = []
         outdir = os.path.join(os.path.dirname(outfile_prefix), 'panoptic')
 
         for idx in range(len(self)):
@@ -362,7 +366,8 @@ class CocoPanopticDataset(CocoDataset):
                 'segments_info': segm_info,
                 'file_name': segm_file
             }
-            pan_json_results.append(record)
+            pred_annotations.append(record)
+        pan_json_results = dict(annotations=pred_annotations)
         return pan_json_results
 
     def results2json(self, results, outfile_prefix):
@@ -386,16 +391,22 @@ class CocoPanopticDataset(CocoDataset):
 
         return result_files
 
-    def evaluate_pan_json(self, result_files, outfile_prefix, logger=None):
+    def evaluate_pan_json(self,
+                          result_files,
+                          outfile_prefix,
+                          logger=None,
+                          classwise=False):
         """Evaluate PQ according to the panoptic results json file."""
+        imgs = self.coco.imgs
         gt_json = self.coco.img_ann_map  # image to annotations
         gt_json = [{
             'image_id': k,
             'segments_info': v,
-            'file_name': self.formatter.format(k)
+            'file_name': imgs[k]['segm_file']
         } for k, v in gt_json.items()]
         pred_json = mmcv.load(result_files['panoptic'])
-        pred_json = dict((el['image_id'], el) for el in pred_json)
+        pred_json = dict(
+            (el['image_id'], el) for el in pred_json['annotations'])
 
         # match the gt_anns and pred_anns in the same image
         matched_annotations_list = []
@@ -412,56 +423,53 @@ class CocoPanopticDataset(CocoDataset):
         pq_stat = pq_compute_multi_core(matched_annotations_list, gt_folder,
                                         pred_folder, self.categories)
 
-        eval_results = {}
-
         metrics = [('All', None), ('Things', True), ('Stuff', False)]
         pq_results = {}
 
-        output = '\n'
         for name, isthing in metrics:
-            pq_results[name], per_class_pq_results = pq_stat.pq_average(
+            pq_results[name], classwise_results = pq_stat.pq_average(
                 self.categories, isthing=isthing)
             if name == 'All':
-                pq_results['per_class'] = per_class_pq_results
-        output += ('{:10s}| {:>5s}  {:>5s}  {:>5s} {:>5s}\n'.format(
-            '', 'PQ', 'SQ', 'RQ', 'N'))
-        output += ('-' * (10 + 7 * 4) + '\n')
-
-        for name, _isthing in metrics:
-            output += '{:10s}| {:5.2f}  {:5.2f}  {:5.2f} {:5d}\n'.format(
-                name, 100 * pq_results[name]['pq'],
-                100 * pq_results[name]['sq'], 100 * pq_results[name]['rq'],
-                pq_results[name]['n'])
-            eval_results[f'{name}_pq'] = pq_results[name]['pq'] * 100.0
-            eval_results[f'{name}_sq'] = pq_results[name]['sq'] * 100.0
-            eval_results[f'{name}_rq'] = pq_results[name]['rq'] * 100.0
-        print_log(output, logger=logger)
+                pq_results['classwise'] = classwise_results
 
-        return eval_results
+        classwise_results = None
+        if classwise:
+            classwise_results = {
+                k: v
+                for k, v in zip(self.CLASSES, pq_results['classwise'].values())
+            }
+        print_panoptic_table(pq_results, classwise_results, logger=logger)
+
+        return parse_pq_results(pq_results)
 
     def evaluate(self,
                  results,
-                 metric='pq',
+                 metric='PQ',
                  logger=None,
                  jsonfile_prefix=None,
+                 classwise=False,
                  **kwargs):
         """Evaluation in COCO Panoptic protocol.
 
         Args:
             results (list[dict]): Testing results of the dataset.
             metric (str | list[str]): Metrics to be evaluated. Only
-                support 'pq' at present.
+                support 'PQ' at present. 'pq' will be regarded as 'PQ.
             logger (logging.Logger | str | None): Logger used for printing
                 related information during evaluation. Default: None.
             jsonfile_prefix (str | None): The prefix of json files. It includes
                 the file path and the prefix of filename, e.g., "a/b/prefix".
                 If not specified, a temp file will be created. Default: None.
+            classwise (bool): Whether to print classwise evaluation results.
+                Default: False.
 
         Returns:
             dict[str, float]: COCO Panoptic style evaluation metric.
         """
         metrics = metric if isinstance(metric, list) else [metric]
-        allowed_metrics = ['pq']  # todo: support other metrics like 'bbox'
+        # Compatible with lowercase 'pq'
+        metrics = ['PQ' if metric == 'pq' else metric for metric in metrics]
+        allowed_metrics = ['PQ']  # todo: support other metrics like 'bbox'
         for metric in metrics:
             if metric not in allowed_metrics:
                 raise KeyError(f'metric {metric} is not supported')
@@ -469,12 +477,68 @@ class CocoPanopticDataset(CocoDataset):
         result_files, tmp_dir = self.format_results(results, jsonfile_prefix)
         eval_results = {}
 
-        outfile_prefix = tmp_dir if tmp_dir is not None else jsonfile_prefix
-        if 'pq' in metrics:
+        outfile_prefix = os.path.join(tmp_dir.name, 'results') \
+            if tmp_dir is not None else jsonfile_prefix
+        if 'PQ' in metrics:
             eval_pan_results = self.evaluate_pan_json(result_files,
-                                                      outfile_prefix, logger)
+                                                      outfile_prefix, logger,
+                                                      classwise)
             eval_results.update(eval_pan_results)
 
         if tmp_dir is not None:
             tmp_dir.cleanup()
         return eval_results
+
+
+def parse_pq_results(pq_results):
+    """Parse the Panoptic Quality results."""
+    result = dict()
+    result['PQ'] = 100 * pq_results['All']['pq']
+    result['SQ'] = 100 * pq_results['All']['sq']
+    result['RQ'] = 100 * pq_results['All']['rq']
+    result['PQ_th'] = 100 * pq_results['Things']['pq']
+    result['SQ_th'] = 100 * pq_results['Things']['sq']
+    result['RQ_th'] = 100 * pq_results['Things']['rq']
+    result['PQ_st'] = 100 * pq_results['Stuff']['pq']
+    result['SQ_st'] = 100 * pq_results['Stuff']['sq']
+    result['RQ_st'] = 100 * pq_results['Stuff']['rq']
+    return result
+
+
+def print_panoptic_table(pq_results, classwise_results=None, logger=None):
+    """Print the panoptic evaluation results table.
+
+    Args:
+        pq_results(dict): The Panoptic Quality results.
+        classwise_results(dict | None): The classwise Panoptic Quality results.
+            The keys are class names and the values are metrics.
+        logger (logging.Logger | str | None): Logger used for printing
+            related information during evaluation. Default: None.
+    """
+
+    headers = ['', 'PQ', 'SQ', 'RQ', 'categories']
+    data = [headers]
+    for name in ['All', 'Things', 'Stuff']:
+        numbers = [
+            f'{(pq_results[name][k] * 100):0.3f}' for k in ['pq', 'sq', 'rq']
+        ]
+        row = [name] + numbers + [pq_results[name]['n']]
+        data.append(row)
+    table = AsciiTable(data)
+    print_log('Panoptic Evaluation Results:\n' + table.table, logger=logger)
+
+    if classwise_results is not None:
+        class_metrics = [(name, ) + tuple(f'{(metrics[k] * 100):0.3f}'
+                                          for k in ['pq', 'sq', 'rq'])
+                         for name, metrics in classwise_results.items()]
+        num_columns = min(8, len(class_metrics) * 4)
+        results_flatten = list(itertools.chain(*class_metrics))
+        headers = ['category', 'PQ', 'SQ', 'RQ'] * (num_columns // 4)
+        results_2d = itertools.zip_longest(
+            *[results_flatten[i::num_columns] for i in range(num_columns)])
+        data = [headers]
+        data += [result for result in results_2d]
+        table = AsciiTable(data)
+        print_log(
+            'Classwise Panoptic Evaluation Results:\n' + table.table,
+            logger=logger)
diff --git a/mmdet/datasets/custom.py b/mmdet/datasets/custom.py
index 88a78b0e..0e82fa39 100644
--- a/mmdet/datasets/custom.py
+++ b/mmdet/datasets/custom.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import warnings
 from collections import OrderedDict
diff --git a/mmdet/datasets/dataset_wrappers.py b/mmdet/datasets/dataset_wrappers.py
index 422cca31..84cdc0ff 100644
--- a/mmdet/datasets/dataset_wrappers.py
+++ b/mmdet/datasets/dataset_wrappers.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import bisect
 import collections
 import copy
@@ -367,8 +368,6 @@ class MultiImageMixDataset:
 
             if 'mix_results' in results:
                 results.pop('mix_results')
-            if 'img_scale' in results:
-                results.pop('img_scale')
 
         return results
 
diff --git a/mmdet/datasets/deepfashion.py b/mmdet/datasets/deepfashion.py
index 11253760..5c99ed5d 100644
--- a/mmdet/datasets/deepfashion.py
+++ b/mmdet/datasets/deepfashion.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .builder import DATASETS
 from .coco import CocoDataset
 
diff --git a/mmdet/datasets/lvis.py b/mmdet/datasets/lvis.py
index 3cf48930..d7a2e919 100644
--- a/mmdet/datasets/lvis.py
+++ b/mmdet/datasets/lvis.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import itertools
 import logging
 import os.path as osp
diff --git a/mmdet/datasets/pipelines/__init__.py b/mmdet/datasets/pipelines/__init__.py
index 1016985f..8730527a 100644
--- a/mmdet/datasets/pipelines/__init__.py
+++ b/mmdet/datasets/pipelines/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .auto_augment import (AutoAugment, BrightnessTransform, ColorTransform,
                            ContrastTransform, EqualizeTransform, Rotate, Shear,
                            Translate)
diff --git a/mmdet/datasets/pipelines/auto_augment.py b/mmdet/datasets/pipelines/auto_augment.py
index 3e0b1e16..ab62bf20 100644
--- a/mmdet/datasets/pipelines/auto_augment.py
+++ b/mmdet/datasets/pipelines/auto_augment.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import cv2
diff --git a/mmdet/datasets/pipelines/compose.py b/mmdet/datasets/pipelines/compose.py
index 15675305..15fd99c7 100644
--- a/mmdet/datasets/pipelines/compose.py
+++ b/mmdet/datasets/pipelines/compose.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import collections
 
 from mmcv.utils import build_from_cfg
diff --git a/mmdet/datasets/pipelines/formating.py b/mmdet/datasets/pipelines/formating.py
index f71bca13..b02c4c1f 100644
--- a/mmdet/datasets/pipelines/formating.py
+++ b/mmdet/datasets/pipelines/formating.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from collections.abc import Sequence
 
 import mmcv
@@ -92,7 +93,7 @@ class ImageToTensor:
             img = results[key]
             if len(img.shape) < 3:
                 img = np.expand_dims(img, -1)
-            results[key] = to_tensor(img.transpose(2, 0, 1))
+            results[key] = (to_tensor(img.transpose(2, 0, 1))).contiguous()
         return results
 
     def __repr__(self):
diff --git a/mmdet/datasets/pipelines/instaboost.py b/mmdet/datasets/pipelines/instaboost.py
index 83fe4105..08ff5417 100644
--- a/mmdet/datasets/pipelines/instaboost.py
+++ b/mmdet/datasets/pipelines/instaboost.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 
 from ..builder import PIPELINES
diff --git a/mmdet/datasets/pipelines/loading.py b/mmdet/datasets/pipelines/loading.py
index 5ea7acfa..1f8ba37e 100644
--- a/mmdet/datasets/pipelines/loading.py
+++ b/mmdet/datasets/pipelines/loading.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 
 import mmcv
@@ -426,6 +427,10 @@ class LoadPanopticAnnotations(LoadAnnotations):
     def _load_masks_and_semantic_segs(self, results):
         """Private function to load mask and semantic segmentation annotations.
 
+        In gt_semantic_seg, the foreground label is from `0` to
+        `num_things - 1`, the background label is from `num_things` to
+        `num_things + num_stuff - 1`, 255 means the ignored label (`VOID`).
+
         Args:
             results (dict): Result dict from :obj:`mmdet.CustomDataset`.
 
@@ -445,11 +450,11 @@ class LoadPanopticAnnotations(LoadAnnotations):
         pan_png = rgb2id(pan_png)
 
         gt_masks = []
-        gt_seg = np.zeros_like(pan_png)  # 0 as ignore
+        gt_seg = np.zeros_like(pan_png) + 255  # 255 as ignore
 
         for mask_info in results['ann_info']['masks']:
             mask = (pan_png == mask_info['id'])
-            gt_seg = np.where(mask, mask_info['category'] + 1, gt_seg)
+            gt_seg = np.where(mask, mask_info['category'], gt_seg)
 
             # The legal thing masks
             if mask_info.get('is_thing'):
diff --git a/mmdet/datasets/pipelines/test_time_aug.py b/mmdet/datasets/pipelines/test_time_aug.py
index fe5175ae..0a0633c5 100644
--- a/mmdet/datasets/pipelines/test_time_aug.py
+++ b/mmdet/datasets/pipelines/test_time_aug.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import mmcv
diff --git a/mmdet/datasets/pipelines/transforms.py b/mmdet/datasets/pipelines/transforms.py
index 022ead87..2cd8face 100644
--- a/mmdet/datasets/pipelines/transforms.py
+++ b/mmdet/datasets/pipelines/transforms.py
@@ -1,6 +1,8 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import inspect
 import math
+import warnings
 
 import cv2
 import mmcv
@@ -268,7 +270,7 @@ class Resize:
                     results['scale'],
                     interpolation='nearest',
                     backend=self.backend)
-            results['gt_semantic_seg'] = gt_seg
+            results[key] = gt_seg
 
     def __call__(self, results):
         """Call function to resize images, bounding boxes, masks, semantic
@@ -566,7 +568,7 @@ class RandomShift:
 
 @PIPELINES.register_module()
 class Pad:
-    """Pad the image & mask.
+    """Pad the image & masks & segmentation map.
 
     There are two padding modes: (1) pad to a fixed size and (2) pad to the
     minimum size that is divisible by some number.
@@ -576,17 +578,25 @@ class Pad:
         size (tuple, optional): Fixed padding size.
         size_divisor (int, optional): The divisor of padded size.
         pad_to_square (bool): Whether to pad the image into a square.
-           Currently only used for YOLOX. Default: False.
-        pad_val (float, optional): Padding value, 0 by default.
+            Currently only used for YOLOX. Default: False.
+        pad_val (dict, optional): A dict for padding value, the default
+            value is `dict(img=0, masks=0, seg=255)`.
     """
 
     def __init__(self,
                  size=None,
                  size_divisor=None,
                  pad_to_square=False,
-                 pad_val=0):
+                 pad_val=dict(img=0, masks=0, seg=255)):
         self.size = size
         self.size_divisor = size_divisor
+        if isinstance(pad_val, float) or isinstance(pad_val, int):
+            warnings.warn(
+                'pad_val of float type is deprecated now, '
+                f'please use pad_val=dict(img={pad_val}, '
+                f'masks={pad_val}, seg=255) instead.', DeprecationWarning)
+            pad_val = dict(img=pad_val, masks=pad_val, seg=255)
+        assert isinstance(pad_val, dict)
         self.pad_val = pad_val
         self.pad_to_square = pad_to_square
 
@@ -601,16 +611,17 @@ class Pad:
 
     def _pad_img(self, results):
         """Pad images according to ``self.size``."""
+        pad_val = self.pad_val.get('img', 0)
         for key in results.get('img_fields', ['img']):
             if self.pad_to_square:
                 max_size = max(results[key].shape[:2])
                 self.size = (max_size, max_size)
             if self.size is not None:
                 padded_img = mmcv.impad(
-                    results[key], shape=self.size, pad_val=self.pad_val)
+                    results[key], shape=self.size, pad_val=pad_val)
             elif self.size_divisor is not None:
                 padded_img = mmcv.impad_to_multiple(
-                    results[key], self.size_divisor, pad_val=self.pad_val)
+                    results[key], self.size_divisor, pad_val=pad_val)
             results[key] = padded_img
         results['pad_shape'] = padded_img.shape
         results['pad_fixed_size'] = self.size
@@ -619,15 +630,17 @@ class Pad:
     def _pad_masks(self, results):
         """Pad masks according to ``results['pad_shape']``."""
         pad_shape = results['pad_shape'][:2]
+        pad_val = self.pad_val.get('masks', 0)
         for key in results.get('mask_fields', []):
-            results[key] = results[key].pad(pad_shape, pad_val=self.pad_val)
+            results[key] = results[key].pad(pad_shape, pad_val=pad_val)
 
     def _pad_seg(self, results):
         """Pad semantic segmentation map according to
         ``results['pad_shape']``."""
+        pad_val = self.pad_val.get('seg', 255)
         for key in results.get('seg_fields', []):
             results[key] = mmcv.impad(
-                results[key], shape=results['pad_shape'][:2])
+                results[key], shape=results['pad_shape'][:2], pad_val=pad_val)
 
     def __call__(self, results):
         """Call function to pad images, masks, semantic segmentation maps.
@@ -714,6 +727,8 @@ class RandomCrop:
             in range [crop_size[0], min(w, crop_size[1])]. Default "absolute".
         allow_negative_crop (bool, optional): Whether to allow a crop that does
             not contain any bbox area. Default False.
+        recompute_bbox (bool, optional): Whether to re-compute the boxes based
+            on cropped instance masks. Default False.
         bbox_clip_border (bool, optional): Whether clip the objects outside
             the border of the image. Defaults to True.
 
@@ -732,6 +747,7 @@ class RandomCrop:
                  crop_size,
                  crop_type='absolute',
                  allow_negative_crop=False,
+                 recompute_bbox=False,
                  bbox_clip_border=True):
         if crop_type not in [
                 'relative_range', 'relative', 'absolute', 'absolute_range'
@@ -747,6 +763,7 @@ class RandomCrop:
         self.crop_type = crop_type
         self.allow_negative_crop = allow_negative_crop
         self.bbox_clip_border = bbox_clip_border
+        self.recompute_bbox = recompute_bbox
         # The key correspondence from bboxes to labels and masks.
         self.bbox2label = {
             'gt_bboxes': 'gt_labels',
@@ -815,6 +832,8 @@ class RandomCrop:
                 results[mask_key] = results[mask_key][
                     valid_inds.nonzero()[0]].crop(
                         np.asarray([crop_x1, crop_y1, crop_x2, crop_y2]))
+                if self.recompute_bbox:
+                    results[key] = results[mask_key].get_bboxes()
 
         # crop semantic seg
         for key in results.get('seg_fields', []):
@@ -1964,16 +1983,20 @@ class Mosaic:
            image. Default to (640, 640).
         center_ratio_range (Sequence[float]): Center ratio range of mosaic
            output. Default to (0.5, 1.5).
+        min_bbox_size (int | float): The minimum pixel for filtering
+            invalid bboxes after the mosaic pipeline. Default to 0.
         pad_val (int): Pad value. Default to 114.
     """
 
     def __init__(self,
                  img_scale=(640, 640),
                  center_ratio_range=(0.5, 1.5),
+                 min_bbox_size=0,
                  pad_val=114):
         assert isinstance(img_scale, tuple)
         self.img_scale = img_scale
         self.center_ratio_range = center_ratio_range
+        self.min_bbox_size = min_bbox_size
         self.pad_val = pad_val
 
     def __call__(self, results):
@@ -2080,6 +2103,9 @@ class Mosaic:
                                              2 * self.img_scale[0])
             mosaic_labels = np.concatenate(mosaic_labels, 0)
 
+            mosaic_bboxes, mosaic_labels = \
+                self._filter_box_candidates(mosaic_bboxes, mosaic_labels)
+
         results['img'] = mosaic_img
         results['img_shape'] = mosaic_img.shape
         results['ori_shape'] = mosaic_img.shape
@@ -2131,7 +2157,7 @@ class Mosaic:
             x1, y1, x2, y2 = max(center_position_xy[0] - img_shape_wh[0], 0), \
                              center_position_xy[1], \
                              center_position_xy[0], \
-                             min(self.img_scale[1] * 2, center_position_xy[1] +
+                             min(self.img_scale[0] * 2, center_position_xy[1] +
                                  img_shape_wh[1])
             crop_coord = img_shape_wh[0] - (x2 - x1), 0, img_shape_wh[0], min(
                 y2 - y1, img_shape_wh[1])
@@ -2150,6 +2176,15 @@ class Mosaic:
         paste_coord = x1, y1, x2, y2
         return paste_coord, crop_coord
 
+    def _filter_box_candidates(self, bboxes, labels):
+        """Filter out bboxes too small after Mosaic."""
+        bbox_w = bboxes[:, 2] - bboxes[:, 0]
+        bbox_h = bboxes[:, 3] - bboxes[:, 1]
+        valid_inds = (bbox_w > self.min_bbox_size) & \
+                     (bbox_h > self.min_bbox_size)
+        valid_inds = np.nonzero(valid_inds)[0]
+        return bboxes[valid_inds], labels[valid_inds]
+
     def __repr__(self):
         repr_str = self.__class__.__name__
         repr_str += f'img_scale={self.img_scale}, '
@@ -2456,7 +2491,7 @@ class RandomAffine:
         width = img.shape[1] + self.border[1] * 2
 
         # Center
-        center_matrix = np.eye(3)
+        center_matrix = np.eye(3, dtype=np.float32)
         center_matrix[0, 2] = -img.shape[1] / 2  # x translation (pixels)
         center_matrix[1, 2] = -img.shape[0] / 2  # y translation (pixels)
 
@@ -2561,21 +2596,24 @@ class RandomAffine:
     @staticmethod
     def _get_rotation_matrix(rotate_degrees):
         radian = math.radians(rotate_degrees)
-        rotation_matrix = np.array([[np.cos(radian), -np.sin(radian), 0.],
-                                    [np.sin(radian),
-                                     np.cos(radian), 0.], [0., 0., 1.]])
+        rotation_matrix = np.array(
+            [[np.cos(radian), -np.sin(radian), 0.],
+             [np.sin(radian), np.cos(radian), 0.], [0., 0., 1.]],
+            dtype=np.float32)
         return rotation_matrix
 
     @staticmethod
     def _get_scaling_matrix(scale_ratio):
-        scaling_matrix = np.array([[scale_ratio, 0., 0.],
-                                   [0., scale_ratio, 0.], [0., 0., 1.]])
+        scaling_matrix = np.array(
+            [[scale_ratio, 0., 0.], [0., scale_ratio, 0.], [0., 0., 1.]],
+            dtype=np.float32)
         return scaling_matrix
 
     @staticmethod
     def _get_share_matrix(scale_ratio):
-        scaling_matrix = np.array([[scale_ratio, 0., 0.],
-                                   [0., scale_ratio, 0.], [0., 0., 1.]])
+        scaling_matrix = np.array(
+            [[scale_ratio, 0., 0.], [0., scale_ratio, 0.], [0., 0., 1.]],
+            dtype=np.float32)
         return scaling_matrix
 
     @staticmethod
@@ -2583,10 +2621,12 @@ class RandomAffine:
         x_radian = math.radians(x_shear_degrees)
         y_radian = math.radians(y_shear_degrees)
         shear_matrix = np.array([[1, np.tan(x_radian), 0.],
-                                 [np.tan(y_radian), 1, 0.], [0., 0., 1.]])
+                                 [np.tan(y_radian), 1, 0.], [0., 0., 1.]],
+                                dtype=np.float32)
         return shear_matrix
 
     @staticmethod
     def _get_translation_matrix(x, y):
-        translation_matrix = np.array([[1, 0., x], [0., 1, y], [0., 0., 1.]])
+        translation_matrix = np.array([[1, 0., x], [0., 1, y], [0., 0., 1.]],
+                                      dtype=np.float32)
         return translation_matrix
diff --git a/mmdet/datasets/samplers/__init__.py b/mmdet/datasets/samplers/__init__.py
index 2596aeb2..0692656c 100644
--- a/mmdet/datasets/samplers/__init__.py
+++ b/mmdet/datasets/samplers/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .distributed_sampler import DistributedSampler
 from .group_sampler import DistributedGroupSampler, GroupSampler
 
diff --git a/mmdet/datasets/samplers/distributed_sampler.py b/mmdet/datasets/samplers/distributed_sampler.py
index cc610194..d9c25bac 100644
--- a/mmdet/datasets/samplers/distributed_sampler.py
+++ b/mmdet/datasets/samplers/distributed_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch
diff --git a/mmdet/datasets/samplers/group_sampler.py b/mmdet/datasets/samplers/group_sampler.py
index 7d94ef29..783d2b21 100644
--- a/mmdet/datasets/samplers/group_sampler.py
+++ b/mmdet/datasets/samplers/group_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import numpy as np
diff --git a/mmdet/datasets/utils.py b/mmdet/datasets/utils.py
index 4eb64231..cda13a6b 100644
--- a/mmdet/datasets/utils.py
+++ b/mmdet/datasets/utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import warnings
 
@@ -116,7 +117,7 @@ class NumClassCheckHook(Hook):
 
     def _check_head(self, runner):
         """Check whether the `num_classes` in head matches the length of
-        `CLASSSES` in `dataset`.
+        `CLASSES` in `dataset`.
 
         Args:
             runner (obj:`EpochBasedRunner`): Epoch based Runner.
diff --git a/mmdet/datasets/voc.py b/mmdet/datasets/voc.py
index 5a746955..3a344a46 100644
--- a/mmdet/datasets/voc.py
+++ b/mmdet/datasets/voc.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from collections import OrderedDict
 
 from mmcv.utils import print_log
@@ -69,20 +70,31 @@ class VOCDataset(XMLDataset):
             mean_aps = []
             for iou_thr in iou_thrs:
                 print_log(f'\n{"-" * 15}iou_thr: {iou_thr}{"-" * 15}')
+                # Follow the official implementation,
+                # http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar
+                # we should use the legacy coordinate system in mmdet 1.x,
+                # which means w, h should be computed as 'x2 - x1 + 1` and
+                # `y2 - y1 + 1`
                 mean_ap, _ = eval_map(
                     results,
                     annotations,
                     scale_ranges=None,
                     iou_thr=iou_thr,
                     dataset=ds_name,
-                    logger=logger)
+                    logger=logger,
+                    use_legacy_coordinate=True)
                 mean_aps.append(mean_ap)
                 eval_results[f'AP{int(iou_thr * 100):02d}'] = round(mean_ap, 3)
             eval_results['mAP'] = sum(mean_aps) / len(mean_aps)
         elif metric == 'recall':
             gt_bboxes = [ann['bboxes'] for ann in annotations]
             recalls = eval_recalls(
-                gt_bboxes, results, proposal_nums, iou_thrs, logger=logger)
+                gt_bboxes,
+                results,
+                proposal_nums,
+                iou_thrs,
+                logger=logger,
+                use_legacy_coordinate=True)
             for i, num in enumerate(proposal_nums):
                 for j, iou_thr in enumerate(iou_thrs):
                     eval_results[f'recall@{num}@{iou_thr}'] = recalls[i, j]
diff --git a/mmdet/datasets/wider_face.py b/mmdet/datasets/wider_face.py
index 3a13907d..0c79dd60 100644
--- a/mmdet/datasets/wider_face.py
+++ b/mmdet/datasets/wider_face.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import xml.etree.ElementTree as ET
 
diff --git a/mmdet/datasets/xml_style.py b/mmdet/datasets/xml_style.py
index 71069488..039d5d7d 100644
--- a/mmdet/datasets/xml_style.py
+++ b/mmdet/datasets/xml_style.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import xml.etree.ElementTree as ET
 
@@ -17,11 +18,19 @@ class XMLDataset(CustomDataset):
         min_size (int | float, optional): The minimum size of bounding
             boxes in the images. If the size of a bounding box is less than
             ``min_size``, it would be add to ignored field.
+        img_subdir (str): Subdir where images are stored. Default: JPEGImages.
+        ann_subdir (str): Subdir where annotations are. Default: Annotations.
     """
 
-    def __init__(self, min_size=None, **kwargs):
+    def __init__(self,
+                 min_size=None,
+                 img_subdir='JPEGImages',
+                 ann_subdir='Annotations',
+                 **kwargs):
         assert self.CLASSES or kwargs.get(
             'classes', None), 'CLASSES in `XMLDataset` can not be None.'
+        self.img_subdir = img_subdir
+        self.ann_subdir = ann_subdir
         super(XMLDataset, self).__init__(**kwargs)
         self.cat2label = {cat: i for i, cat in enumerate(self.CLASSES)}
         self.min_size = min_size
@@ -39,8 +48,8 @@ class XMLDataset(CustomDataset):
         data_infos = []
         img_ids = mmcv.list_from_file(ann_file)
         for img_id in img_ids:
-            filename = f'JPEGImages/{img_id}.jpg'
-            xml_path = osp.join(self.img_prefix, 'Annotations',
+            filename = osp.join(self.img_subdir, f'{img_id}.jpg')
+            xml_path = osp.join(self.img_prefix, self.ann_subdir,
                                 f'{img_id}.xml')
             tree = ET.parse(xml_path)
             root = tree.getroot()
@@ -49,8 +58,7 @@ class XMLDataset(CustomDataset):
                 width = int(size.find('width').text)
                 height = int(size.find('height').text)
             else:
-                img_path = osp.join(self.img_prefix, 'JPEGImages',
-                                    '{}.jpg'.format(img_id))
+                img_path = osp.join(self.img_prefix, filename)
                 img = Image.open(img_path)
                 width, height = img.size
             data_infos.append(
@@ -66,7 +74,7 @@ class XMLDataset(CustomDataset):
                 continue
             if self.filter_empty_gt:
                 img_id = img_info['id']
-                xml_path = osp.join(self.img_prefix, 'Annotations',
+                xml_path = osp.join(self.img_prefix, self.ann_subdir,
                                     f'{img_id}.xml')
                 tree = ET.parse(xml_path)
                 root = tree.getroot()
@@ -90,7 +98,7 @@ class XMLDataset(CustomDataset):
         """
 
         img_id = self.data_infos[idx]['id']
-        xml_path = osp.join(self.img_prefix, 'Annotations', f'{img_id}.xml')
+        xml_path = osp.join(self.img_prefix, self.ann_subdir, f'{img_id}.xml')
         tree = ET.parse(xml_path)
         root = tree.getroot()
         bboxes = []
@@ -157,7 +165,7 @@ class XMLDataset(CustomDataset):
 
         cat_ids = []
         img_id = self.data_infos[idx]['id']
-        xml_path = osp.join(self.img_prefix, 'Annotations', f'{img_id}.xml')
+        xml_path = osp.join(self.img_prefix, self.ann_subdir, f'{img_id}.xml')
         tree = ET.parse(xml_path)
         root = tree.getroot()
         for obj in root.findall('object'):
diff --git a/mmdet/models/__init__.py b/mmdet/models/__init__.py
index 087471d5..12efb013 100644
--- a/mmdet/models/__init__.py
+++ b/mmdet/models/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .backbones import *  # noqa: F401,F403
 from .builder import (BACKBONES, DETECTORS, HEADS, LOSSES, NECKS,
                       ROI_EXTRACTORS, SHARED_HEADS, build_backbone,
@@ -9,6 +10,7 @@ from .losses import *  # noqa: F401,F403
 from .necks import *  # noqa: F401,F403
 from .plugins import *  # noqa: F401,F403
 from .roi_heads import *  # noqa: F401,F403
+from .seg_heads import *  # noqa: F401,F403
 
 __all__ = [
     'BACKBONES', 'NECKS', 'ROI_EXTRACTORS', 'SHARED_HEADS', 'HEADS', 'LOSSES',
diff --git a/mmdet/models/backbones/__init__.py b/mmdet/models/backbones/__init__.py
index 6395b776..5418105a 100644
--- a/mmdet/models/backbones/__init__.py
+++ b/mmdet/models/backbones/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .csp_darknet import CSPDarknet
 from .darknet import Darknet
 from .detectors_resnet import DetectoRS_ResNet
@@ -5,16 +6,21 @@ from .detectors_resnext import DetectoRS_ResNeXt
 from .hourglass import HourglassNet
 from .hrnet import HRNet
 from .mobilenet_v2 import MobileNetV2
+from .pvt import PyramidVisionTransformer, PyramidVisionTransformerV2
 from .regnet import RegNet
 from .res2net import Res2Net
 from .resnest import ResNeSt
 from .resnet import ResNet, ResNetV1d
 from .resnext import ResNeXt
 from .ssd_vgg import SSDVGG
+from .swin import SwinTransformer
 from .trident_resnet import TridentResNet
+from .efficientnet import EfficientNet
 
 __all__ = [
     'RegNet', 'ResNet', 'ResNetV1d', 'ResNeXt', 'SSDVGG', 'HRNet',
     'MobileNetV2', 'Res2Net', 'HourglassNet', 'DetectoRS_ResNet',
-    'DetectoRS_ResNeXt', 'Darknet', 'ResNeSt', 'TridentResNet', 'CSPDarknet'
+    'DetectoRS_ResNeXt', 'Darknet', 'ResNeSt', 'TridentResNet', 'CSPDarknet',
+    'SwinTransformer', 'PyramidVisionTransformer', 'PyramidVisionTransformerV2',
+    'EfficientNet'
 ]
diff --git a/mmdet/models/backbones/csp_darknet.py b/mmdet/models/backbones/csp_darknet.py
index f1e80ba2..41f5a1ac 100644
--- a/mmdet/models/backbones/csp_darknet.py
+++ b/mmdet/models/backbones/csp_darknet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch
diff --git a/mmdet/models/backbones/darknet.py b/mmdet/models/backbones/darknet.py
index 9e07ba08..adfb1159 100644
--- a/mmdet/models/backbones/darknet.py
+++ b/mmdet/models/backbones/darknet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # Copyright (c) 2019 Western Digital Corporation or its affiliates.
 
 import warnings
@@ -132,7 +133,7 @@ class Darknet(BaseModule):
         self.norm_eval = norm_eval
 
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is deprecated, '
                           'please use "init_cfg" instead')
diff --git a/mmdet/models/backbones/detectors_resnet.py b/mmdet/models/backbones/detectors_resnet.py
index c7ec491d..a3c0d40b 100644
--- a/mmdet/models/backbones/detectors_resnet.py
+++ b/mmdet/models/backbones/detectors_resnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.utils.checkpoint as cp
 from mmcv.cnn import (build_conv_layer, build_norm_layer, constant_init,
@@ -239,7 +240,7 @@ class DetectoRS_ResNet(ResNet):
                  init_cfg=None,
                  **kwargs):
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
         self.pretrained = pretrained
         if init_cfg is not None:
             assert isinstance(init_cfg, dict), \
diff --git a/mmdet/models/backbones/detectors_resnext.py b/mmdet/models/backbones/detectors_resnext.py
index 57d032fe..5e8b20a0 100644
--- a/mmdet/models/backbones/detectors_resnext.py
+++ b/mmdet/models/backbones/detectors_resnext.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 from mmcv.cnn import build_conv_layer, build_norm_layer
diff --git a/mmdet/models/backbones/hourglass.py b/mmdet/models/backbones/hourglass.py
index d9e16e67..f0dfb434 100644
--- a/mmdet/models/backbones/hourglass.py
+++ b/mmdet/models/backbones/hourglass.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
@@ -148,8 +149,16 @@ class HourglassNet(BaseModule):
         cur_channel = stage_channels[0]
 
         self.stem = nn.Sequential(
-            ConvModule(3, 128, 7, padding=3, stride=2, norm_cfg=norm_cfg),
-            ResLayer(BasicBlock, 128, 256, 1, stride=2, norm_cfg=norm_cfg))
+            ConvModule(
+                3, cur_channel // 2, 7, padding=3, stride=2,
+                norm_cfg=norm_cfg),
+            ResLayer(
+                BasicBlock,
+                cur_channel // 2,
+                cur_channel,
+                1,
+                stride=2,
+                norm_cfg=norm_cfg))
 
         self.hourglass_modules = nn.ModuleList([
             HourglassModule(downsample_times, stage_channels, stage_blocks)
diff --git a/mmdet/models/backbones/hrnet.py b/mmdet/models/backbones/hrnet.py
index a3b99558..06c210a6 100644
--- a/mmdet/models/backbones/hrnet.py
+++ b/mmdet/models/backbones/hrnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch.nn as nn
@@ -203,24 +204,37 @@ class HRModule(BaseModule):
 class HRNet(BaseModule):
     """HRNet backbone.
 
-    High-Resolution Representations for Labeling Pixels and Regions
-    arXiv: https://arxiv.org/abs/1904.04514
+    `High-Resolution Representations for Labeling Pixels and Regions
+    arXiv: <https://arxiv.org/abs/1904.04514>`_.
 
     Args:
-        extra (dict): detailed configuration for each stage of HRNet.
+        extra (dict): Detailed configuration for each stage of HRNet.
+            There must be 4 stages, the configuration for each stage must have
+            5 keys:
+
+                - num_modules(int): The number of HRModule in this stage.
+                - num_branches(int): The number of branches in the HRModule.
+                - block(str): The type of convolution block.
+                - num_blocks(tuple): The number of blocks in each branch.
+                    The length must be equal to num_branches.
+                - num_channels(tuple): The number of channels in each branch.
+                    The length must be equal to num_branches.
         in_channels (int): Number of input image channels. Default: 3.
-        conv_cfg (dict): dictionary to construct and config conv layer.
-        norm_cfg (dict): dictionary to construct and config norm layer.
+        conv_cfg (dict): Dictionary to construct and config conv layer.
+        norm_cfg (dict): Dictionary to construct and config norm layer.
         norm_eval (bool): Whether to set norm layers to eval mode, namely,
             freeze running stats (mean and var). Note: Effect on Batch Norm
-            and its variants only.
+            and its variants only. Default: True.
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
-            memory while slowing down the training speed.
-        zero_init_residual (bool): whether to use zero init for last norm layer
-            in resblocks to let them behave as identity.
-        pretrained (str, optional): model pretrained path. Default: None
+            memory while slowing down the training speed. Default: False.
+        zero_init_residual (bool): Whether to use zero init for last norm layer
+            in resblocks to let them behave as identity. Default: False.
+        multiscale_output (bool): Whether to output multi-level features
+            produced by multiple branches. If False, only the first level
+            feature will be output. Default: True.
+        pretrained (str, optional): Model pretrained path. Default: None.
         init_cfg (dict or list[dict], optional): Initialization config dict.
-            Default: None
+            Default: None.
 
     Example:
         >>> from mmdet.models import HRNet
@@ -272,13 +286,14 @@ class HRNet(BaseModule):
                  norm_eval=True,
                  with_cp=False,
                  zero_init_residual=False,
+                 multiscale_output=True,
                  pretrained=None,
                  init_cfg=None):
         super(HRNet, self).__init__(init_cfg)
 
         self.pretrained = pretrained
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is deprecated, '
                           'please use "init_cfg" instead')
@@ -295,6 +310,16 @@ class HRNet(BaseModule):
         else:
             raise TypeError('pretrained must be a str or None')
 
+        # Assert configurations of 4 stages are in extra
+        assert 'stage1' in extra and 'stage2' in extra \
+               and 'stage3' in extra and 'stage4' in extra
+        # Assert whether the length of `num_blocks` and `num_channels` are
+        # equal to `num_branches`
+        for i in range(4):
+            cfg = extra[f'stage{i + 1}']
+            assert len(cfg['num_blocks']) == cfg['num_branches'] and \
+                   len(cfg['num_channels']) == cfg['num_branches']
+
         self.extra = extra
         self.conv_cfg = conv_cfg
         self.norm_cfg = norm_cfg
@@ -372,7 +397,7 @@ class HRNet(BaseModule):
         self.transition3 = self._make_transition_layer(pre_stage_channels,
                                                        num_channels)
         self.stage4, pre_stage_channels = self._make_stage(
-            self.stage4_cfg, num_channels)
+            self.stage4_cfg, num_channels, multiscale_output=multiscale_output)
 
     @property
     def norm1(self):
diff --git a/mmdet/models/backbones/mobilenet_v2.py b/mmdet/models/backbones/mobilenet_v2.py
index 0905a473..8c6fcfaa 100644
--- a/mmdet/models/backbones/mobilenet_v2.py
+++ b/mmdet/models/backbones/mobilenet_v2.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch.nn as nn
@@ -57,7 +58,7 @@ class MobileNetV2(BaseModule):
 
         self.pretrained = pretrained
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is deprecated, '
                           'please use "init_cfg" instead')
diff --git a/mmdet/models/backbones/pvt.py b/mmdet/models/backbones/pvt.py
new file mode 100644
index 00000000..243ff603
--- /dev/null
+++ b/mmdet/models/backbones/pvt.py
@@ -0,0 +1,554 @@
+import math
+import warnings
+
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from mmcv.cnn import (Conv2d, build_activation_layer, build_norm_layer,
+                      constant_init, normal_init, trunc_normal_init)
+from mmcv.cnn.bricks.drop import build_dropout
+from mmcv.cnn.bricks.transformer import MultiheadAttention
+from mmcv.runner import (BaseModule, ModuleList, Sequential, _load_checkpoint,
+                         load_state_dict)
+from torch.nn.modules.utils import _pair as to_2tuple
+
+from ...utils import get_root_logger
+from ..builder import BACKBONES
+from ..utils import PatchEmbed, nchw_to_nlc, nlc_to_nchw, pvt_convert
+
+
+class MixFFN(BaseModule):
+    """An implementation of MixFFN of PVT.
+
+    The differences between MixFFN & FFN:
+        1. Use 1X1 Conv to replace Linear layer.
+        2. Introduce 3X3 Depth-wise Conv to encode positional information.
+
+    Args:
+        embed_dims (int): The feature dimension. Same as
+            `MultiheadAttention`.
+        feedforward_channels (int): The hidden dimension of FFNs.
+        act_cfg (dict, optional): The activation config for FFNs.
+            Default: dict(type='GELU').
+        ffn_drop (float, optional): Probability of an element to be
+            zeroed in FFN. Default 0.0.
+        dropout_layer (obj:`ConfigDict`): The dropout_layer used
+            when adding the shortcut.
+            Default: None.
+        use_conv (bool): If True, add 3x3 DWConv between two Linear layers.
+            Defaults: False.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self,
+                 embed_dims,
+                 feedforward_channels,
+                 act_cfg=dict(type='GELU'),
+                 ffn_drop=0.,
+                 dropout_layer=None,
+                 use_conv=False,
+                 init_cfg=None):
+        super(MixFFN, self).__init__(init_cfg=init_cfg)
+
+        self.embed_dims = embed_dims
+        self.feedforward_channels = feedforward_channels
+        self.act_cfg = act_cfg
+        activate = build_activation_layer(act_cfg)
+
+        in_channels = embed_dims
+        fc1 = Conv2d(
+            in_channels=in_channels,
+            out_channels=feedforward_channels,
+            kernel_size=1,
+            stride=1,
+            bias=True)
+        if use_conv:
+            # 3x3 depth wise conv to provide positional encode information
+            dw_conv = Conv2d(
+                in_channels=feedforward_channels,
+                out_channels=feedforward_channels,
+                kernel_size=3,
+                stride=1,
+                padding=(3 - 1) // 2,
+                bias=True,
+                groups=feedforward_channels)
+        fc2 = Conv2d(
+            in_channels=feedforward_channels,
+            out_channels=in_channels,
+            kernel_size=1,
+            stride=1,
+            bias=True)
+        drop = nn.Dropout(ffn_drop)
+        layers = [fc1, activate, drop, fc2, drop]
+        if use_conv:
+            layers.insert(1, dw_conv)
+        self.layers = Sequential(*layers)
+        self.dropout_layer = build_dropout(
+            dropout_layer) if dropout_layer else torch.nn.Identity()
+
+    def forward(self, x, hw_shape, identity=None):
+        out = nlc_to_nchw(x, hw_shape)
+        out = self.layers(out)
+        out = nchw_to_nlc(out)
+        if identity is None:
+            identity = x
+        return identity + self.dropout_layer(out)
+
+
+class SpatialReductionAttention(MultiheadAttention):
+    """An implementation of Spatial Reduction Attention of PVT.
+
+    This module is modified from MultiheadAttention which is a module from
+    mmcv.cnn.bricks.transformer.
+
+    Args:
+        embed_dims (int): The embedding dimension.
+        num_heads (int): Parallel attention heads.
+        attn_drop (float): A Dropout layer on attn_output_weights.
+            Default: 0.0.
+        proj_drop (float): A Dropout layer after `nn.MultiheadAttention`.
+            Default: 0.0.
+        dropout_layer (obj:`ConfigDict`): The dropout_layer used
+            when adding the shortcut. Default: None.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default: False.
+        qkv_bias (bool): enable bias for qkv if True. Default: True.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: dict(type='LN').
+        sr_ratio (int): The ratio of spatial reduction of Spatial Reduction
+            Attention of PVT. Default: 1.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self,
+                 embed_dims,
+                 num_heads,
+                 attn_drop=0.,
+                 proj_drop=0.,
+                 dropout_layer=None,
+                 batch_first=True,
+                 qkv_bias=True,
+                 norm_cfg=dict(type='LN'),
+                 sr_ratio=1,
+                 init_cfg=None):
+        super().__init__(
+            embed_dims,
+            num_heads,
+            attn_drop,
+            proj_drop,
+            batch_first=batch_first,
+            dropout_layer=dropout_layer,
+            bias=qkv_bias,
+            init_cfg=init_cfg)
+
+        self.sr_ratio = sr_ratio
+        if sr_ratio > 1:
+            self.sr = Conv2d(
+                in_channels=embed_dims,
+                out_channels=embed_dims,
+                kernel_size=sr_ratio,
+                stride=sr_ratio)
+            # The ret[0] of build_norm_layer is norm name.
+            self.norm = build_norm_layer(norm_cfg, embed_dims)[1]
+
+    def forward(self, x, hw_shape, identity=None):
+
+        x_q = x
+        if self.sr_ratio > 1:
+            x_kv = nlc_to_nchw(x, hw_shape)
+            x_kv = self.sr(x_kv)
+            x_kv = nchw_to_nlc(x_kv)
+            x_kv = self.norm(x_kv)
+        else:
+            x_kv = x
+
+        if identity is None:
+            identity = x_q
+
+        out = self.attn(query=x_q, key=x_kv, value=x_kv)[0]
+
+        return identity + self.dropout_layer(self.proj_drop(out))
+
+
+class PVTEncoderLayer(BaseModule):
+    """Implements one encoder layer in PVT.
+
+    Args:
+        embed_dims (int): The feature dimension.
+        num_heads (int): Parallel attention heads.
+        feedforward_channels (int): The hidden dimension for FFNs.
+        drop_rate (float): Probability of an element to be zeroed.
+            after the feed forward layer. Default: 0.0.
+        attn_drop_rate (float): The drop out rate for attention layer.
+            Default: 0.0.
+        drop_path_rate (float): stochastic depth rate. Default: 0.0.
+        qkv_bias (bool): enable bias for qkv if True.
+            Default: True.
+        act_cfg (dict): The activation config for FFNs.
+            Default: dict(type='GELU').
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: dict(type='LN').
+        sr_ratio (int): The ratio of spatial reduction of Spatial Reduction
+            Attention of PVT. Default: 1.
+        use_conv_ffn (bool): If True, use Convolutional FFN to replace FFN.
+            Default: False.
+        init_cfg (dict, optional): Initialization config dict.
+            Default: None.
+    """
+
+    def __init__(self,
+                 embed_dims,
+                 num_heads,
+                 feedforward_channels,
+                 drop_rate=0.,
+                 attn_drop_rate=0.,
+                 drop_path_rate=0.,
+                 qkv_bias=True,
+                 act_cfg=dict(type='GELU'),
+                 norm_cfg=dict(type='LN'),
+                 sr_ratio=1,
+                 use_conv_ffn=False,
+                 init_cfg=None):
+        super(PVTEncoderLayer, self).__init__(init_cfg=init_cfg)
+
+        # The ret[0] of build_norm_layer is norm name.
+        self.norm1 = build_norm_layer(norm_cfg, embed_dims)[1]
+
+        self.attn = SpatialReductionAttention(
+            embed_dims=embed_dims,
+            num_heads=num_heads,
+            attn_drop=attn_drop_rate,
+            proj_drop=drop_rate,
+            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),
+            qkv_bias=qkv_bias,
+            norm_cfg=norm_cfg,
+            sr_ratio=sr_ratio)
+
+        # The ret[0] of build_norm_layer is norm name.
+        self.norm2 = build_norm_layer(norm_cfg, embed_dims)[1]
+
+        self.ffn = MixFFN(
+            embed_dims=embed_dims,
+            feedforward_channels=feedforward_channels,
+            ffn_drop=drop_rate,
+            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),
+            use_conv=use_conv_ffn,
+            act_cfg=act_cfg)
+
+    def forward(self, x, hw_shape):
+        x = self.attn(self.norm1(x), hw_shape, identity=x)
+        x = self.ffn(self.norm2(x), hw_shape, identity=x)
+
+        return x
+
+
+class AbsolutePositionEmbedding(BaseModule):
+    """An implementation of the absolute position embedding in PVT.
+
+    Args:
+        pos_shape (int): The shape of the absolute position embedding.
+        pos_dim (int): The dimension of the absolute position embedding.
+        drop_rate (float): Probability of an element to be zeroed.
+            Default: 0.0.
+    """
+
+    def __init__(self, pos_shape, pos_dim, drop_rate=0., init_cfg=None):
+        super().__init__(init_cfg=init_cfg)
+
+        if isinstance(pos_shape, int):
+            pos_shape = to_2tuple(pos_shape)
+        elif isinstance(pos_shape, tuple):
+            if len(pos_shape) == 1:
+                pos_shape = to_2tuple(pos_shape[0])
+            assert len(pos_shape) == 2, \
+                f'The size of image should have length 1 or 2, ' \
+                f'but got {len(pos_shape)}'
+        self.pos_shape = pos_shape
+        self.pos_dim = pos_dim
+
+        self.pos_embed = nn.Parameter(
+            torch.zeros(1, pos_shape[0] * pos_shape[1], pos_dim))
+        self.drop = nn.Dropout(p=drop_rate)
+
+    def init_weights(self):
+        trunc_normal_init(self.pos_embed, std=0.02)
+
+    def resize_pos_embed(self, pos_embed, input_shape, mode='bilinear'):
+        """Resize pos_embed weights.
+
+        Resize pos_embed using bilinear interpolate method.
+
+        Args:
+            pos_embed (torch.Tensor): Position embedding weights.
+            input_shape (tuple): Tuple for (downsampled input image height,
+                downsampled input image width).
+            mode (str): Algorithm used for upsampling:
+                ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |
+                ``'trilinear'``. Default: ``'bilinear'``.
+
+        Return:
+            torch.Tensor: The resized pos_embed of shape [B, L_new, C].
+        """
+        assert pos_embed.ndim == 3, 'shape of pos_embed must be [B, L, C]'
+        pos_h, pos_w = self.pos_shape
+        pos_embed_weight = pos_embed[:, (-1 * pos_h * pos_w):]
+        pos_embed_weight = pos_embed_weight.reshape(
+            1, pos_h, pos_w, self.pos_dim).permute(0, 3, 1, 2).contiguous()
+        pos_embed_weight = F.interpolate(
+            pos_embed_weight, size=input_shape, mode=mode)
+        pos_embed_weight = torch.flatten(pos_embed_weight,
+                                         2).transpose(1, 2).contiguous()
+        pos_embed = pos_embed_weight
+
+        return pos_embed
+
+    def forward(self, x, hw_shape, mode='bilinear'):
+        pos_embed = self.resize_pos_embed(self.pos_embed, hw_shape, mode)
+        return self.drop(x + pos_embed)
+
+
+@BACKBONES.register_module()
+class PyramidVisionTransformer(BaseModule):
+    """Pyramid Vision Transformer (PVT)
+
+    Implementation of `Pyramid Vision Transformer: A Versatile Backbone for
+    Dense Prediction without Convolutions
+    <https://arxiv.org/pdf/2102.12122.pdf>`_.
+
+    Args:
+        pretrain_img_size (int | tuple[int]): The size of input image when
+            pretrain. Defaults: 224.
+        in_channels (int): Number of input channels. Default: 3.
+        embed_dims (int): Embedding dimension. Default: 64.
+        num_stags (int): The num of stages. Default: 4.
+        num_layers (Sequence[int]): The layer number of each transformer encode
+            layer. Default: [3, 4, 6, 3].
+        num_heads (Sequence[int]): The attention heads of each transformer
+            encode layer. Default: [1, 2, 5, 8].
+        patch_sizes (Sequence[int]): The patch_size of each patch embedding.
+            Default: [4, 2, 2, 2].
+        strides (Sequence[int]): The stride of each patch embedding.
+            Default: [4, 2, 2, 2].
+        paddings (Sequence[int]): The padding of each patch embedding.
+            Default: [0, 0, 0, 0].
+        sr_ratios (Sequence[int]): The spatial reduction rate of each
+            transformer encode layer. Default: [8, 4, 2, 1].
+        out_indices (Sequence[int] | int): Output from which stages.
+            Default: (0, 1, 2, 3).
+        mlp_ratios (Sequence[int]): The ratio of the mlp hidden dim to the
+            embedding dim of each transformer encode layer.
+            Default: [8, 8, 4, 4].
+        qkv_bias (bool): Enable bias for qkv if True. Default: True.
+        drop_rate (float): Probability of an element to be zeroed.
+            Default 0.0.
+        attn_drop_rate (float): The drop out rate for attention layer.
+            Default 0.0.
+        drop_path_rate (float): stochastic depth rate. Default 0.1.
+        use_abs_pos_embed (bool): If True, add absolute position embedding to
+            the patch embedding. Defaults: True.
+        use_conv_ffn (bool): If True, use Convolutional FFN to replace FFN.
+            Default: False.
+        act_cfg (dict): The activation config for FFNs.
+            Default: dict(type='GELU').
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: dict(type='LN').
+        pretrained (str, optional): model pretrained path. Default: None.
+        convert_weights (bool): The flag indicates whether the
+            pre-trained model is from the original repo. We may need
+            to convert some keys to make it compatible.
+            Default: True.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+            Default: None.
+    """
+
+    def __init__(self,
+                 pretrain_img_size=224,
+                 in_channels=3,
+                 embed_dims=64,
+                 num_stages=4,
+                 num_layers=[3, 4, 6, 3],
+                 num_heads=[1, 2, 5, 8],
+                 patch_sizes=[4, 2, 2, 2],
+                 strides=[4, 2, 2, 2],
+                 paddings=[0, 0, 0, 0],
+                 sr_ratios=[8, 4, 2, 1],
+                 out_indices=(0, 1, 2, 3),
+                 mlp_ratios=[8, 8, 4, 4],
+                 qkv_bias=True,
+                 drop_rate=0.,
+                 attn_drop_rate=0.,
+                 drop_path_rate=0.1,
+                 use_abs_pos_embed=True,
+                 norm_after_stage=False,
+                 use_conv_ffn=False,
+                 act_cfg=dict(type='GELU'),
+                 norm_cfg=dict(type='LN', eps=1e-6),
+                 pretrained=None,
+                 convert_weights=True,
+                 init_cfg=None):
+        super().__init__(init_cfg=init_cfg)
+
+        self.convert_weights = convert_weights
+        if isinstance(pretrain_img_size, int):
+            pretrain_img_size = to_2tuple(pretrain_img_size)
+        elif isinstance(pretrain_img_size, tuple):
+            if len(pretrain_img_size) == 1:
+                pretrain_img_size = to_2tuple(pretrain_img_size[0])
+            assert len(pretrain_img_size) == 2, \
+                f'The size of image should have length 1 or 2, ' \
+                f'but got {len(pretrain_img_size)}'
+
+        assert not (init_cfg and pretrained), \
+            'init_cfg and pretrained cannot be setting at the same time'
+        if isinstance(pretrained, str):
+            warnings.warn('DeprecationWarning: pretrained is deprecated, '
+                          'please use "init_cfg" instead')
+            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
+        elif pretrained is None:
+            self.init_cfg = init_cfg
+        else:
+            raise TypeError('pretrained must be a str or None')
+
+        self.embed_dims = embed_dims
+
+        self.num_stages = num_stages
+        self.num_layers = num_layers
+        self.num_heads = num_heads
+        self.patch_sizes = patch_sizes
+        self.strides = strides
+        self.sr_ratios = sr_ratios
+        assert num_stages == len(num_layers) == len(num_heads) \
+               == len(patch_sizes) == len(strides) == len(sr_ratios)
+
+        self.out_indices = out_indices
+        assert max(out_indices) < self.num_stages
+        self.pretrained = pretrained
+
+        # transformer encoder
+        dpr = [
+            x.item()
+            for x in torch.linspace(0, drop_path_rate, sum(num_layers))
+        ]  # stochastic num_layer decay rule
+
+        cur = 0
+        self.layers = ModuleList()
+        for i, num_layer in enumerate(num_layers):
+            embed_dims_i = embed_dims * num_heads[i]
+            patch_embed = PatchEmbed(
+                in_channels=in_channels,
+                embed_dims=embed_dims_i,
+                kernel_size=patch_sizes[i],
+                stride=strides[i],
+                padding=paddings[i],
+                bias=True,
+                norm_cfg=norm_cfg)
+
+            layers = ModuleList()
+            if use_abs_pos_embed:
+                pos_shape = pretrain_img_size // np.prod(patch_sizes[:i + 1])
+                pos_embed = AbsolutePositionEmbedding(
+                    pos_shape=pos_shape,
+                    pos_dim=embed_dims_i,
+                    drop_rate=drop_rate)
+                layers.append(pos_embed)
+            layers.extend([
+                PVTEncoderLayer(
+                    embed_dims=embed_dims_i,
+                    num_heads=num_heads[i],
+                    feedforward_channels=mlp_ratios[i] * embed_dims_i,
+                    drop_rate=drop_rate,
+                    attn_drop_rate=attn_drop_rate,
+                    drop_path_rate=dpr[cur + idx],
+                    qkv_bias=qkv_bias,
+                    act_cfg=act_cfg,
+                    norm_cfg=norm_cfg,
+                    sr_ratio=sr_ratios[i],
+                    use_conv_ffn=use_conv_ffn) for idx in range(num_layer)
+            ])
+            in_channels = embed_dims_i
+            # The ret[0] of build_norm_layer is norm name.
+            if norm_after_stage:
+                norm = build_norm_layer(norm_cfg, embed_dims_i)[1]
+            else:
+                norm = nn.Identity()
+            self.layers.append(ModuleList([patch_embed, layers, norm]))
+            cur += num_layer
+
+    def init_weights(self):
+        logger = get_root_logger()
+        if self.init_cfg is None:
+            logger.warn(f'No pre-trained weights for '
+                        f'{self.__class__.__name__}, '
+                        f'training start from scratch')
+            for m in self.modules():
+                if isinstance(m, nn.Linear):
+                    trunc_normal_init(m.weight, std=.02)
+                    if m.bias is not None:
+                        constant_init(m.bias, 0)
+                elif isinstance(m, nn.LayerNorm):
+                    constant_init(m.bias, 0)
+                    constant_init(m.weight, 1.0)
+                elif isinstance(m, nn.Conv2d):
+                    fan_out = m.kernel_size[0] * m.kernel_size[
+                        1] * m.out_channels
+                    fan_out //= m.groups
+                    normal_init(m.weight, 0, math.sqrt(2.0 / fan_out))
+                    if m.bias is not None:
+                        constant_init(m.bias, 0)
+                elif isinstance(m, AbsolutePositionEmbedding):
+                    m.init_weights()
+        else:
+            assert 'checkpoint' in self.init_cfg, f'Only support ' \
+                                                  f'specify `Pretrained` in ' \
+                                                  f'`init_cfg` in ' \
+                                                  f'{self.__class__.__name__} '
+            checkpoint = _load_checkpoint(
+                self.init_cfg.checkpoint, logger=logger, map_location='cpu')
+            logger.warn(f'Load pre-trained model for '
+                        f'{self.__class__.__name__} from original repo')
+            if 'state_dict' in checkpoint:
+                state_dict = checkpoint['state_dict']
+            elif 'model' in checkpoint:
+                state_dict = checkpoint['model']
+            else:
+                state_dict = checkpoint
+            if self.convert_weights:
+                # Because pvt backbones are not supported by mmcls,
+                # so we need to convert pre-trained weights to match this
+                # implementation.
+                state_dict = pvt_convert(state_dict)
+            load_state_dict(self, state_dict, strict=False, logger=logger)
+
+    def forward(self, x):
+        outs = []
+
+        for i, layer in enumerate(self.layers):
+            x, hw_shape = layer[0](x)
+
+            for block in layer[1]:
+                x = block(x, hw_shape)
+            x = layer[2](x)
+            x = nlc_to_nchw(x, hw_shape)
+            if i in self.out_indices:
+                outs.append(x)
+
+        return outs
+
+
+@BACKBONES.register_module()
+class PyramidVisionTransformerV2(PyramidVisionTransformer):
+    """Implementation of `PVTv2: Improved Baselines with Pyramid Vision
+    Transformer <https://arxiv.org/pdf/2106.13797.pdf>`_."""
+
+    def __init__(self, **kwargs):
+        super(PyramidVisionTransformerV2, self).__init__(
+            patch_sizes=[7, 3, 3, 3],
+            paddings=[3, 1, 1, 1],
+            use_abs_pos_embed=False,
+            norm_after_stage=True,
+            use_conv_ffn=True,
+            **kwargs)
diff --git a/mmdet/models/backbones/regnet.py b/mmdet/models/backbones/regnet.py
index 024d4323..63adc3c1 100644
--- a/mmdet/models/backbones/regnet.py
+++ b/mmdet/models/backbones/regnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import numpy as np
@@ -171,7 +172,7 @@ class RegNet(ResNet):
 
         block_init_cfg = None
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is deprecated, '
                           'please use "init_cfg" instead')
diff --git a/mmdet/models/backbones/res2net.py b/mmdet/models/backbones/res2net.py
index 84951f00..96afb2fb 100644
--- a/mmdet/models/backbones/res2net.py
+++ b/mmdet/models/backbones/res2net.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch
diff --git a/mmdet/models/backbones/resnest.py b/mmdet/models/backbones/resnest.py
index 0fd65aeb..69629b96 100644
--- a/mmdet/models/backbones/resnest.py
+++ b/mmdet/models/backbones/resnest.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch
diff --git a/mmdet/models/backbones/resnet.py b/mmdet/models/backbones/resnet.py
index a61fa488..c424448e 100644
--- a/mmdet/models/backbones/resnet.py
+++ b/mmdet/models/backbones/resnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch.nn as nn
@@ -395,7 +396,7 @@ class ResNet(BaseModule):
 
         block_init_cfg = None
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is deprecated, '
                           'please use "init_cfg" instead')
diff --git a/mmdet/models/backbones/resnext.py b/mmdet/models/backbones/resnext.py
index 6dbcbd51..8675d7c1 100644
--- a/mmdet/models/backbones/resnext.py
+++ b/mmdet/models/backbones/resnext.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 from mmcv.cnn import build_conv_layer, build_norm_layer
diff --git a/mmdet/models/backbones/ssd_vgg.py b/mmdet/models/backbones/ssd_vgg.py
index e8a6689a..c15aeac0 100644
--- a/mmdet/models/backbones/ssd_vgg.py
+++ b/mmdet/models/backbones/ssd_vgg.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch.nn as nn
@@ -78,7 +79,7 @@ class SSDVGG(VGG, BaseModule):
         self.out_feature_indices = out_feature_indices
 
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
 
         if init_cfg is not None:
             self.init_cfg = init_cfg
diff --git a/mmdet/models/backbones/swin.py b/mmdet/models/backbones/swin.py
new file mode 100644
index 00000000..fce01664
--- /dev/null
+++ b/mmdet/models/backbones/swin.py
@@ -0,0 +1,764 @@
+import warnings
+from collections import OrderedDict
+from copy import deepcopy
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.utils.checkpoint as cp
+from mmcv.cnn import build_norm_layer, constant_init, trunc_normal_init
+from mmcv.cnn.bricks.transformer import FFN, build_dropout
+from mmcv.runner import BaseModule, ModuleList, _load_checkpoint
+from mmcv.utils import to_2tuple
+
+from ...utils import get_root_logger
+from ..builder import BACKBONES
+from ..utils.ckpt_convert import swin_converter
+from ..utils.transformer import PatchEmbed, PatchMerging
+
+
+class WindowMSA(BaseModule):
+    """Window based multi-head self-attention (W-MSA) module with relative
+    position bias.
+
+    Args:
+        embed_dims (int): Number of input channels.
+        num_heads (int): Number of attention heads.
+        window_size (tuple[int]): The height and width of the window.
+        qkv_bias (bool, optional):  If True, add a learnable bias to q, k, v.
+            Default: True.
+        qk_scale (float | None, optional): Override default qk scale of
+            head_dim ** -0.5 if set. Default: None.
+        attn_drop_rate (float, optional): Dropout ratio of attention weight.
+            Default: 0.0
+        proj_drop_rate (float, optional): Dropout ratio of output. Default: 0.
+        init_cfg (dict | None, optional): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self,
+                 embed_dims,
+                 num_heads,
+                 window_size,
+                 qkv_bias=True,
+                 qk_scale=None,
+                 attn_drop_rate=0.,
+                 proj_drop_rate=0.,
+                 init_cfg=None):
+
+        super().__init__()
+        self.embed_dims = embed_dims
+        self.window_size = window_size  # Wh, Ww
+        self.num_heads = num_heads
+        head_embed_dims = embed_dims // num_heads
+        self.scale = qk_scale or head_embed_dims**-0.5
+        self.init_cfg = init_cfg
+
+        # define a parameter table of relative position bias
+        self.relative_position_bias_table = nn.Parameter(
+            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1),
+                        num_heads))  # 2*Wh-1 * 2*Ww-1, nH
+
+        # About 2x faster than original impl
+        Wh, Ww = self.window_size
+        rel_index_coords = self.double_step_seq(2 * Ww - 1, Wh, 1, Ww)
+        rel_position_index = rel_index_coords + rel_index_coords.T
+        rel_position_index = rel_position_index.flip(1).contiguous()
+        self.register_buffer('relative_position_index', rel_position_index)
+
+        self.qkv = nn.Linear(embed_dims, embed_dims * 3, bias=qkv_bias)
+        self.attn_drop = nn.Dropout(attn_drop_rate)
+        self.proj = nn.Linear(embed_dims, embed_dims)
+        self.proj_drop = nn.Dropout(proj_drop_rate)
+
+        self.softmax = nn.Softmax(dim=-1)
+
+    def init_weights(self):
+        trunc_normal_init(self.relative_position_bias_table, std=0.02)
+
+    def forward(self, x, mask=None):
+        """
+        Args:
+
+            x (tensor): input features with shape of (num_windows*B, N, C)
+            mask (tensor | None, Optional): mask with shape of (num_windows,
+                Wh*Ww, Wh*Ww), value should be between (-inf, 0].
+        """
+        B, N, C = x.shape
+        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads,
+                                  C // self.num_heads).permute(2, 0, 3, 1, 4)
+        # make torchscript happy (cannot use tensor as tuple)
+        q, k, v = qkv[0], qkv[1], qkv[2]
+
+        q = q * self.scale
+        attn = (q @ k.transpose(-2, -1))
+
+        relative_position_bias = self.relative_position_bias_table[
+            self.relative_position_index.view(-1)].view(
+                self.window_size[0] * self.window_size[1],
+                self.window_size[0] * self.window_size[1],
+                -1)  # Wh*Ww,Wh*Ww,nH
+        relative_position_bias = relative_position_bias.permute(
+            2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
+        attn = attn + relative_position_bias.unsqueeze(0)
+
+        if mask is not None:
+            nW = mask.shape[0]
+            attn = attn.view(B // nW, nW, self.num_heads, N,
+                             N) + mask.unsqueeze(1).unsqueeze(0)
+            attn = attn.view(-1, self.num_heads, N, N)
+        attn = self.softmax(attn)
+
+        attn = self.attn_drop(attn)
+
+        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
+        x = self.proj(x)
+        x = self.proj_drop(x)
+        return x
+
+    @staticmethod
+    def double_step_seq(step1, len1, step2, len2):
+        seq1 = torch.arange(0, step1 * len1, step1)
+        seq2 = torch.arange(0, step2 * len2, step2)
+        return (seq1[:, None] + seq2[None, :]).reshape(1, -1)
+
+
+class ShiftWindowMSA(BaseModule):
+    """Shifted Window Multihead Self-Attention Module.
+
+    Args:
+        embed_dims (int): Number of input channels.
+        num_heads (int): Number of attention heads.
+        window_size (int): The height and width of the window.
+        shift_size (int, optional): The shift step of each window towards
+            right-bottom. If zero, act as regular window-msa. Defaults to 0.
+        qkv_bias (bool, optional): If True, add a learnable bias to q, k, v.
+            Default: True
+        qk_scale (float | None, optional): Override default qk scale of
+            head_dim ** -0.5 if set. Defaults: None.
+        attn_drop_rate (float, optional): Dropout ratio of attention weight.
+            Defaults: 0.
+        proj_drop_rate (float, optional): Dropout ratio of output.
+            Defaults: 0.
+        dropout_layer (dict, optional): The dropout_layer used before output.
+            Defaults: dict(type='DropPath', drop_prob=0.).
+        init_cfg (dict, optional): The extra config for initialization.
+            Default: None.
+    """
+
+    def __init__(self,
+                 embed_dims,
+                 num_heads,
+                 window_size,
+                 shift_size=0,
+                 qkv_bias=True,
+                 qk_scale=None,
+                 attn_drop_rate=0,
+                 proj_drop_rate=0,
+                 dropout_layer=dict(type='DropPath', drop_prob=0.),
+                 init_cfg=None):
+        super().__init__(init_cfg)
+
+        self.window_size = window_size
+        self.shift_size = shift_size
+        assert 0 <= self.shift_size < self.window_size
+
+        self.w_msa = WindowMSA(
+            embed_dims=embed_dims,
+            num_heads=num_heads,
+            window_size=to_2tuple(window_size),
+            qkv_bias=qkv_bias,
+            qk_scale=qk_scale,
+            attn_drop_rate=attn_drop_rate,
+            proj_drop_rate=proj_drop_rate,
+            init_cfg=None)
+
+        self.drop = build_dropout(dropout_layer)
+
+    def forward(self, query, hw_shape):
+        B, L, C = query.shape
+        H, W = hw_shape
+        assert L == H * W, 'input feature has wrong size'
+        query = query.view(B, H, W, C)
+
+        # pad feature maps to multiples of window size
+        pad_r = (self.window_size - W % self.window_size) % self.window_size
+        pad_b = (self.window_size - H % self.window_size) % self.window_size
+        query = F.pad(query, (0, 0, 0, pad_r, 0, pad_b))
+        H_pad, W_pad = query.shape[1], query.shape[2]
+
+        # cyclic shift
+        if self.shift_size > 0:
+            shifted_query = torch.roll(
+                query,
+                shifts=(-self.shift_size, -self.shift_size),
+                dims=(1, 2))
+
+            # calculate attention mask for SW-MSA
+            img_mask = torch.zeros((1, H_pad, W_pad, 1), device=query.device)
+            h_slices = (slice(0, -self.window_size),
+                        slice(-self.window_size,
+                              -self.shift_size), slice(-self.shift_size, None))
+            w_slices = (slice(0, -self.window_size),
+                        slice(-self.window_size,
+                              -self.shift_size), slice(-self.shift_size, None))
+            cnt = 0
+            for h in h_slices:
+                for w in w_slices:
+                    img_mask[:, h, w, :] = cnt
+                    cnt += 1
+
+            # nW, window_size, window_size, 1
+            mask_windows = self.window_partition(img_mask)
+            mask_windows = mask_windows.view(
+                -1, self.window_size * self.window_size)
+            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
+            attn_mask = attn_mask.masked_fill(attn_mask != 0,
+                                              float(-100.0)).masked_fill(
+                                                  attn_mask == 0, float(0.0))
+        else:
+            shifted_query = query
+            attn_mask = None
+
+        # nW*B, window_size, window_size, C
+        query_windows = self.window_partition(shifted_query)
+        # nW*B, window_size*window_size, C
+        query_windows = query_windows.view(-1, self.window_size**2, C)
+
+        # W-MSA/SW-MSA (nW*B, window_size*window_size, C)
+        attn_windows = self.w_msa(query_windows, mask=attn_mask)
+
+        # merge windows
+        attn_windows = attn_windows.view(-1, self.window_size,
+                                         self.window_size, C)
+
+        # B H' W' C
+        shifted_x = self.window_reverse(attn_windows, H_pad, W_pad)
+        # reverse cyclic shift
+        if self.shift_size > 0:
+            x = torch.roll(
+                shifted_x,
+                shifts=(self.shift_size, self.shift_size),
+                dims=(1, 2))
+        else:
+            x = shifted_x
+
+        if pad_r > 0 or pad_b:
+            x = x[:, :H, :W, :].contiguous()
+
+        x = x.view(B, H * W, C)
+
+        x = self.drop(x)
+        return x
+
+    def window_reverse(self, windows, H, W):
+        """
+        Args:
+            windows: (num_windows*B, window_size, window_size, C)
+            H (int): Height of image
+            W (int): Width of image
+        Returns:
+            x: (B, H, W, C)
+        """
+        window_size = self.window_size
+        B = int(windows.shape[0] / (H * W / window_size / window_size))
+        x = windows.view(B, H // window_size, W // window_size, window_size,
+                         window_size, -1)
+        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
+        return x
+
+    def window_partition(self, x):
+        """
+        Args:
+            x: (B, H, W, C)
+        Returns:
+            windows: (num_windows*B, window_size, window_size, C)
+        """
+        B, H, W, C = x.shape
+        window_size = self.window_size
+        x = x.view(B, H // window_size, window_size, W // window_size,
+                   window_size, C)
+        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()
+        windows = windows.view(-1, window_size, window_size, C)
+        return windows
+
+
+class SwinBlock(BaseModule):
+    """"
+    Args:
+        embed_dims (int): The feature dimension.
+        num_heads (int): Parallel attention heads.
+        feedforward_channels (int): The hidden dimension for FFNs.
+        window_size (int, optional): The local window scale. Default: 7.
+        shift (bool, optional): whether to shift window or not. Default False.
+        qkv_bias (bool, optional): enable bias for qkv if True. Default: True.
+        qk_scale (float | None, optional): Override default qk scale of
+            head_dim ** -0.5 if set. Default: None.
+        drop_rate (float, optional): Dropout rate. Default: 0.
+        attn_drop_rate (float, optional): Attention dropout rate. Default: 0.
+        drop_path_rate (float, optional): Stochastic depth rate. Default: 0.
+        act_cfg (dict, optional): The config dict of activation function.
+            Default: dict(type='GELU').
+        norm_cfg (dict, optional): The config dict of normalization.
+            Default: dict(type='LN').
+        with_cp (bool, optional): Use checkpoint or not. Using checkpoint
+            will save some memory while slowing down the training speed.
+            Default: False.
+        init_cfg (dict | list | None, optional): The init config.
+            Default: None.
+    """
+
+    def __init__(self,
+                 embed_dims,
+                 num_heads,
+                 feedforward_channels,
+                 window_size=7,
+                 shift=False,
+                 qkv_bias=True,
+                 qk_scale=None,
+                 drop_rate=0.,
+                 attn_drop_rate=0.,
+                 drop_path_rate=0.,
+                 act_cfg=dict(type='GELU'),
+                 norm_cfg=dict(type='LN'),
+                 with_cp=False,
+                 init_cfg=None):
+
+        super(SwinBlock, self).__init__()
+
+        self.init_cfg = init_cfg
+        self.with_cp = with_cp
+
+        self.norm1 = build_norm_layer(norm_cfg, embed_dims)[1]
+        self.attn = ShiftWindowMSA(
+            embed_dims=embed_dims,
+            num_heads=num_heads,
+            window_size=window_size,
+            shift_size=window_size // 2 if shift else 0,
+            qkv_bias=qkv_bias,
+            qk_scale=qk_scale,
+            attn_drop_rate=attn_drop_rate,
+            proj_drop_rate=drop_rate,
+            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),
+            init_cfg=None)
+
+        self.norm2 = build_norm_layer(norm_cfg, embed_dims)[1]
+        self.ffn = FFN(
+            embed_dims=embed_dims,
+            feedforward_channels=feedforward_channels,
+            num_fcs=2,
+            ffn_drop=drop_rate,
+            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),
+            act_cfg=act_cfg,
+            add_identity=True,
+            init_cfg=None)
+
+    def forward(self, x, hw_shape):
+
+        def _inner_forward(x):
+            identity = x
+            x = self.norm1(x)
+            x = self.attn(x, hw_shape)
+
+            x = x + identity
+
+            identity = x
+            x = self.norm2(x)
+            x = self.ffn(x, identity=identity)
+
+            return x
+
+        if self.with_cp and x.requires_grad:
+            x = cp.checkpoint(_inner_forward, x)
+        else:
+            x = _inner_forward(x)
+
+        return x
+
+
+class SwinBlockSequence(BaseModule):
+    """Implements one stage in Swin Transformer.
+
+    Args:
+        embed_dims (int): The feature dimension.
+        num_heads (int): Parallel attention heads.
+        feedforward_channels (int): The hidden dimension for FFNs.
+        depth (int): The number of blocks in this stage.
+        window_size (int, optional): The local window scale. Default: 7.
+        qkv_bias (bool, optional): enable bias for qkv if True. Default: True.
+        qk_scale (float | None, optional): Override default qk scale of
+            head_dim ** -0.5 if set. Default: None.
+        drop_rate (float, optional): Dropout rate. Default: 0.
+        attn_drop_rate (float, optional): Attention dropout rate. Default: 0.
+        drop_path_rate (float | list[float], optional): Stochastic depth
+            rate. Default: 0.
+        downsample (BaseModule | None, optional): The downsample operation
+            module. Default: None.
+        act_cfg (dict, optional): The config dict of activation function.
+            Default: dict(type='GELU').
+        norm_cfg (dict, optional): The config dict of normalization.
+            Default: dict(type='LN').
+        with_cp (bool, optional): Use checkpoint or not. Using checkpoint
+            will save some memory while slowing down the training speed.
+            Default: False.
+        init_cfg (dict | list | None, optional): The init config.
+            Default: None.
+    """
+
+    def __init__(self,
+                 embed_dims,
+                 num_heads,
+                 feedforward_channels,
+                 depth,
+                 window_size=7,
+                 qkv_bias=True,
+                 qk_scale=None,
+                 drop_rate=0.,
+                 attn_drop_rate=0.,
+                 drop_path_rate=0.,
+                 downsample=None,
+                 act_cfg=dict(type='GELU'),
+                 norm_cfg=dict(type='LN'),
+                 with_cp=False,
+                 init_cfg=None):
+        super().__init__(init_cfg=init_cfg)
+
+        if isinstance(drop_path_rate, list):
+            drop_path_rates = drop_path_rate
+            assert len(drop_path_rates) == depth
+        else:
+            drop_path_rates = [deepcopy(drop_path_rate) for _ in range(depth)]
+
+        self.blocks = ModuleList()
+        for i in range(depth):
+            block = SwinBlock(
+                embed_dims=embed_dims,
+                num_heads=num_heads,
+                feedforward_channels=feedforward_channels,
+                window_size=window_size,
+                shift=False if i % 2 == 0 else True,
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop_rate=drop_rate,
+                attn_drop_rate=attn_drop_rate,
+                drop_path_rate=drop_path_rates[i],
+                act_cfg=act_cfg,
+                norm_cfg=norm_cfg,
+                with_cp=with_cp,
+                init_cfg=None)
+            self.blocks.append(block)
+
+        self.downsample = downsample
+
+    def forward(self, x, hw_shape):
+        for block in self.blocks:
+            x = block(x, hw_shape)
+
+        if self.downsample:
+            x_down, down_hw_shape = self.downsample(x, hw_shape)
+            return x_down, down_hw_shape, x, hw_shape
+        else:
+            return x, hw_shape, x, hw_shape
+
+
+@BACKBONES.register_module()
+class SwinTransformer(BaseModule):
+    """ Swin Transformer
+    A PyTorch implement of : `Swin Transformer:
+    Hierarchical Vision Transformer using Shifted Windows`  -
+        https://arxiv.org/abs/2103.14030
+
+    Inspiration from
+    https://github.com/microsoft/Swin-Transformer
+
+    Args:
+        pretrain_img_size (int | tuple[int]): The size of input image when
+            pretrain. Defaults: 224.
+        in_channels (int): The num of input channels.
+            Defaults: 3.
+        embed_dims (int): The feature dimension. Default: 96.
+        patch_size (int | tuple[int]): Patch size. Default: 4.
+        window_size (int): Window size. Default: 7.
+        mlp_ratio (int): Ratio of mlp hidden dim to embedding dim.
+            Default: 4.
+        depths (tuple[int]): Depths of each Swin Transformer stage.
+            Default: (2, 2, 6, 2).
+        num_heads (tuple[int]): Parallel attention heads of each Swin
+            Transformer stage. Default: (3, 6, 12, 24).
+        strides (tuple[int]): The patch merging or patch embedding stride of
+            each Swin Transformer stage. (In swin, we set kernel size equal to
+            stride.) Default: (4, 2, 2, 2).
+        out_indices (tuple[int]): Output from which stages.
+            Default: (0, 1, 2, 3).
+        qkv_bias (bool, optional): If True, add a learnable bias to query, key,
+            value. Default: True
+        qk_scale (float | None, optional): Override default qk scale of
+            head_dim ** -0.5 if set. Default: None.
+        patch_norm (bool): If add a norm layer for patch embed and patch
+            merging. Default: True.
+        drop_rate (float): Dropout rate. Defaults: 0.
+        attn_drop_rate (float): Attention dropout rate. Default: 0.
+        drop_path_rate (float): Stochastic depth rate. Defaults: 0.1.
+        use_abs_pos_embed (bool): If True, add absolute position embedding to
+            the patch embedding. Defaults: False.
+        act_cfg (dict): Config dict for activation layer.
+            Default: dict(type='LN').
+        norm_cfg (dict): Config dict for normalization layer at
+            output of backone. Defaults: dict(type='LN').
+        with_cp (bool, optional): Use checkpoint or not. Using checkpoint
+            will save some memory while slowing down the training speed.
+            Default: False.
+        pretrained (str, optional): model pretrained path. Default: None.
+        convert_weights (bool): The flag indicates whether the
+            pre-trained model is from the original repo. We may need
+            to convert some keys to make it compatible.
+            Default: False.
+        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).
+            -1 means not freezing any parameters.
+        init_cfg (dict, optional): The Config for initialization.
+            Defaults to None.
+    """
+
+    def __init__(self,
+                 pretrain_img_size=224,
+                 in_channels=3,
+                 embed_dims=96,
+                 patch_size=4,
+                 window_size=7,
+                 mlp_ratio=4,
+                 depths=(2, 2, 6, 2),
+                 num_heads=(3, 6, 12, 24),
+                 strides=(4, 2, 2, 2),
+                 out_indices=(0, 1, 2, 3),
+                 qkv_bias=True,
+                 qk_scale=None,
+                 patch_norm=True,
+                 drop_rate=0.,
+                 attn_drop_rate=0.,
+                 drop_path_rate=0.1,
+                 use_abs_pos_embed=False,
+                 act_cfg=dict(type='GELU'),
+                 norm_cfg=dict(type='LN'),
+                 with_cp=False,
+                 pretrained=None,
+                 convert_weights=False,
+                 frozen_stages=-1,
+                 init_cfg=None):
+        self.convert_weights = convert_weights
+        self.frozen_stages = frozen_stages
+        if isinstance(pretrain_img_size, int):
+            pretrain_img_size = to_2tuple(pretrain_img_size)
+        elif isinstance(pretrain_img_size, tuple):
+            if len(pretrain_img_size) == 1:
+                pretrain_img_size = to_2tuple(pretrain_img_size[0])
+            assert len(pretrain_img_size) == 2, \
+                f'The size of image should have length 1 or 2, ' \
+                f'but got {len(pretrain_img_size)}'
+
+        assert not (init_cfg and pretrained), \
+            'init_cfg and pretrained cannot be specified at the same time'
+        if isinstance(pretrained, str):
+            warnings.warn('DeprecationWarning: pretrained is deprecated, '
+                          'please use "init_cfg" instead')
+            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
+        elif pretrained is None:
+            self.init_cfg = init_cfg
+        else:
+            raise TypeError('pretrained must be a str or None')
+
+        super(SwinTransformer, self).__init__(init_cfg=init_cfg)
+
+        num_layers = len(depths)
+        self.out_indices = out_indices
+        self.use_abs_pos_embed = use_abs_pos_embed
+
+        assert strides[0] == patch_size, 'Use non-overlapping patch embed.'
+
+        self.patch_embed = PatchEmbed(
+            in_channels=in_channels,
+            embed_dims=embed_dims,
+            conv_type='Conv2d',
+            kernel_size=patch_size,
+            stride=strides[0],
+            norm_cfg=norm_cfg if patch_norm else None,
+            init_cfg=None)
+
+        if self.use_abs_pos_embed:
+            patch_row = pretrain_img_size[0] // patch_size
+            patch_col = pretrain_img_size[1] // patch_size
+            num_patches = patch_row * patch_col
+            self.absolute_pos_embed = nn.Parameter(
+                torch.zeros((1, num_patches, embed_dims)))
+
+        self.drop_after_pos = nn.Dropout(p=drop_rate)
+
+        # set stochastic depth decay rule
+        total_depth = sum(depths)
+        dpr = [
+            x.item() for x in torch.linspace(0, drop_path_rate, total_depth)
+        ]
+
+        self.stages = ModuleList()
+        in_channels = embed_dims
+        for i in range(num_layers):
+            if i < num_layers - 1:
+                downsample = PatchMerging(
+                    in_channels=in_channels,
+                    out_channels=2 * in_channels,
+                    stride=strides[i + 1],
+                    norm_cfg=norm_cfg if patch_norm else None,
+                    init_cfg=None)
+            else:
+                downsample = None
+
+            stage = SwinBlockSequence(
+                embed_dims=in_channels,
+                num_heads=num_heads[i],
+                feedforward_channels=mlp_ratio * in_channels,
+                depth=depths[i],
+                window_size=window_size,
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop_rate=drop_rate,
+                attn_drop_rate=attn_drop_rate,
+                drop_path_rate=dpr[sum(depths[:i]):sum(depths[:i + 1])],
+                downsample=downsample,
+                act_cfg=act_cfg,
+                norm_cfg=norm_cfg,
+                with_cp=with_cp,
+                init_cfg=None)
+            self.stages.append(stage)
+            if downsample:
+                in_channels = downsample.out_channels
+
+        self.num_features = [int(embed_dims * 2**i) for i in range(num_layers)]
+        # Add a norm layer for each output
+        for i in out_indices:
+            layer = build_norm_layer(norm_cfg, self.num_features[i])[1]
+            layer_name = f'norm{i}'
+            self.add_module(layer_name, layer)
+
+    def train(self, mode=True):
+        """Convert the model into training mode while keep layers freezed."""
+        super(SwinTransformer, self).train(mode)
+        self._freeze_stages()
+
+    def _freeze_stages(self):
+        if self.frozen_stages >= 0:
+            self.patch_embed.eval()
+            for param in self.patch_embed.parameters():
+                param.requires_grad = False
+            if self.use_abs_pos_embed:
+                self.absolute_pos_embed.requires_grad = False
+            self.drop_after_pos.eval()
+
+        for i in range(1, self.frozen_stages + 1):
+
+            if (i - 1) in self.out_indices:
+                norm_layer = getattr(self, f'norm{i-1}')
+                norm_layer.eval()
+                for param in norm_layer.parameters():
+                    param.requires_grad = False
+
+            m = self.stages[i - 1]
+            m.eval()
+            for param in m.parameters():
+                param.requires_grad = False
+
+    def init_weights(self):
+        logger = get_root_logger()
+        if self.init_cfg is None:
+            logger.warn(f'No pre-trained weights for '
+                        f'{self.__class__.__name__}, '
+                        f'training start from scratch')
+            if self.use_abs_pos_embed:
+                trunc_normal_init(self.absolute_pos_embed, std=0.02)
+            for m in self.modules():
+                if isinstance(m, nn.Linear):
+                    trunc_normal_init(m.weight, std=.02)
+                    if m.bias is not None:
+                        constant_init(m.bias, 0)
+                elif isinstance(m, nn.LayerNorm):
+                    constant_init(m.bias, 0)
+                    constant_init(m.weight, 1.0)
+        else:
+            assert 'checkpoint' in self.init_cfg, f'Only support ' \
+                                                  f'specify `Pretrained` in ' \
+                                                  f'`init_cfg` in ' \
+                                                  f'{self.__class__.__name__} '
+            ckpt = _load_checkpoint(
+                self.init_cfg.checkpoint, logger=logger, map_location='cpu')
+            if 'state_dict' in ckpt:
+                _state_dict = ckpt['state_dict']
+            elif 'model' in ckpt:
+                _state_dict = ckpt['model']
+            else:
+                _state_dict = ckpt
+            if self.convert_weights:
+                # supported loading weight from original repo,
+                _state_dict = swin_converter(_state_dict)
+
+            state_dict = OrderedDict()
+            for k, v in _state_dict.items():
+                if k.startswith('backbone.'):
+                    state_dict[k[9:]] = v
+
+            # strip prefix of state_dict
+            if list(state_dict.keys())[0].startswith('module.'):
+                state_dict = {k[7:]: v for k, v in state_dict.items()}
+
+            # reshape absolute position embedding
+            if state_dict.get('absolute_pos_embed') is not None:
+                absolute_pos_embed = state_dict['absolute_pos_embed']
+                N1, L, C1 = absolute_pos_embed.size()
+                N2, C2, H, W = self.absolute_pos_embed.size()
+                if N1 != N2 or C1 != C2 or L != H * W:
+                    logger.warning('Error in loading absolute_pos_embed, pass')
+                else:
+                    state_dict['absolute_pos_embed'] = absolute_pos_embed.view(
+                        N2, H, W, C2).permute(0, 3, 1, 2).contiguous()
+
+            # interpolate position bias table if needed
+            relative_position_bias_table_keys = [
+                k for k in state_dict.keys()
+                if 'relative_position_bias_table' in k
+            ]
+            for table_key in relative_position_bias_table_keys:
+                table_pretrained = state_dict[table_key]
+                table_current = self.state_dict()[table_key]
+                L1, nH1 = table_pretrained.size()
+                L2, nH2 = table_current.size()
+                if nH1 != nH2:
+                    logger.warning(f'Error in loading {table_key}, pass')
+                elif L1 != L2:
+                    S1 = int(L1**0.5)
+                    S2 = int(L2**0.5)
+                    table_pretrained_resized = F.interpolate(
+                        table_pretrained.permute(1, 0).reshape(1, nH1, S1, S1),
+                        size=(S2, S2),
+                        mode='bicubic')
+                    state_dict[table_key] = table_pretrained_resized.view(
+                        nH2, L2).permute(1, 0).contiguous()
+
+            # load state_dict
+            self.load_state_dict(state_dict, False)
+
+    def forward(self, x):
+        x, hw_shape = self.patch_embed(x)
+
+        if self.use_abs_pos_embed:
+            x = x + self.absolute_pos_embed
+        x = self.drop_after_pos(x)
+
+        outs = []
+        for i, stage in enumerate(self.stages):
+            x, hw_shape, out, out_hw_shape = stage(x, hw_shape)
+            if i in self.out_indices:
+                norm_layer = getattr(self, f'norm{i}')
+                out = norm_layer(out)
+                out = out.view(-1, *out_hw_shape,
+                               self.num_features[i]).permute(0, 3, 1,
+                                                             2).contiguous()
+                outs.append(out)
+
+        return outs
diff --git a/mmdet/models/backbones/trident_resnet.py b/mmdet/models/backbones/trident_resnet.py
index 44d8c96c..013ba64b 100644
--- a/mmdet/models/backbones/trident_resnet.py
+++ b/mmdet/models/backbones/trident_resnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/builder.py b/mmdet/models/builder.py
index 85dc2562..ace6209f 100644
--- a/mmdet/models/builder.py
+++ b/mmdet/models/builder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 from mmcv.cnn import MODELS as MMCV_MODELS
diff --git a/mmdet/models/dense_heads/__init__.py b/mmdet/models/dense_heads/__init__.py
index e433a931..c3440fed 100644
--- a/mmdet/models/dense_heads/__init__.py
+++ b/mmdet/models/dense_heads/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .anchor_free_head import AnchorFreeHead
 from .anchor_head import AnchorHead
 from .atss_head import ATSSHead
@@ -27,6 +28,7 @@ from .retina_head import RetinaHead
 from .retina_sepbn_head import RetinaSepBNHead
 from .rpn_head import RPNHead
 from .sabl_retina_head import SABLRetinaHead
+from .solo_head import DecoupledSOLOHead, DecoupledSOLOLightHead, SOLOHead
 from .ssd_head import SSDHead
 from .vfnet_head import VFNetHead
 from .yolact_head import YOLACTHead, YOLACTProtonet, YOLACTSegmHead
@@ -44,5 +46,6 @@ __all__ = [
     'SABLRetinaHead', 'CentripetalHead', 'VFNetHead', 'StageCascadeRPNHead',
     'CascadeRPNHead', 'EmbeddingRPNHead', 'LDHead', 'CascadeRPNHead',
     'AutoAssignHead', 'DETRHead', 'YOLOFHead', 'DeformableDETRHead',
-    'CenterNetHead', 'YOLOXHead'
+    'SOLOHead', 'DecoupledSOLOHead', 'CenterNetHead', 'YOLOXHead',
+    'DecoupledSOLOLightHead'
 ]
diff --git a/mmdet/models/dense_heads/anchor_free_head.py b/mmdet/models/dense_heads/anchor_free_head.py
index e399bee9..07dfcb03 100644
--- a/mmdet/models/dense_heads/anchor_free_head.py
+++ b/mmdet/models/dense_heads/anchor_free_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import abstractmethod
 
 import torch
diff --git a/mmdet/models/dense_heads/anchor_head.py b/mmdet/models/dense_heads/anchor_head.py
index 96753ef8..446d42d2 100644
--- a/mmdet/models/dense_heads/anchor_head.py
+++ b/mmdet/models/dense_heads/anchor_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.runner import force_fp32
diff --git a/mmdet/models/dense_heads/atss_head.py b/mmdet/models/dense_heads/atss_head.py
index 17dd3956..bf93f6f2 100644
--- a/mmdet/models/dense_heads/atss_head.py
+++ b/mmdet/models/dense_heads/atss_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.cnn import ConvModule, Scale
diff --git a/mmdet/models/dense_heads/autoassign_head.py b/mmdet/models/dense_heads/autoassign_head.py
index 99953c3f..d6d5d831 100644
--- a/mmdet/models/dense_heads/autoassign_head.py
+++ b/mmdet/models/dense_heads/autoassign_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/dense_heads/base_dense_head.py b/mmdet/models/dense_heads/base_dense_head.py
index 8b1a1417..0a2d0521 100644
--- a/mmdet/models/dense_heads/base_dense_head.py
+++ b/mmdet/models/dense_heads/base_dense_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 
 from mmcv.runner import BaseModule
diff --git a/mmdet/models/dense_heads/base_mask_head.py b/mmdet/models/dense_heads/base_mask_head.py
new file mode 100644
index 00000000..5eb94fb2
--- /dev/null
+++ b/mmdet/models/dense_heads/base_mask_head.py
@@ -0,0 +1,116 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from abc import ABCMeta, abstractmethod
+
+from mmcv.runner import BaseModule
+
+
+class BaseMaskHead(BaseModule, metaclass=ABCMeta):
+    """Base class for mask heads used in One-Stage Instance Segmentation."""
+
+    def __init__(self, init_cfg):
+        super(BaseMaskHead, self).__init__(init_cfg)
+
+    @abstractmethod
+    def loss(self, **kwargs):
+        pass
+
+    @abstractmethod
+    def get_results(self, **kwargs):
+        """Get precessed :obj:`InstanceData` of multiple images."""
+        pass
+
+    def forward_train(self,
+                      x,
+                      gt_labels,
+                      gt_masks,
+                      img_metas,
+                      gt_bboxes=None,
+                      gt_bboxes_ignore=None,
+                      positive_infos=None,
+                      **kwargs):
+        """
+        Args:
+            x (list[Tensor] | tuple[Tensor]): Features from FPN.
+                Each has a shape (B, C, H, W).
+            gt_labels (list[Tensor]): Ground truth labels of all images.
+                each has a shape (num_gts,).
+            gt_masks (list[Tensor]) : Masks for each bbox, has a shape
+                (num_gts, h , w).
+            img_metas (list[dict]): Meta information of each image, e.g.,
+                image size, scaling factor, etc.
+            gt_bboxes (list[Tensor]): Ground truth bboxes of the image,
+                each item has a shape (num_gts, 4).
+            gt_bboxes_ignore (list[Tensor], None): Ground truth bboxes to be
+                ignored, each item has a shape (num_ignored_gts, 4).
+            positive_infos (list[:obj:`InstanceData`], optional): Information
+                of positive samples. Used when the label assignment is
+                done outside the MaskHead, e.g., in BboxHead in
+                YOLACT or CondInst, etc. When the label assignment is done in
+                MaskHead, it would be None, like SOLO. All values
+                in it should have shape (num_positive_samples, *).
+
+        Returns:
+            dict[str, Tensor]: A dictionary of loss components.
+        """
+        if positive_infos is None:
+            outs = self(x)
+        else:
+            outs = self(x, positive_infos)
+
+        assert isinstance(outs, tuple), 'Forward results should be a tuple, ' \
+                                        'even if only one item is returned'
+        loss = self.loss(
+            *outs,
+            gt_labels=gt_labels,
+            gt_masks=gt_masks,
+            img_metas=img_metas,
+            gt_bboxes=gt_bboxes,
+            gt_bboxes_ignore=gt_bboxes_ignore,
+            positive_infos=positive_infos,
+            **kwargs)
+        return loss
+
+    def simple_test(self,
+                    feats,
+                    img_metas,
+                    rescale=False,
+                    instances_list=None,
+                    **kwargs):
+        """Test function without test-time augmentation.
+
+        Args:
+            feats (tuple[torch.Tensor]): Multi-level features from the
+                upstream network, each is a 4D-tensor.
+            img_metas (list[dict]): List of image information.
+            rescale (bool, optional): Whether to rescale the results.
+                Defaults to False.
+            instances_list (list[obj:`InstanceData`], optional): Detection
+                results of each image after the post process. Only exist
+                if there is a `bbox_head`, like `YOLACT`, `CondInst`, etc.
+
+        Returns:
+            list[obj:`InstanceData`]: Instance segmentation \
+                results of each image after the post process. \
+                Each item usually contains following keys. \
+
+                - scores (Tensor): Classification scores, has a shape
+                  (num_instance,)
+                - labels (Tensor): Has a shape (num_instances,).
+                - masks (Tensor): Processed mask results, has a
+                  shape (num_instances, h, w).
+        """
+        if instances_list is None:
+            outs = self(feats)
+        else:
+            outs = self(feats, instances_list=instances_list)
+        mask_inputs = outs + (img_metas, )
+        results_list = self.get_results(
+            *mask_inputs,
+            rescale=rescale,
+            instances_list=instances_list,
+            **kwargs)
+        return results_list
+
+    def onnx_export(self, img, img_metas):
+        raise NotImplementedError(f'{self.__class__.__name__} does '
+                                  f'not support ONNX EXPORT')
diff --git a/mmdet/models/dense_heads/cascade_rpn_head.py b/mmdet/models/dense_heads/cascade_rpn_head.py
index 9dd33f6f..c02be290 100644
--- a/mmdet/models/dense_heads/cascade_rpn_head.py
+++ b/mmdet/models/dense_heads/cascade_rpn_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from __future__ import division
 import copy
 import warnings
diff --git a/mmdet/models/dense_heads/centernet_head.py b/mmdet/models/dense_heads/centernet_head.py
index 8fb07f16..e5b2c2bd 100644
--- a/mmdet/models/dense_heads/centernet_head.py
+++ b/mmdet/models/dense_heads/centernet_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.cnn import bias_init_with_prob, normal_init
@@ -367,7 +368,8 @@ class CenterNetHead(BaseDenseHead, BBoxTestMixin):
         if labels.numel() == 0:
             return bboxes, labels
 
-        out_bboxes, keep = batched_nms(bboxes[:, :4], bboxes[:, -1], labels,
+        out_bboxes, keep = batched_nms(bboxes[:, :4].contiguous(),
+                                       bboxes[:, -1].contiguous(), labels,
                                        cfg.nms_cfg)
         out_labels = labels[keep]
 
diff --git a/mmdet/models/dense_heads/centripetal_head.py b/mmdet/models/dense_heads/centripetal_head.py
index a9d3ddf5..bf231c69 100644
--- a/mmdet/models/dense_heads/centripetal_head.py
+++ b/mmdet/models/dense_heads/centripetal_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule, normal_init
 from mmcv.ops import DeformConv2d
diff --git a/mmdet/models/dense_heads/corner_head.py b/mmdet/models/dense_heads/corner_head.py
index 634bd4a3..7ed6cae8 100644
--- a/mmdet/models/dense_heads/corner_head.py
+++ b/mmdet/models/dense_heads/corner_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from logging import warning
 from math import ceil, log
 
diff --git a/mmdet/models/dense_heads/deformable_detr_head.py b/mmdet/models/dense_heads/deformable_detr_head.py
index a7d4332c..71c27852 100644
--- a/mmdet/models/dense_heads/deformable_detr_head.py
+++ b/mmdet/models/dense_heads/deformable_detr_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import torch
diff --git a/mmdet/models/dense_heads/dense_test_mixins.py b/mmdet/models/dense_heads/dense_test_mixins.py
index 7f136a4a..cb9df1a9 100644
--- a/mmdet/models/dense_heads/dense_test_mixins.py
+++ b/mmdet/models/dense_heads/dense_test_mixins.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import sys
 from inspect import signature
 
diff --git a/mmdet/models/dense_heads/detr_head.py b/mmdet/models/dense_heads/detr_head.py
index 08a2c824..47f60dcc 100644
--- a/mmdet/models/dense_heads/detr_head.py
+++ b/mmdet/models/dense_heads/detr_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/dense_heads/embedding_rpn_head.py b/mmdet/models/dense_heads/embedding_rpn_head.py
index 88d83fe0..22060b96 100644
--- a/mmdet/models/dense_heads/embedding_rpn_head.py
+++ b/mmdet/models/dense_heads/embedding_rpn_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.runner import BaseModule
diff --git a/mmdet/models/dense_heads/fcos_head.py b/mmdet/models/dense_heads/fcos_head.py
index 323d154b..8eedd788 100644
--- a/mmdet/models/dense_heads/fcos_head.py
+++ b/mmdet/models/dense_heads/fcos_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/dense_heads/fovea_head.py b/mmdet/models/dense_heads/fovea_head.py
index 657a8791..5dab829e 100644
--- a/mmdet/models/dense_heads/fovea_head.py
+++ b/mmdet/models/dense_heads/fovea_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.cnn import ConvModule
diff --git a/mmdet/models/dense_heads/free_anchor_retina_head.py b/mmdet/models/dense_heads/free_anchor_retina_head.py
index b7f8aa76..6859ee3f 100644
--- a/mmdet/models/dense_heads/free_anchor_retina_head.py
+++ b/mmdet/models/dense_heads/free_anchor_retina_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn.functional as F
 
diff --git a/mmdet/models/dense_heads/fsaf_head.py b/mmdet/models/dense_heads/fsaf_head.py
index 6aa442da..25f58042 100644
--- a/mmdet/models/dense_heads/fsaf_head.py
+++ b/mmdet/models/dense_heads/fsaf_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 from mmcv.runner import force_fp32
diff --git a/mmdet/models/dense_heads/ga_retina_head.py b/mmdet/models/dense_heads/ga_retina_head.py
index cc83bd51..e61fa91c 100644
--- a/mmdet/models/dense_heads/ga_retina_head.py
+++ b/mmdet/models/dense_heads/ga_retina_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmcv.ops import MaskedConv2d
diff --git a/mmdet/models/dense_heads/ga_rpn_head.py b/mmdet/models/dense_heads/ga_rpn_head.py
index 5a84cc53..4123c8b3 100644
--- a/mmdet/models/dense_heads/ga_rpn_head.py
+++ b/mmdet/models/dense_heads/ga_rpn_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import warnings
 
diff --git a/mmdet/models/dense_heads/gfl_head.py b/mmdet/models/dense_heads/gfl_head.py
index a62cf7a4..e76f5f4c 100644
--- a/mmdet/models/dense_heads/gfl_head.py
+++ b/mmdet/models/dense_heads/gfl_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/dense_heads/guided_anchor_head.py b/mmdet/models/dense_heads/guided_anchor_head.py
index 252e1ea7..6d540a1a 100644
--- a/mmdet/models/dense_heads/guided_anchor_head.py
+++ b/mmdet/models/dense_heads/guided_anchor_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.ops import DeformConv2d, MaskedConv2d
diff --git a/mmdet/models/dense_heads/ld_head.py b/mmdet/models/dense_heads/ld_head.py
index 501e1f7b..7deae7ce 100644
--- a/mmdet/models/dense_heads/ld_head.py
+++ b/mmdet/models/dense_heads/ld_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmcv.runner import force_fp32
 
diff --git a/mmdet/models/dense_heads/nasfcos_head.py b/mmdet/models/dense_heads/nasfcos_head.py
index 086ebf86..380c912c 100644
--- a/mmdet/models/dense_heads/nasfcos_head.py
+++ b/mmdet/models/dense_heads/nasfcos_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import torch.nn as nn
diff --git a/mmdet/models/dense_heads/paa_head.py b/mmdet/models/dense_heads/paa_head.py
index 49b0996d..f007d07b 100644
--- a/mmdet/models/dense_heads/paa_head.py
+++ b/mmdet/models/dense_heads/paa_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 from mmcv.runner import force_fp32
diff --git a/mmdet/models/dense_heads/pisa_retinanet_head.py b/mmdet/models/dense_heads/pisa_retinanet_head.py
index bd87b9ae..b365580b 100644
--- a/mmdet/models/dense_heads/pisa_retinanet_head.py
+++ b/mmdet/models/dense_heads/pisa_retinanet_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmcv.runner import force_fp32
 
diff --git a/mmdet/models/dense_heads/pisa_ssd_head.py b/mmdet/models/dense_heads/pisa_ssd_head.py
index 90ef3c83..610bb3de 100644
--- a/mmdet/models/dense_heads/pisa_ssd_head.py
+++ b/mmdet/models/dense_heads/pisa_ssd_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import multi_apply
diff --git a/mmdet/models/dense_heads/reppoints_head.py b/mmdet/models/dense_heads/reppoints_head.py
index d05476fb..81658616 100644
--- a/mmdet/models/dense_heads/reppoints_head.py
+++ b/mmdet/models/dense_heads/reppoints_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
diff --git a/mmdet/models/dense_heads/retina_head.py b/mmdet/models/dense_heads/retina_head.py
index 698aec50..f5fbee15 100644
--- a/mmdet/models/dense_heads/retina_head.py
+++ b/mmdet/models/dense_heads/retina_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 
diff --git a/mmdet/models/dense_heads/retina_sepbn_head.py b/mmdet/models/dense_heads/retina_sepbn_head.py
index 6fda7fc8..5ef1659c 100644
--- a/mmdet/models/dense_heads/retina_sepbn_head.py
+++ b/mmdet/models/dense_heads/retina_sepbn_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule, bias_init_with_prob, normal_init
 
diff --git a/mmdet/models/dense_heads/rpn_head.py b/mmdet/models/dense_heads/rpn_head.py
index 1ad7d469..3eef10f9 100644
--- a/mmdet/models/dense_heads/rpn_head.py
+++ b/mmdet/models/dense_heads/rpn_head.py
@@ -1,8 +1,10 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+from mmcv.cnn import ConvModule
 from mmcv.ops import batched_nms
 from mmcv.runner import force_fp32
 
@@ -17,19 +19,41 @@ class RPNHead(AnchorHead):
     Args:
         in_channels (int): Number of channels in the input feature map.
         init_cfg (dict or list[dict], optional): Initialization config dict.
+        num_convs (int): Number of convolution layers in the head. Default 1.
     """  # noqa: W605
 
     def __init__(self,
                  in_channels,
                  init_cfg=dict(type='Normal', layer='Conv2d', std=0.01),
+                 num_convs=1,
                  **kwargs):
+        self.num_convs = num_convs
         super(RPNHead, self).__init__(
             1, in_channels, init_cfg=init_cfg, **kwargs)
 
     def _init_layers(self):
         """Initialize layers of the head."""
-        self.rpn_conv = nn.Conv2d(
-            self.in_channels, self.feat_channels, 3, padding=1)
+        if self.num_convs > 1:
+            rpn_convs = []
+            for i in range(self.num_convs):
+                if i == 0:
+                    in_channels = self.in_channels
+                else:
+                    in_channels = self.feat_channels
+                # use ``inplace=False`` to avoid error: one of the variables
+                # needed for gradient computation has been modified by an
+                # inplace operation.
+                rpn_convs.append(
+                    ConvModule(
+                        in_channels,
+                        self.feat_channels,
+                        3,
+                        padding=1,
+                        inplace=False))
+            self.rpn_conv = nn.Sequential(*rpn_convs)
+        else:
+            self.rpn_conv = nn.Conv2d(
+                self.in_channels, self.feat_channels, 3, padding=1)
         self.rpn_cls = nn.Conv2d(self.feat_channels,
                                  self.num_anchors * self.cls_out_channels, 1)
         self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)
diff --git a/mmdet/models/dense_heads/sabl_retina_head.py b/mmdet/models/dense_heads/sabl_retina_head.py
index 6f91c82b..24b85da1 100644
--- a/mmdet/models/dense_heads/sabl_retina_head.py
+++ b/mmdet/models/dense_heads/sabl_retina_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
diff --git a/mmdet/models/dense_heads/solo_head.py b/mmdet/models/dense_heads/solo_head.py
new file mode 100644
index 00000000..148f819f
--- /dev/null
+++ b/mmdet/models/dense_heads/solo_head.py
@@ -0,0 +1,1177 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import mmcv
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from mmcv.cnn import ConvModule
+
+from mmdet.core import InstanceData, mask_matrix_nms, multi_apply
+from mmdet.core.utils import center_of_mass, generate_coordinate
+from mmdet.models.builder import HEADS, build_loss
+from .base_mask_head import BaseMaskHead
+
+
+@HEADS.register_module()
+class SOLOHead(BaseMaskHead):
+    """SOLO mask head used in `SOLO: Segmenting Objects by Locations.
+
+    <https://arxiv.org/abs/1912.04488>`_
+
+    Args:
+        num_classes (int): Number of categories excluding the background
+            category.
+        in_channels (int): Number of channels in the input feature map.
+        feat_channels (int): Number of hidden channels. Used in child classes.
+            Default: 256.
+        stacked_convs (int): Number of stacking convs of the head.
+            Default: 4.
+        strides (tuple): Downsample factor of each feature map.
+        scale_ranges (tuple[tuple[int, int]]): Area range of multiple
+            level masks, in the format [(min1, max1), (min2, max2), ...].
+            A range of (16, 64) means the area range between (16, 64).
+        pos_scale (float): Constant scale factor to control the center region.
+        num_grids (list[int]): Divided image into a uniform grids, each
+            feature map has a different grid value. The number of output
+            channels is grid ** 2. Default: [40, 36, 24, 16, 12].
+        cls_down_index (int): The index of downsample operation in
+            classification branch. Default: 0.
+        loss_mask (dict): Config of mask loss.
+        loss_cls (dict): Config of classification loss.
+        norm_cfg (dict): dictionary to construct and config norm layer.
+            Default: norm_cfg=dict(type='GN', num_groups=32,
+                                   requires_grad=True).
+        train_cfg (dict): Training config of head.
+        test_cfg (dict): Testing config of head.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+    """
+
+    def __init__(
+        self,
+        num_classes,
+        in_channels,
+        feat_channels=256,
+        stacked_convs=4,
+        strides=(4, 8, 16, 32, 64),
+        scale_ranges=((8, 32), (16, 64), (32, 128), (64, 256), (128, 512)),
+        pos_scale=0.2,
+        num_grids=[40, 36, 24, 16, 12],
+        cls_down_index=0,
+        loss_mask=None,
+        loss_cls=None,
+        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True),
+        train_cfg=None,
+        test_cfg=None,
+        init_cfg=[
+            dict(type='Normal', layer='Conv2d', std=0.01),
+            dict(
+                type='Normal',
+                std=0.01,
+                bias_prob=0.01,
+                override=dict(name='conv_mask_list')),
+            dict(
+                type='Normal',
+                std=0.01,
+                bias_prob=0.01,
+                override=dict(name='conv_cls'))
+        ],
+    ):
+        super(SOLOHead, self).__init__(init_cfg)
+        self.num_classes = num_classes
+        self.cls_out_channels = self.num_classes
+        self.in_channels = in_channels
+        self.feat_channels = feat_channels
+        self.stacked_convs = stacked_convs
+        self.strides = strides
+        self.num_grids = num_grids
+        # number of FPN feats
+        self.num_levels = len(strides)
+        assert self.num_levels == len(scale_ranges) == len(num_grids)
+        self.scale_ranges = scale_ranges
+        self.pos_scale = pos_scale
+
+        self.cls_down_index = cls_down_index
+        self.loss_cls = build_loss(loss_cls)
+        self.loss_mask = build_loss(loss_mask)
+        self.norm_cfg = norm_cfg
+        self.init_cfg = init_cfg
+        self.train_cfg = train_cfg
+        self.test_cfg = test_cfg
+        self._init_layers()
+
+    def _init_layers(self):
+        self.mask_convs = nn.ModuleList()
+        self.cls_convs = nn.ModuleList()
+        for i in range(self.stacked_convs):
+            chn = self.in_channels + 2 if i == 0 else self.feat_channels
+            self.mask_convs.append(
+                ConvModule(
+                    chn,
+                    self.feat_channels,
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg))
+            chn = self.in_channels if i == 0 else self.feat_channels
+            self.cls_convs.append(
+                ConvModule(
+                    chn,
+                    self.feat_channels,
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg))
+        self.conv_mask_list = nn.ModuleList()
+        for num_grid in self.num_grids:
+            self.conv_mask_list.append(
+                nn.Conv2d(self.feat_channels, num_grid**2, 1))
+
+        self.conv_cls = nn.Conv2d(
+            self.feat_channels, self.cls_out_channels, 3, padding=1)
+
+    def resize_feats(self, feats):
+        """Downsample the first feat and upsample last feat in feats."""
+        out = []
+        for i in range(len(feats)):
+            if i == 0:
+                out.append(
+                    F.interpolate(feats[0], scale_factor=0.5, mode='bilinear'))
+            elif i == len(feats) - 1:
+                out.append(
+                    F.interpolate(
+                        feats[i],
+                        size=feats[i - 1].shape[-2:],
+                        mode='bilinear'))
+            else:
+                out.append(feats[i])
+        return out
+
+    def forward(self, feats):
+        assert len(feats) == self.num_levels
+        feats = self.resize_feats(feats)
+        mlvl_mask_preds = []
+        mlvl_cls_preds = []
+        for i in range(self.num_levels):
+            x = feats[i]
+            mask_feat = x
+            cls_feat = x
+            # generate and concat the coordinate
+            coord_feat = generate_coordinate(mask_feat.size(),
+                                             mask_feat.device)
+            mask_feat = torch.cat([mask_feat, coord_feat], 1)
+
+            for mask_layer in (self.mask_convs):
+                mask_feat = mask_layer(mask_feat)
+
+            mask_feat = F.interpolate(
+                mask_feat, scale_factor=2, mode='bilinear')
+            mask_pred = self.conv_mask_list[i](mask_feat)
+
+            # cls branch
+            for j, cls_layer in enumerate(self.cls_convs):
+                if j == self.cls_down_index:
+                    num_grid = self.num_grids[i]
+                    cls_feat = F.interpolate(
+                        cls_feat, size=num_grid, mode='bilinear')
+                cls_feat = cls_layer(cls_feat)
+
+            cls_pred = self.conv_cls(cls_feat)
+
+            if not self.training:
+                feat_wh = feats[0].size()[-2:]
+                upsampled_size = (feat_wh[0] * 2, feat_wh[1] * 2)
+                mask_pred = F.interpolate(
+                    mask_pred.sigmoid(), size=upsampled_size, mode='bilinear')
+                cls_pred = cls_pred.sigmoid()
+                # get local maximum
+                local_max = F.max_pool2d(cls_pred, 2, stride=1, padding=1)
+                keep_mask = local_max[:, :, :-1, :-1] == cls_pred
+                cls_pred = cls_pred * keep_mask
+
+            mlvl_mask_preds.append(mask_pred)
+            mlvl_cls_preds.append(cls_pred)
+        return mlvl_mask_preds, mlvl_cls_preds
+
+    def loss(self,
+             mlvl_mask_preds,
+             mlvl_cls_preds,
+             gt_labels,
+             gt_masks,
+             img_metas,
+             gt_bboxes=None,
+             **kwargs):
+        """Calculate the loss of total batch.
+
+        Args:
+            mlvl_mask_preds (list[Tensor]): Multi-level mask prediction.
+                Each element in the list has shape
+                (batch_size, num_grids**2 ,h ,w).
+            mlvl_cls_preds (list[Tensor]): Multi-level scores. Each element
+                in the list has shape
+                (batch_size, num_classes, num_grids ,num_grids).
+            gt_labels (list[Tensor]): Labels of multiple images.
+            gt_masks (list[Tensor]): Ground truth masks of multiple images.
+                Each has shape (num_instances, h, w).
+            img_metas (list[dict]): Meta information of multiple images.
+            gt_bboxes (list[Tensor]): Ground truth bboxes of multiple
+                images. Default: None.
+
+        Returns:
+            dict[str, Tensor]: A dictionary of loss components.
+        """
+        num_levels = self.num_levels
+        num_imgs = len(gt_labels)
+
+        featmap_sizes = [featmap.size()[-2:] for featmap in mlvl_mask_preds]
+
+        # `BoolTensor` in `pos_masks` represent
+        # whether the corresponding point is
+        # positive
+        pos_mask_targets, labels, pos_masks = multi_apply(
+            self._get_targets_single,
+            gt_bboxes,
+            gt_labels,
+            gt_masks,
+            featmap_sizes=featmap_sizes)
+
+        # change from the outside list meaning multi images
+        # to the outside list meaning multi levels
+        mlvl_pos_mask_targets = [[] for _ in range(num_levels)]
+        mlvl_pos_mask_preds = [[] for _ in range(num_levels)]
+        mlvl_pos_masks = [[] for _ in range(num_levels)]
+        mlvl_labels = [[] for _ in range(num_levels)]
+        for img_id in range(num_imgs):
+            assert num_levels == len(pos_mask_targets[img_id])
+            for lvl in range(num_levels):
+                mlvl_pos_mask_targets[lvl].append(
+                    pos_mask_targets[img_id][lvl])
+                mlvl_pos_mask_preds[lvl].append(
+                    mlvl_mask_preds[lvl][img_id, pos_masks[img_id][lvl], ...])
+                mlvl_pos_masks[lvl].append(pos_masks[img_id][lvl].flatten())
+                mlvl_labels[lvl].append(labels[img_id][lvl].flatten())
+
+        # cat multiple image
+        temp_mlvl_cls_preds = []
+        for lvl in range(num_levels):
+            mlvl_pos_mask_targets[lvl] = torch.cat(
+                mlvl_pos_mask_targets[lvl], dim=0)
+            mlvl_pos_mask_preds[lvl] = torch.cat(
+                mlvl_pos_mask_preds[lvl], dim=0)
+            mlvl_pos_masks[lvl] = torch.cat(mlvl_pos_masks[lvl], dim=0)
+            mlvl_labels[lvl] = torch.cat(mlvl_labels[lvl], dim=0)
+            temp_mlvl_cls_preds.append(mlvl_cls_preds[lvl].permute(
+                0, 2, 3, 1).reshape(-1, self.cls_out_channels))
+
+        num_pos = sum(item.sum() for item in mlvl_pos_masks)
+        # dice loss
+        loss_mask = []
+        for pred, target in zip(mlvl_pos_mask_preds, mlvl_pos_mask_targets):
+            if pred.size()[0] == 0:
+                loss_mask.append(pred.sum().unsqueeze(0))
+                continue
+            loss_mask.append(
+                self.loss_mask(pred, target, reduction_override='none'))
+        if num_pos > 0:
+            loss_mask = torch.cat(loss_mask).sum() / num_pos
+        else:
+            loss_mask = torch.cat(loss_mask).mean()
+
+        flatten_labels = torch.cat(mlvl_labels)
+        flatten_cls_preds = torch.cat(temp_mlvl_cls_preds)
+        loss_cls = self.loss_cls(
+            flatten_cls_preds, flatten_labels, avg_factor=num_pos + 1)
+        return dict(loss_mask=loss_mask, loss_cls=loss_cls)
+
+    def _get_targets_single(self,
+                            gt_bboxes,
+                            gt_labels,
+                            gt_masks,
+                            featmap_sizes=None):
+        """Compute targets for predictions of single image.
+
+        Args:
+            gt_bboxes (Tensor): Ground truth bbox of each instance,
+                shape (num_gts, 4).
+            gt_labels (Tensor): Ground truth label of each instance,
+                shape (num_gts,).
+            gt_masks (Tensor): Ground truth mask of each instance,
+                shape (num_gts, h, w).
+            featmap_sizes (list[:obj:`torch.size`]): Size of each
+                feature map from feature pyramid, each element
+                means (feat_h, feat_w). Default: None.
+
+        Returns:
+            Tuple: Usually returns a tuple containing targets for predictions.
+
+                - mlvl_pos_mask_targets (list[Tensor]): Each element represent
+                  the binary mask targets for positive points in this
+                  level, has shape (num_pos, out_h, out_w).
+                - mlvl_labels (list[Tensor]): Each element is
+                  classification labels for all
+                  points in this level, has shape
+                  (num_grid, num_grid).
+                - mlvl_pos_masks (list[Tensor]): Each element is
+                  a `BoolTensor` to represent whether the
+                  corresponding point in single level
+                  is positive, has shape (num_grid **2).
+        """
+        device = gt_labels.device
+        gt_areas = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) *
+                              (gt_bboxes[:, 3] - gt_bboxes[:, 1]))
+
+        mlvl_pos_mask_targets = []
+        mlvl_labels = []
+        mlvl_pos_masks = []
+        for (lower_bound, upper_bound), stride, featmap_size, num_grid \
+                in zip(self.scale_ranges, self.strides,
+                       featmap_sizes, self.num_grids):
+
+            mask_target = torch.zeros(
+                [num_grid**2, featmap_size[0], featmap_size[1]],
+                dtype=torch.uint8,
+                device=device)
+            # FG cat_id: [0, num_classes -1], BG cat_id: num_classes
+            labels = torch.zeros([num_grid, num_grid],
+                                 dtype=torch.int64,
+                                 device=device) + self.num_classes
+            pos_mask = torch.zeros([num_grid**2],
+                                   dtype=torch.bool,
+                                   device=device)
+
+            gt_inds = ((gt_areas >= lower_bound) &
+                       (gt_areas <= upper_bound)).nonzero().flatten()
+            if len(gt_inds) == 0:
+                mlvl_pos_mask_targets.append(
+                    mask_target.new_zeros(0, featmap_size[0], featmap_size[1]))
+                mlvl_labels.append(labels)
+                mlvl_pos_masks.append(pos_mask)
+                continue
+            hit_gt_bboxes = gt_bboxes[gt_inds]
+            hit_gt_labels = gt_labels[gt_inds]
+            hit_gt_masks = gt_masks[gt_inds, ...]
+
+            pos_w_ranges = 0.5 * (hit_gt_bboxes[:, 2] -
+                                  hit_gt_bboxes[:, 0]) * self.pos_scale
+            pos_h_ranges = 0.5 * (hit_gt_bboxes[:, 3] -
+                                  hit_gt_bboxes[:, 1]) * self.pos_scale
+
+            # Make sure hit_gt_masks has a value
+            valid_mask_flags = hit_gt_masks.sum(dim=-1).sum(dim=-1) > 0
+            output_stride = stride / 2
+
+            for gt_mask, gt_label, pos_h_range, pos_w_range, \
+                valid_mask_flag in \
+                    zip(hit_gt_masks, hit_gt_labels, pos_h_ranges,
+                        pos_w_ranges, valid_mask_flags):
+                if not valid_mask_flag:
+                    continue
+                upsampled_size = (featmap_sizes[0][0] * 4,
+                                  featmap_sizes[0][1] * 4)
+                center_h, center_w = center_of_mass(gt_mask)
+
+                coord_w = int(
+                    (center_w / upsampled_size[1]) // (1. / num_grid))
+                coord_h = int(
+                    (center_h / upsampled_size[0]) // (1. / num_grid))
+
+                # left, top, right, down
+                top_box = max(
+                    0,
+                    int(((center_h - pos_h_range) / upsampled_size[0]) //
+                        (1. / num_grid)))
+                down_box = min(
+                    num_grid - 1,
+                    int(((center_h + pos_h_range) / upsampled_size[0]) //
+                        (1. / num_grid)))
+                left_box = max(
+                    0,
+                    int(((center_w - pos_w_range) / upsampled_size[1]) //
+                        (1. / num_grid)))
+                right_box = min(
+                    num_grid - 1,
+                    int(((center_w + pos_w_range) / upsampled_size[1]) //
+                        (1. / num_grid)))
+
+                top = max(top_box, coord_h - 1)
+                down = min(down_box, coord_h + 1)
+                left = max(coord_w - 1, left_box)
+                right = min(right_box, coord_w + 1)
+
+                labels[top:(down + 1), left:(right + 1)] = gt_label
+                # ins
+                gt_mask = np.uint8(gt_mask.cpu().numpy())
+                # Follow the original implementation, F.interpolate is
+                # different from cv2 and opencv
+                gt_mask = mmcv.imrescale(gt_mask, scale=1. / output_stride)
+                gt_mask = torch.from_numpy(gt_mask).to(device=device)
+
+                for i in range(top, down + 1):
+                    for j in range(left, right + 1):
+                        index = int(i * num_grid + j)
+                        mask_target[index, :gt_mask.shape[0], :gt_mask.
+                                    shape[1]] = gt_mask
+                        pos_mask[index] = True
+            mlvl_pos_mask_targets.append(mask_target[pos_mask])
+            mlvl_labels.append(labels)
+            mlvl_pos_masks.append(pos_mask)
+        return mlvl_pos_mask_targets, mlvl_labels, mlvl_pos_masks
+
+    def get_results(self, mlvl_mask_preds, mlvl_cls_scores, img_metas,
+                    **kwargs):
+        """Get multi-image mask results.
+
+        Args:
+            mlvl_mask_preds (list[Tensor]): Multi-level mask prediction.
+                Each element in the list has shape
+                (batch_size, num_grids**2 ,h ,w).
+            mlvl_cls_scores (list[Tensor]): Multi-level scores. Each element
+                in the list has shape
+                (batch_size, num_classes, num_grids ,num_grids).
+            img_metas (list[dict]): Meta information of all images.
+
+        Returns:
+            list[:obj:`InstanceData`]: Processed results of multiple
+            images.Each :obj:`InstanceData` usually contains
+            following keys.
+
+                - scores (Tensor): Classification scores, has shape
+                  (num_instance,).
+                - labels (Tensor): Has shape (num_instances,).
+                - masks (Tensor): Processed mask results, has
+                  shape (num_instances, h, w).
+        """
+        mlvl_cls_scores = [
+            item.permute(0, 2, 3, 1) for item in mlvl_cls_scores
+        ]
+        assert len(mlvl_mask_preds) == len(mlvl_cls_scores)
+        num_levels = len(mlvl_cls_scores)
+
+        results_list = []
+        for img_id in range(len(img_metas)):
+            cls_pred_list = [
+                mlvl_cls_scores[lvl][img_id].view(-1, self.cls_out_channels)
+                for lvl in range(num_levels)
+            ]
+            mask_pred_list = [
+                mlvl_mask_preds[lvl][img_id] for lvl in range(num_levels)
+            ]
+
+            cls_pred_list = torch.cat(cls_pred_list, dim=0)
+            mask_pred_list = torch.cat(mask_pred_list, dim=0)
+
+            results = self._get_results_single(
+                cls_pred_list, mask_pred_list, img_meta=img_metas[img_id])
+            results_list.append(results)
+
+        return results_list
+
+    def _get_results_single(self, cls_scores, mask_preds, img_meta, cfg=None):
+        """Get processed mask related results of single image.
+
+        Args:
+            cls_scores (Tensor): Classification score of all points
+                in single image, has shape (num_points, num_classes).
+            mask_preds (Tensor): Mask prediction of all points in
+                single image, has shape (num_points, feat_h, feat_w).
+            img_meta (dict): Meta information of corresponding image.
+            cfg (dict, optional): Config used in test phase.
+                Default: None.
+
+        Returns:
+            :obj:`InstanceData`: Processed results of single image.
+             it usually contains following keys.
+
+                - scores (Tensor): Classification scores, has shape
+                  (num_instance,).
+                - labels (Tensor): Has shape (num_instances,).
+                - masks (Tensor): Processed mask results, has
+                  shape (num_instances, h, w).
+        """
+
+        def empty_results(results, cls_scores):
+            """Generate a empty results."""
+            results.scores = cls_scores.new_ones(0)
+            results.masks = cls_scores.new_zeros(0, *results.ori_shape[:2])
+            results.labels = cls_scores.new_ones(0)
+            return results
+
+        cfg = self.test_cfg if cfg is None else cfg
+        assert len(cls_scores) == len(mask_preds)
+        results = InstanceData(img_meta)
+
+        featmap_size = mask_preds.size()[-2:]
+
+        img_shape = results.img_shape
+        ori_shape = results.ori_shape
+
+        h, w, _ = img_shape
+        upsampled_size = (featmap_size[0] * 4, featmap_size[1] * 4)
+
+        score_mask = (cls_scores > cfg.score_thr)
+        cls_scores = cls_scores[score_mask]
+        if len(cls_scores) == 0:
+            return empty_results(results, cls_scores)
+
+        inds = score_mask.nonzero()
+        cls_labels = inds[:, 1]
+
+        # Filter the mask mask with an area is smaller than
+        # stride of corresponding feature level
+        lvl_interval = cls_labels.new_tensor(self.num_grids).pow(2).cumsum(0)
+        strides = cls_scores.new_ones(lvl_interval[-1])
+        strides[:lvl_interval[0]] *= self.strides[0]
+        for lvl in range(1, self.num_levels):
+            strides[lvl_interval[lvl -
+                                 1]:lvl_interval[lvl]] *= self.strides[lvl]
+        strides = strides[inds[:, 0]]
+        mask_preds = mask_preds[inds[:, 0]]
+
+        masks = mask_preds > cfg.mask_thr
+        sum_masks = masks.sum((1, 2)).float()
+        keep = sum_masks > strides
+        if keep.sum() == 0:
+            return empty_results(results, cls_scores)
+        masks = masks[keep]
+        mask_preds = mask_preds[keep]
+        sum_masks = sum_masks[keep]
+        cls_scores = cls_scores[keep]
+        cls_labels = cls_labels[keep]
+
+        # maskness.
+        mask_scores = (mask_preds * masks).sum((1, 2)) / sum_masks
+        cls_scores *= mask_scores
+
+        scores, labels, _, keep_inds = mask_matrix_nms(
+            masks,
+            cls_labels,
+            cls_scores,
+            mask_area=sum_masks,
+            nms_pre=cfg.nms_pre,
+            max_num=cfg.max_per_img,
+            kernel=cfg.kernel,
+            sigma=cfg.sigma,
+            filter_thr=cfg.filter_thr)
+        mask_preds = mask_preds[keep_inds]
+        mask_preds = F.interpolate(
+            mask_preds.unsqueeze(0), size=upsampled_size,
+            mode='bilinear')[:, :, :h, :w]
+        mask_preds = F.interpolate(
+            mask_preds, size=ori_shape[:2], mode='bilinear').squeeze(0)
+        masks = mask_preds > cfg.mask_thr
+
+        results.masks = masks
+        results.labels = labels
+        results.scores = scores
+
+        return results
+
+
+@HEADS.register_module()
+class DecoupledSOLOHead(SOLOHead):
+    """Decoupled SOLO mask head used in `SOLO: Segmenting Objects by Locations.
+
+    <https://arxiv.org/abs/1912.04488>`_
+
+    Args:
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+    """
+
+    def __init__(self,
+                 *args,
+                 init_cfg=[
+                     dict(type='Normal', layer='Conv2d', std=0.01),
+                     dict(
+                         type='Normal',
+                         std=0.01,
+                         bias_prob=0.01,
+                         override=dict(name='conv_mask_list_x')),
+                     dict(
+                         type='Normal',
+                         std=0.01,
+                         bias_prob=0.01,
+                         override=dict(name='conv_mask_list_y')),
+                     dict(
+                         type='Normal',
+                         std=0.01,
+                         bias_prob=0.01,
+                         override=dict(name='conv_cls'))
+                 ],
+                 **kwargs):
+        super(DecoupledSOLOHead, self).__init__(
+            *args, init_cfg=init_cfg, **kwargs)
+
+    def _init_layers(self):
+        self.mask_convs_x = nn.ModuleList()
+        self.mask_convs_y = nn.ModuleList()
+        self.cls_convs = nn.ModuleList()
+
+        for i in range(self.stacked_convs):
+            chn = self.in_channels + 1 if i == 0 else self.feat_channels
+            self.mask_convs_x.append(
+                ConvModule(
+                    chn,
+                    self.feat_channels,
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg))
+            self.mask_convs_y.append(
+                ConvModule(
+                    chn,
+                    self.feat_channels,
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg))
+
+            chn = self.in_channels if i == 0 else self.feat_channels
+            self.cls_convs.append(
+                ConvModule(
+                    chn,
+                    self.feat_channels,
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg))
+
+        self.conv_mask_list_x = nn.ModuleList()
+        self.conv_mask_list_y = nn.ModuleList()
+        for num_grid in self.num_grids:
+            self.conv_mask_list_x.append(
+                nn.Conv2d(self.feat_channels, num_grid, 3, padding=1))
+            self.conv_mask_list_y.append(
+                nn.Conv2d(self.feat_channels, num_grid, 3, padding=1))
+        self.conv_cls = nn.Conv2d(
+            self.feat_channels, self.cls_out_channels, 3, padding=1)
+
+    def forward(self, feats):
+        assert len(feats) == self.num_levels
+        feats = self.resize_feats(feats)
+        mask_preds_x = []
+        mask_preds_y = []
+        cls_preds = []
+        for i in range(self.num_levels):
+            x = feats[i]
+            mask_feat = x
+            cls_feat = x
+            # generate and concat the coordinate
+            coord_feat = generate_coordinate(mask_feat.size(),
+                                             mask_feat.device)
+            mask_feat_x = torch.cat([mask_feat, coord_feat[:, 0:1, ...]], 1)
+            mask_feat_y = torch.cat([mask_feat, coord_feat[:, 1:2, ...]], 1)
+
+            for mask_layer_x, mask_layer_y in \
+                    zip(self.mask_convs_x, self.mask_convs_y):
+                mask_feat_x = mask_layer_x(mask_feat_x)
+                mask_feat_y = mask_layer_y(mask_feat_y)
+
+            mask_feat_x = F.interpolate(
+                mask_feat_x, scale_factor=2, mode='bilinear')
+            mask_feat_y = F.interpolate(
+                mask_feat_y, scale_factor=2, mode='bilinear')
+
+            mask_pred_x = self.conv_mask_list_x[i](mask_feat_x)
+            mask_pred_y = self.conv_mask_list_y[i](mask_feat_y)
+
+            # cls branch
+            for j, cls_layer in enumerate(self.cls_convs):
+                if j == self.cls_down_index:
+                    num_grid = self.num_grids[i]
+                    cls_feat = F.interpolate(
+                        cls_feat, size=num_grid, mode='bilinear')
+                cls_feat = cls_layer(cls_feat)
+
+            cls_pred = self.conv_cls(cls_feat)
+
+            if not self.training:
+                feat_wh = feats[0].size()[-2:]
+                upsampled_size = (feat_wh[0] * 2, feat_wh[1] * 2)
+                mask_pred_x = F.interpolate(
+                    mask_pred_x.sigmoid(),
+                    size=upsampled_size,
+                    mode='bilinear')
+                mask_pred_y = F.interpolate(
+                    mask_pred_y.sigmoid(),
+                    size=upsampled_size,
+                    mode='bilinear')
+                cls_pred = cls_pred.sigmoid()
+                # get local maximum
+                local_max = F.max_pool2d(cls_pred, 2, stride=1, padding=1)
+                keep_mask = local_max[:, :, :-1, :-1] == cls_pred
+                cls_pred = cls_pred * keep_mask
+
+            mask_preds_x.append(mask_pred_x)
+            mask_preds_y.append(mask_pred_y)
+            cls_preds.append(cls_pred)
+        return mask_preds_x, mask_preds_y, cls_preds
+
+    def loss(self,
+             mlvl_mask_preds_x,
+             mlvl_mask_preds_y,
+             mlvl_cls_preds,
+             gt_labels,
+             gt_masks,
+             img_metas,
+             gt_bboxes=None,
+             **kwargs):
+        """Calculate the loss of total batch.
+
+        Args:
+            mlvl_mask_preds_x (list[Tensor]): Multi-level mask prediction
+                from x branch. Each element in the list has shape
+                (batch_size, num_grids ,h ,w).
+            mlvl_mask_preds_x (list[Tensor]): Multi-level mask prediction
+                from y branch. Each element in the list has shape
+                (batch_size, num_grids ,h ,w).
+            mlvl_cls_preds (list[Tensor]): Multi-level scores. Each element
+                in the list has shape
+                (batch_size, num_classes, num_grids ,num_grids).
+            gt_labels (list[Tensor]): Labels of multiple images.
+            gt_masks (list[Tensor]): Ground truth masks of multiple images.
+                Each has shape (num_instances, h, w).
+            img_metas (list[dict]): Meta information of multiple images.
+            gt_bboxes (list[Tensor]): Ground truth bboxes of multiple
+                images. Default: None.
+
+        Returns:
+            dict[str, Tensor]: A dictionary of loss components.
+        """
+        num_levels = self.num_levels
+        num_imgs = len(gt_labels)
+        featmap_sizes = [featmap.size()[-2:] for featmap in mlvl_mask_preds_x]
+
+        pos_mask_targets, labels, \
+            xy_pos_indexes = \
+            multi_apply(self._get_targets_single,
+                        gt_bboxes,
+                        gt_labels,
+                        gt_masks,
+                        featmap_sizes=featmap_sizes)
+
+        # change from the outside list meaning multi images
+        # to the outside list meaning multi levels
+        mlvl_pos_mask_targets = [[] for _ in range(num_levels)]
+        mlvl_pos_mask_preds_x = [[] for _ in range(num_levels)]
+        mlvl_pos_mask_preds_y = [[] for _ in range(num_levels)]
+        mlvl_labels = [[] for _ in range(num_levels)]
+        for img_id in range(num_imgs):
+
+            for lvl in range(num_levels):
+                mlvl_pos_mask_targets[lvl].append(
+                    pos_mask_targets[img_id][lvl])
+                mlvl_pos_mask_preds_x[lvl].append(
+                    mlvl_mask_preds_x[lvl][img_id,
+                                           xy_pos_indexes[img_id][lvl][:, 1]])
+                mlvl_pos_mask_preds_y[lvl].append(
+                    mlvl_mask_preds_y[lvl][img_id,
+                                           xy_pos_indexes[img_id][lvl][:, 0]])
+                mlvl_labels[lvl].append(labels[img_id][lvl].flatten())
+
+        # cat multiple image
+        temp_mlvl_cls_preds = []
+        for lvl in range(num_levels):
+            mlvl_pos_mask_targets[lvl] = torch.cat(
+                mlvl_pos_mask_targets[lvl], dim=0)
+            mlvl_pos_mask_preds_x[lvl] = torch.cat(
+                mlvl_pos_mask_preds_x[lvl], dim=0)
+            mlvl_pos_mask_preds_y[lvl] = torch.cat(
+                mlvl_pos_mask_preds_y[lvl], dim=0)
+            mlvl_labels[lvl] = torch.cat(mlvl_labels[lvl], dim=0)
+            temp_mlvl_cls_preds.append(mlvl_cls_preds[lvl].permute(
+                0, 2, 3, 1).reshape(-1, self.cls_out_channels))
+
+        num_pos = 0.
+        # dice loss
+        loss_mask = []
+        for pred_x, pred_y, target in \
+                zip(mlvl_pos_mask_preds_x,
+                    mlvl_pos_mask_preds_y, mlvl_pos_mask_targets):
+            num_masks = pred_x.size(0)
+            if num_masks == 0:
+                # make sure can get grad
+                loss_mask.append((pred_x.sum() + pred_y.sum()).unsqueeze(0))
+                continue
+            num_pos += num_masks
+            pred_mask = pred_y.sigmoid() * pred_x.sigmoid()
+            loss_mask.append(
+                self.loss_mask(pred_mask, target, reduction_override='none'))
+        if num_pos > 0:
+            loss_mask = torch.cat(loss_mask).sum() / num_pos
+        else:
+            loss_mask = torch.cat(loss_mask).mean()
+
+        # cate
+        flatten_labels = torch.cat(mlvl_labels)
+        flatten_cls_preds = torch.cat(temp_mlvl_cls_preds)
+
+        loss_cls = self.loss_cls(
+            flatten_cls_preds, flatten_labels, avg_factor=num_pos + 1)
+        return dict(loss_mask=loss_mask, loss_cls=loss_cls)
+
+    def _get_targets_single(self,
+                            gt_bboxes,
+                            gt_labels,
+                            gt_masks,
+                            featmap_sizes=None):
+        """Compute targets for predictions of single image.
+
+        Args:
+            gt_bboxes (Tensor): Ground truth bbox of each instance,
+                shape (num_gts, 4).
+            gt_labels (Tensor): Ground truth label of each instance,
+                shape (num_gts,).
+            gt_masks (Tensor): Ground truth mask of each instance,
+                shape (num_gts, h, w).
+            featmap_sizes (list[:obj:`torch.size`]): Size of each
+                feature map from feature pyramid, each element
+                means (feat_h, feat_w). Default: None.
+
+        Returns:
+            Tuple: Usually returns a tuple containing targets for predictions.
+
+                - mlvl_pos_mask_targets (list[Tensor]): Each element represent
+                  the binary mask targets for positive points in this
+                  level, has shape (num_pos, out_h, out_w).
+                - mlvl_labels (list[Tensor]): Each element is
+                  classification labels for all
+                  points in this level, has shape
+                  (num_grid, num_grid).
+                - mlvl_xy_pos_indexes (list[Tensor]): Each element
+                  in the list contains the index of positive samples in
+                  corresponding level, has shape (num_pos, 2), last
+                  dimension 2 present (index_x, index_y).
+        """
+        mlvl_pos_mask_targets, mlvl_labels, \
+            mlvl_pos_masks = \
+            super()._get_targets_single(gt_bboxes, gt_labels, gt_masks,
+                                        featmap_sizes=featmap_sizes)
+
+        mlvl_xy_pos_indexes = [(item - self.num_classes).nonzero()
+                               for item in mlvl_labels]
+
+        return mlvl_pos_mask_targets, mlvl_labels, mlvl_xy_pos_indexes
+
+    def get_results(self,
+                    mlvl_mask_preds_x,
+                    mlvl_mask_preds_y,
+                    mlvl_cls_scores,
+                    img_metas,
+                    rescale=None,
+                    **kwargs):
+        """Get multi-image mask results.
+
+        Args:
+            mlvl_mask_preds_x (list[Tensor]): Multi-level mask prediction
+                from x branch. Each element in the list has shape
+                (batch_size, num_grids ,h ,w).
+            mlvl_mask_preds_y (list[Tensor]): Multi-level mask prediction
+                from y branch. Each element in the list has shape
+                (batch_size, num_grids ,h ,w).
+            mlvl_cls_scores (list[Tensor]): Multi-level scores. Each element
+                in the list has shape
+                (batch_size, num_classes ,num_grids ,num_grids).
+            img_metas (list[dict]): Meta information of all images.
+
+        Returns:
+            list[:obj:`InstanceData`]: Processed results of multiple
+            images.Each :obj:`InstanceData` usually contains
+            following keys.
+
+                - scores (Tensor): Classification scores, has shape
+                  (num_instance,).
+                - labels (Tensor): Has shape (num_instances,).
+                - masks (Tensor): Processed mask results, has
+                  shape (num_instances, h, w).
+        """
+        mlvl_cls_scores = [
+            item.permute(0, 2, 3, 1) for item in mlvl_cls_scores
+        ]
+        assert len(mlvl_mask_preds_x) == len(mlvl_cls_scores)
+        num_levels = len(mlvl_cls_scores)
+
+        results_list = []
+        for img_id in range(len(img_metas)):
+            cls_pred_list = [
+                mlvl_cls_scores[i][img_id].view(
+                    -1, self.cls_out_channels).detach()
+                for i in range(num_levels)
+            ]
+            mask_pred_list_x = [
+                mlvl_mask_preds_x[i][img_id] for i in range(num_levels)
+            ]
+            mask_pred_list_y = [
+                mlvl_mask_preds_y[i][img_id] for i in range(num_levels)
+            ]
+
+            cls_pred_list = torch.cat(cls_pred_list, dim=0)
+            mask_pred_list_x = torch.cat(mask_pred_list_x, dim=0)
+            mask_pred_list_y = torch.cat(mask_pred_list_y, dim=0)
+
+            results = self._get_results_single(
+                cls_pred_list,
+                mask_pred_list_x,
+                mask_pred_list_y,
+                img_meta=img_metas[img_id],
+                cfg=self.test_cfg)
+            results_list.append(results)
+        return results_list
+
+    def _get_results_single(self, cls_scores, mask_preds_x, mask_preds_y,
+                            img_meta, cfg):
+        """Get processed mask related results of single image.
+
+        Args:
+            cls_scores (Tensor): Classification score of all points
+                in single image, has shape (num_points, num_classes).
+            mask_preds_x (Tensor): Mask prediction of x branch of
+                all points in single image, has shape
+                (sum_num_grids, feat_h, feat_w).
+            mask_preds_y (Tensor): Mask prediction of y branch of
+                all points in single image, has shape
+                (sum_num_grids, feat_h, feat_w).
+            img_meta (dict): Meta information of corresponding image.
+            cfg (dict): Config used in test phase.
+
+        Returns:
+            :obj:`InstanceData`: Processed results of single image.
+             it usually contains following keys.
+
+                - scores (Tensor): Classification scores, has shape
+                  (num_instance,).
+                - labels (Tensor): Has shape (num_instances,).
+                - masks (Tensor): Processed mask results, has
+                  shape (num_instances, h, w).
+        """
+
+        def empty_results(results, cls_scores):
+            """Generate a empty results."""
+            results.scores = cls_scores.new_ones(0)
+            results.masks = cls_scores.new_zeros(0, *results.ori_shape[:2])
+            results.labels = cls_scores.new_ones(0)
+            return results
+
+        cfg = self.test_cfg if cfg is None else cfg
+
+        results = InstanceData(img_meta)
+        img_shape = results.img_shape
+        ori_shape = results.ori_shape
+        h, w, _ = img_shape
+        featmap_size = mask_preds_x.size()[-2:]
+        upsampled_size = (featmap_size[0] * 4, featmap_size[1] * 4)
+
+        score_mask = (cls_scores > cfg.score_thr)
+        cls_scores = cls_scores[score_mask]
+        inds = score_mask.nonzero()
+        lvl_interval = inds.new_tensor(self.num_grids).pow(2).cumsum(0)
+        num_all_points = lvl_interval[-1]
+        lvl_start_index = inds.new_ones(num_all_points)
+        num_grids = inds.new_ones(num_all_points)
+        seg_size = inds.new_tensor(self.num_grids).cumsum(0)
+        mask_lvl_start_index = inds.new_ones(num_all_points)
+        strides = inds.new_ones(num_all_points)
+
+        lvl_start_index[:lvl_interval[0]] *= 0
+        mask_lvl_start_index[:lvl_interval[0]] *= 0
+        num_grids[:lvl_interval[0]] *= self.num_grids[0]
+        strides[:lvl_interval[0]] *= self.strides[0]
+
+        for lvl in range(1, self.num_levels):
+            lvl_start_index[lvl_interval[lvl - 1]:lvl_interval[lvl]] *= \
+                lvl_interval[lvl - 1]
+            mask_lvl_start_index[lvl_interval[lvl - 1]:lvl_interval[lvl]] *= \
+                seg_size[lvl - 1]
+            num_grids[lvl_interval[lvl - 1]:lvl_interval[lvl]] *= \
+                self.num_grids[lvl]
+            strides[lvl_interval[lvl - 1]:lvl_interval[lvl]] *= \
+                self.strides[lvl]
+
+        lvl_start_index = lvl_start_index[inds[:, 0]]
+        mask_lvl_start_index = mask_lvl_start_index[inds[:, 0]]
+        num_grids = num_grids[inds[:, 0]]
+        strides = strides[inds[:, 0]]
+
+        y_lvl_offset = (inds[:, 0] - lvl_start_index) // num_grids
+        x_lvl_offset = (inds[:, 0] - lvl_start_index) % num_grids
+        y_inds = mask_lvl_start_index + y_lvl_offset
+        x_inds = mask_lvl_start_index + x_lvl_offset
+
+        cls_labels = inds[:, 1]
+        mask_preds = mask_preds_x[x_inds, ...] * mask_preds_y[y_inds, ...]
+
+        masks = mask_preds > cfg.mask_thr
+        sum_masks = masks.sum((1, 2)).float()
+        keep = sum_masks > strides
+        if keep.sum() == 0:
+            return empty_results(results, cls_scores)
+
+        masks = masks[keep]
+        mask_preds = mask_preds[keep]
+        sum_masks = sum_masks[keep]
+        cls_scores = cls_scores[keep]
+        cls_labels = cls_labels[keep]
+
+        # maskness.
+        mask_scores = (mask_preds * masks).sum((1, 2)) / sum_masks
+        cls_scores *= mask_scores
+
+        scores, labels, _, keep_inds = mask_matrix_nms(
+            masks,
+            cls_labels,
+            cls_scores,
+            mask_area=sum_masks,
+            nms_pre=cfg.nms_pre,
+            max_num=cfg.max_per_img,
+            kernel=cfg.kernel,
+            sigma=cfg.sigma,
+            filter_thr=cfg.filter_thr)
+        mask_preds = mask_preds[keep_inds]
+        mask_preds = F.interpolate(
+            mask_preds.unsqueeze(0), size=upsampled_size,
+            mode='bilinear')[:, :, :h, :w]
+        mask_preds = F.interpolate(
+            mask_preds, size=ori_shape[:2], mode='bilinear').squeeze(0)
+        masks = mask_preds > cfg.mask_thr
+
+        results.masks = masks
+        results.labels = labels
+        results.scores = scores
+
+        return results
+
+
+@HEADS.register_module()
+class DecoupledSOLOLightHead(DecoupledSOLOHead):
+    """Decoupled Light SOLO mask head used in `SOLO: Segmenting Objects by
+    Locations <https://arxiv.org/abs/1912.04488>`_
+
+    Args:
+        with_dcn (bool): Whether use dcn in mask_convs and cls_convs,
+            default: False.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+    """
+
+    def __init__(self,
+                 *args,
+                 dcn_cfg=None,
+                 init_cfg=[
+                     dict(type='Normal', layer='Conv2d', std=0.01),
+                     dict(
+                         type='Normal',
+                         std=0.01,
+                         bias_prob=0.01,
+                         override=dict(name='conv_mask_list_x')),
+                     dict(
+                         type='Normal',
+                         std=0.01,
+                         bias_prob=0.01,
+                         override=dict(name='conv_mask_list_y')),
+                     dict(
+                         type='Normal',
+                         std=0.01,
+                         bias_prob=0.01,
+                         override=dict(name='conv_cls'))
+                 ],
+                 **kwargs):
+        assert dcn_cfg is None or isinstance(dcn_cfg, dict)
+        self.dcn_cfg = dcn_cfg
+        super(DecoupledSOLOLightHead, self).__init__(
+            *args, init_cfg=init_cfg, **kwargs)
+
+    def _init_layers(self):
+        self.mask_convs = nn.ModuleList()
+        self.cls_convs = nn.ModuleList()
+
+        for i in range(self.stacked_convs):
+            if self.dcn_cfg is not None\
+                    and i == self.stacked_convs - 1:
+                conv_cfg = self.dcn_cfg
+            else:
+                conv_cfg = None
+
+            chn = self.in_channels + 2 if i == 0 else self.feat_channels
+            self.mask_convs.append(
+                ConvModule(
+                    chn,
+                    self.feat_channels,
+                    3,
+                    stride=1,
+                    padding=1,
+                    conv_cfg=conv_cfg,
+                    norm_cfg=self.norm_cfg))
+
+            chn = self.in_channels if i == 0 else self.feat_channels
+            self.cls_convs.append(
+                ConvModule(
+                    chn,
+                    self.feat_channels,
+                    3,
+                    stride=1,
+                    padding=1,
+                    conv_cfg=conv_cfg,
+                    norm_cfg=self.norm_cfg))
+
+        self.conv_mask_list_x = nn.ModuleList()
+        self.conv_mask_list_y = nn.ModuleList()
+        for num_grid in self.num_grids:
+            self.conv_mask_list_x.append(
+                nn.Conv2d(self.feat_channels, num_grid, 3, padding=1))
+            self.conv_mask_list_y.append(
+                nn.Conv2d(self.feat_channels, num_grid, 3, padding=1))
+        self.conv_cls = nn.Conv2d(
+            self.feat_channels, self.cls_out_channels, 3, padding=1)
+
+    def forward(self, feats):
+        assert len(feats) == self.num_levels
+        feats = self.resize_feats(feats)
+        mask_preds_x = []
+        mask_preds_y = []
+        cls_preds = []
+        for i in range(self.num_levels):
+            x = feats[i]
+            mask_feat = x
+            cls_feat = x
+            # generate and concat the coordinate
+            coord_feat = generate_coordinate(mask_feat.size(),
+                                             mask_feat.device)
+            mask_feat = torch.cat([mask_feat, coord_feat], 1)
+
+            for mask_layer in self.mask_convs:
+                mask_feat = mask_layer(mask_feat)
+
+            mask_feat = F.interpolate(
+                mask_feat, scale_factor=2, mode='bilinear')
+
+            mask_pred_x = self.conv_mask_list_x[i](mask_feat)
+            mask_pred_y = self.conv_mask_list_y[i](mask_feat)
+
+            # cls branch
+            for j, cls_layer in enumerate(self.cls_convs):
+                if j == self.cls_down_index:
+                    num_grid = self.num_grids[i]
+                    cls_feat = F.interpolate(
+                        cls_feat, size=num_grid, mode='bilinear')
+                cls_feat = cls_layer(cls_feat)
+
+            cls_pred = self.conv_cls(cls_feat)
+
+            if not self.training:
+                feat_wh = feats[0].size()[-2:]
+                upsampled_size = (feat_wh[0] * 2, feat_wh[1] * 2)
+                mask_pred_x = F.interpolate(
+                    mask_pred_x.sigmoid(),
+                    size=upsampled_size,
+                    mode='bilinear')
+                mask_pred_y = F.interpolate(
+                    mask_pred_y.sigmoid(),
+                    size=upsampled_size,
+                    mode='bilinear')
+                cls_pred = cls_pred.sigmoid()
+                # get local maximum
+                local_max = F.max_pool2d(cls_pred, 2, stride=1, padding=1)
+                keep_mask = local_max[:, :, :-1, :-1] == cls_pred
+                cls_pred = cls_pred * keep_mask
+
+            mask_preds_x.append(mask_pred_x)
+            mask_preds_y.append(mask_pred_y)
+            cls_preds.append(cls_pred)
+        return mask_preds_x, mask_preds_y, cls_preds
diff --git a/mmdet/models/dense_heads/ssd_head.py b/mmdet/models/dense_heads/ssd_head.py
index 22dac741..8232489b 100644
--- a/mmdet/models/dense_heads/ssd_head.py
+++ b/mmdet/models/dense_heads/ssd_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
@@ -326,12 +327,6 @@ class SSDHead(AnchorHead):
         for i in range(num_images):
             all_anchors.append(torch.cat(anchor_list[i]))
 
-        # check NaN and Inf
-        assert torch.isfinite(all_cls_scores).all().item(), \
-            'classification scores become infinite or NaN!'
-        assert torch.isfinite(all_bbox_preds).all().item(), \
-            'bbox predications become infinite or NaN!'
-
         losses_cls, losses_bbox = multi_apply(
             self.loss_single,
             all_cls_scores,
diff --git a/mmdet/models/dense_heads/vfnet_head.py b/mmdet/models/dense_heads/vfnet_head.py
index 6d887d56..7b7515c6 100644
--- a/mmdet/models/dense_heads/vfnet_head.py
+++ b/mmdet/models/dense_heads/vfnet_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
diff --git a/mmdet/models/dense_heads/yolact_head.py b/mmdet/models/dense_heads/yolact_head.py
index 5958263f..d6a640a7 100644
--- a/mmdet/models/dense_heads/yolact_head.py
+++ b/mmdet/models/dense_heads/yolact_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
@@ -688,8 +689,15 @@ class YOLACTProtonet(BaseModule):
         prototypes = prototypes.permute(0, 2, 3, 1).contiguous()
 
         num_imgs = x.size(0)
-        # Training state
-        if self.training:
+
+        # The reason for not using self.training is that
+        # val workflow will have a dimension mismatch error.
+        # Note that this writing method is very tricky.
+        # Fix https://github.com/open-mmlab/mmdetection/issues/5978
+        is_train_or_val_workflow = (coeff_pred[0].dim() == 4)
+
+        # Train or val workflow
+        if is_train_or_val_workflow:
             coeff_pred_list = []
             for coeff_pred_per_level in coeff_pred:
                 coeff_pred_per_level = \
@@ -706,7 +714,7 @@ class YOLACTProtonet(BaseModule):
             cur_img_meta = img_meta[idx]
 
             # Testing state
-            if not self.training:
+            if not is_train_or_val_workflow:
                 bboxes_for_cropping = cur_bboxes
             else:
                 cur_sampling_results = sampling_results[idx]
diff --git a/mmdet/models/dense_heads/yolo_head.py b/mmdet/models/dense_heads/yolo_head.py
index ea5c802e..e76c8e95 100644
--- a/mmdet/models/dense_heads/yolo_head.py
+++ b/mmdet/models/dense_heads/yolo_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # Copyright (c) 2019 Western Digital Corporation or its affiliates.
 
 import warnings
@@ -193,7 +194,8 @@ class YOLOV3Head(BaseDenseHead, BBoxTestMixin):
                    cfg=None,
                    rescale=False,
                    with_nms=True):
-        """Transform network output for a batch into bbox predictions.
+        """Transform network output for a batch into bbox predictions. It has
+        been accelerated since PR #5991.
 
         Args:
             pred_maps (list[Tensor]): Raw predictions for a batch of images.
@@ -214,200 +216,66 @@ class YOLOV3Head(BaseDenseHead, BBoxTestMixin):
                 each element represents the class label of the corresponding
                 box.
         """
-        num_levels = len(pred_maps)
-        pred_maps_list = [pred_maps[i].detach() for i in range(num_levels)]
-        scale_factors = [
-            img_metas[i]['scale_factor']
-            for i in range(pred_maps_list[0].shape[0])
-        ]
-        result_list = self._get_bboxes(pred_maps_list, scale_factors, cfg,
-                                       rescale, with_nms)
-        return result_list
-
-    def _get_bboxes(self,
-                    pred_maps_list,
-                    scale_factors,
-                    cfg,
-                    rescale=False,
-                    with_nms=True):
-        """Transform outputs for a single batch item into bbox predictions.
-
-        Args:
-            pred_maps_list (list[Tensor]): Prediction maps for different scales
-                of each single image in the batch.
-            scale_factors (list(ndarray)): Scale factor of the image arrange as
-                (w_scale, h_scale, w_scale, h_scale).
-            cfg (mmcv.Config | None): Test / postprocessing configuration,
-                if None, test_cfg would be used.
-            rescale (bool): If True, return boxes in original image space.
-                Default: False.
-            with_nms (bool): If True, do nms before return boxes.
-                Default: True.
-
-        Returns:
-            list[tuple[Tensor, Tensor]]: Each item in result_list is 2-tuple.
-                The first item is an (n, 5) tensor, where 5 represent
-                (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1.
-                The shape of the second tensor in the tuple is (n,), and
-                each element represents the class label of the corresponding
-                box.
-        """
+        assert len(pred_maps) == self.num_levels
         cfg = self.test_cfg if cfg is None else cfg
-        assert len(pred_maps_list) == self.num_levels
-
-        device = pred_maps_list[0].device
-        batch_size = pred_maps_list[0].shape[0]
-
-        featmap_sizes = [
-            pred_maps_list[i].shape[-2:] for i in range(self.num_levels)
-        ]
-        multi_lvl_anchors = self.anchor_generator.grid_anchors(
-            featmap_sizes, device)
-        # convert to tensor to keep tracing
-        nms_pre_tensor = torch.tensor(
-            cfg.get('nms_pre', -1), device=device, dtype=torch.long)
-
-        multi_lvl_bboxes = []
-        multi_lvl_cls_scores = []
-        multi_lvl_conf_scores = []
-        for i in range(self.num_levels):
-            # get some key info for current scale
-            pred_map = pred_maps_list[i]
-            stride = self.featmap_strides[i]
-            # (b,h, w, num_anchors*num_attrib) ->
-            # (b,h*w*num_anchors, num_attrib)
-            pred_map = pred_map.permute(0, 2, 3,
-                                        1).reshape(batch_size, -1,
-                                                   self.num_attrib)
-            # Inplace operation like
-            # ```pred_map[..., :2] = \torch.sigmoid(pred_map[..., :2])```
-            # would create constant tensor when exporting to onnx
-            pred_map_conf = torch.sigmoid(pred_map[..., :2])
-            pred_map_rest = pred_map[..., 2:]
-            pred_map = torch.cat([pred_map_conf, pred_map_rest], dim=-1)
-            pred_map_boxes = pred_map[..., :4]
-            multi_lvl_anchor = multi_lvl_anchors[i]
-            multi_lvl_anchor = multi_lvl_anchor.expand_as(pred_map_boxes)
-            bbox_pred = self.bbox_coder.decode(multi_lvl_anchor,
-                                               pred_map_boxes, stride)
-            # conf and cls
-            conf_pred = torch.sigmoid(pred_map[..., 4])
-            cls_pred = torch.sigmoid(pred_map[..., 5:]).view(
-                batch_size, -1, self.num_classes)  # Cls pred one-hot.
-
-            # Get top-k prediction
-            from mmdet.core.export import get_k_for_topk
-            nms_pre = get_k_for_topk(nms_pre_tensor, bbox_pred.shape[1])
-            if nms_pre > 0:
-                _, topk_inds = conf_pred.topk(nms_pre)
-                batch_inds = torch.arange(batch_size).view(
-                    -1, 1).expand_as(topk_inds).long()
-                # Avoid onnx2tensorrt issue in https://github.com/NVIDIA/TensorRT/issues/1134 # noqa: E501
-                if torch.onnx.is_in_onnx_export():
-                    transformed_inds = (
-                        bbox_pred.shape[1] * batch_inds + topk_inds)
-                    bbox_pred = bbox_pred.reshape(
-                        -1, 4)[transformed_inds, :].reshape(batch_size, -1, 4)
-                    cls_pred = cls_pred.reshape(
-                        -1, self.num_classes)[transformed_inds, :].reshape(
-                            batch_size, -1, self.num_classes)
-                    conf_pred = conf_pred.reshape(-1,
-                                                  1)[transformed_inds].reshape(
-                                                      batch_size, -1)
-                else:
-                    bbox_pred = bbox_pred[batch_inds, topk_inds, :]
-                    cls_pred = cls_pred[batch_inds, topk_inds, :]
-                    conf_pred = conf_pred[batch_inds, topk_inds]
-            # Save the result of current scale
-            multi_lvl_bboxes.append(bbox_pred)
-            multi_lvl_cls_scores.append(cls_pred)
-            multi_lvl_conf_scores.append(conf_pred)
-
-        # Merge the results of different scales together
-        batch_mlvl_bboxes = torch.cat(multi_lvl_bboxes, dim=1)
-        batch_mlvl_scores = torch.cat(multi_lvl_cls_scores, dim=1)
-        batch_mlvl_conf_scores = torch.cat(multi_lvl_conf_scores, dim=1)
-
-        # Replace multiclass_nms with ONNX::NonMaxSuppression in deployment
-        if torch.onnx.is_in_onnx_export() and with_nms:
-            from mmdet.core.export import add_dummy_nms_for_onnx
-            conf_thr = cfg.get('conf_thr', -1)
-            score_thr = cfg.get('score_thr', -1)
-            # follow original pipeline of YOLOv3
-            if conf_thr > 0:
-                mask = (batch_mlvl_conf_scores >= conf_thr).float()
-                batch_mlvl_conf_scores *= mask
-            if score_thr > 0:
-                mask = (batch_mlvl_scores > score_thr).float()
-                batch_mlvl_scores *= mask
-            batch_mlvl_conf_scores = batch_mlvl_conf_scores.unsqueeze(
-                2).expand_as(batch_mlvl_scores)
-            batch_mlvl_scores = batch_mlvl_scores * batch_mlvl_conf_scores
-            max_output_boxes_per_class = cfg.nms.get(
-                'max_output_boxes_per_class', 200)
-            iou_threshold = cfg.nms.get('iou_threshold', 0.5)
-            # keep aligned with original pipeline, improve
-            # mAP by 1% for YOLOv3 in ONNX
-            score_threshold = 0
-            nms_pre = cfg.get('deploy_nms_pre', -1)
-            return add_dummy_nms_for_onnx(
-                batch_mlvl_bboxes,
-                batch_mlvl_scores,
-                max_output_boxes_per_class,
-                iou_threshold,
-                score_threshold,
-                nms_pre,
-                cfg.max_per_img,
-            )
+        scale_factors = [img_meta['scale_factor'] for img_meta in img_metas]
 
-        if with_nms and (batch_mlvl_conf_scores.size(0) == 0):
+        num_imgs = len(img_metas)
+        featmap_sizes = [pred_map.shape[-2:] for pred_map in pred_maps]
+
+        mlvl_anchors = self.anchor_generator.grid_anchors(
+            featmap_sizes, pred_maps[0].device)
+        flatten_preds = []
+        flatten_strides = []
+        for pred, stride in zip(pred_maps, self.featmap_strides):
+            pred = pred.permute(0, 2, 3, 1).reshape(num_imgs, -1,
+                                                    self.num_attrib)
+            pred[..., :2].sigmoid_()
+            flatten_preds.append(pred)
+            flatten_strides.append(
+                pred.new_tensor(stride).expand(pred.size(1)))
+
+        flatten_preds = torch.cat(flatten_preds, dim=1)
+        flatten_bbox_preds = flatten_preds[..., :4]
+        flatten_objectness = flatten_preds[..., 4].sigmoid()
+        flatten_cls_scores = flatten_preds[..., 5:].sigmoid()
+        flatten_anchors = torch.cat(mlvl_anchors)
+        flatten_strides = torch.cat(flatten_strides)
+        flatten_bboxes = self.bbox_coder.decode(flatten_anchors,
+                                                flatten_bbox_preds,
+                                                flatten_strides.unsqueeze(-1))
+
+        if with_nms and (flatten_objectness.size(0) == 0):
             return torch.zeros((0, 5)), torch.zeros((0, ))
 
         if rescale:
-            batch_mlvl_bboxes /= batch_mlvl_bboxes.new_tensor(
+            flatten_bboxes /= flatten_bboxes.new_tensor(
                 scale_factors).unsqueeze(1)
 
-        # In mmdet 2.x, the class_id for background is num_classes.
-        # i.e., the last column.
-        padding = batch_mlvl_scores.new_zeros(batch_size,
-                                              batch_mlvl_scores.shape[1], 1)
-        batch_mlvl_scores = torch.cat([batch_mlvl_scores, padding], dim=-1)
-
-        # Support exporting to onnx without nms
-        if with_nms and cfg.get('nms', None) is not None:
-            det_results = []
-            for (mlvl_bboxes, mlvl_scores,
-                 mlvl_conf_scores) in zip(batch_mlvl_bboxes, batch_mlvl_scores,
-                                          batch_mlvl_conf_scores):
-                # Filtering out all predictions with conf < conf_thr
-                conf_thr = cfg.get('conf_thr', -1)
-                if conf_thr > 0 and (not torch.onnx.is_in_onnx_export()):
-                    # TensorRT not support NonZero
-                    # add as_tuple=False for compatibility in Pytorch 1.6
-                    # flatten would create a Reshape op with constant values,
-                    # and raise RuntimeError when doing inference in ONNX
-                    # Runtime with a different input image (#4221).
-                    conf_inds = mlvl_conf_scores.ge(conf_thr).nonzero(
-                        as_tuple=False).squeeze(1)
-                    mlvl_bboxes = mlvl_bboxes[conf_inds, :]
-                    mlvl_scores = mlvl_scores[conf_inds, :]
-                    mlvl_conf_scores = mlvl_conf_scores[conf_inds]
-
-                det_bboxes, det_labels = multiclass_nms(
-                    mlvl_bboxes,
-                    mlvl_scores,
-                    cfg.score_thr,
-                    cfg.nms,
-                    cfg.max_per_img,
-                    score_factors=mlvl_conf_scores)
-                det_results.append(tuple([det_bboxes, det_labels]))
+        padding = flatten_bboxes.new_zeros(num_imgs, flatten_bboxes.shape[1],
+                                           1)
+        flatten_cls_scores = torch.cat([flatten_cls_scores, padding], dim=-1)
 
-        else:
-            det_results = [
-                tuple(mlvl_bs)
-                for mlvl_bs in zip(batch_mlvl_bboxes, batch_mlvl_scores,
-                                   batch_mlvl_conf_scores)
-            ]
+        det_results = []
+        for (bboxes, scores, objectness) in zip(flatten_bboxes,
+                                                flatten_cls_scores,
+                                                flatten_objectness):
+            # Filtering out all predictions with conf < conf_thr
+            conf_thr = cfg.get('conf_thr', -1)
+            if conf_thr > 0:
+                conf_inds = objectness >= conf_thr
+                bboxes = bboxes[conf_inds, :]
+                scores = scores[conf_inds, :]
+                objectness = objectness[conf_inds]
+
+            det_bboxes, det_labels = multiclass_nms(
+                bboxes,
+                scores,
+                cfg.score_thr,
+                cfg.nms,
+                cfg.max_per_img,
+                score_factors=objectness)
+            det_results.append(tuple([det_bboxes, det_labels]))
         return det_results
 
     @force_fp32(apply_to=('pred_maps', ))
@@ -620,3 +488,114 @@ class YOLOV3Head(BaseDenseHead, BBoxTestMixin):
             list[ndarray]: bbox results of each class
         """
         return self.aug_test_bboxes(feats, img_metas, rescale=rescale)
+
+    @force_fp32(apply_to=('pred_maps'))
+    def onnx_export(self, pred_maps, img_metas, with_nms=True):
+        num_levels = len(pred_maps)
+        pred_maps_list = [pred_maps[i].detach() for i in range(num_levels)]
+
+        cfg = self.test_cfg
+        assert len(pred_maps_list) == self.num_levels
+
+        device = pred_maps_list[0].device
+        batch_size = pred_maps_list[0].shape[0]
+
+        featmap_sizes = [
+            pred_maps_list[i].shape[-2:] for i in range(self.num_levels)
+        ]
+        multi_lvl_anchors = self.anchor_generator.grid_anchors(
+            featmap_sizes, device)
+        # convert to tensor to keep tracing
+        nms_pre_tensor = torch.tensor(
+            cfg.get('nms_pre', -1), device=device, dtype=torch.long)
+
+        multi_lvl_bboxes = []
+        multi_lvl_cls_scores = []
+        multi_lvl_conf_scores = []
+        for i in range(self.num_levels):
+            # get some key info for current scale
+            pred_map = pred_maps_list[i]
+            stride = self.featmap_strides[i]
+            # (b,h, w, num_anchors*num_attrib) ->
+            # (b,h*w*num_anchors, num_attrib)
+            pred_map = pred_map.permute(0, 2, 3,
+                                        1).reshape(batch_size, -1,
+                                                   self.num_attrib)
+            # Inplace operation like
+            # ```pred_map[..., :2] = \torch.sigmoid(pred_map[..., :2])```
+            # would create constant tensor when exporting to onnx
+            pred_map_conf = torch.sigmoid(pred_map[..., :2])
+            pred_map_rest = pred_map[..., 2:]
+            pred_map = torch.cat([pred_map_conf, pred_map_rest], dim=-1)
+            pred_map_boxes = pred_map[..., :4]
+            multi_lvl_anchor = multi_lvl_anchors[i]
+            multi_lvl_anchor = multi_lvl_anchor.expand_as(pred_map_boxes)
+            bbox_pred = self.bbox_coder.decode(multi_lvl_anchor,
+                                               pred_map_boxes, stride)
+            # conf and cls
+            conf_pred = torch.sigmoid(pred_map[..., 4])
+            cls_pred = torch.sigmoid(pred_map[..., 5:]).view(
+                batch_size, -1, self.num_classes)  # Cls pred one-hot.
+
+            # Get top-k prediction
+            from mmdet.core.export import get_k_for_topk
+            nms_pre = get_k_for_topk(nms_pre_tensor, bbox_pred.shape[1])
+            if nms_pre > 0:
+                _, topk_inds = conf_pred.topk(nms_pre)
+                batch_inds = torch.arange(batch_size).view(
+                    -1, 1).expand_as(topk_inds).long()
+                # Avoid onnx2tensorrt issue in https://github.com/NVIDIA/TensorRT/issues/1134 # noqa: E501
+                transformed_inds = (
+                    bbox_pred.shape[1] * batch_inds + topk_inds)
+                bbox_pred = bbox_pred.reshape(-1,
+                                              4)[transformed_inds, :].reshape(
+                                                  batch_size, -1, 4)
+                cls_pred = cls_pred.reshape(
+                    -1, self.num_classes)[transformed_inds, :].reshape(
+                        batch_size, -1, self.num_classes)
+                conf_pred = conf_pred.reshape(-1, 1)[transformed_inds].reshape(
+                    batch_size, -1)
+
+            # Save the result of current scale
+            multi_lvl_bboxes.append(bbox_pred)
+            multi_lvl_cls_scores.append(cls_pred)
+            multi_lvl_conf_scores.append(conf_pred)
+
+        # Merge the results of different scales together
+        batch_mlvl_bboxes = torch.cat(multi_lvl_bboxes, dim=1)
+        batch_mlvl_scores = torch.cat(multi_lvl_cls_scores, dim=1)
+        batch_mlvl_conf_scores = torch.cat(multi_lvl_conf_scores, dim=1)
+
+        # Replace multiclass_nms with ONNX::NonMaxSuppression in deployment
+        from mmdet.core.export import add_dummy_nms_for_onnx
+        conf_thr = cfg.get('conf_thr', -1)
+        score_thr = cfg.get('score_thr', -1)
+        # follow original pipeline of YOLOv3
+        if conf_thr > 0:
+            mask = (batch_mlvl_conf_scores >= conf_thr).float()
+            batch_mlvl_conf_scores *= mask
+        if score_thr > 0:
+            mask = (batch_mlvl_scores > score_thr).float()
+            batch_mlvl_scores *= mask
+        batch_mlvl_conf_scores = batch_mlvl_conf_scores.unsqueeze(2).expand_as(
+            batch_mlvl_scores)
+        batch_mlvl_scores = batch_mlvl_scores * batch_mlvl_conf_scores
+        if with_nms:
+            max_output_boxes_per_class = cfg.nms.get(
+                'max_output_boxes_per_class', 200)
+            iou_threshold = cfg.nms.get('iou_threshold', 0.5)
+            # keep aligned with original pipeline, improve
+            # mAP by 1% for YOLOv3 in ONNX
+            score_threshold = 0
+            nms_pre = cfg.get('deploy_nms_pre', -1)
+            return add_dummy_nms_for_onnx(
+                batch_mlvl_bboxes,
+                batch_mlvl_scores,
+                max_output_boxes_per_class,
+                iou_threshold,
+                score_threshold,
+                nms_pre,
+                cfg.max_per_img,
+            )
+        else:
+            return batch_mlvl_bboxes, batch_mlvl_scores
diff --git a/mmdet/models/dense_heads/yolof_head.py b/mmdet/models/dense_heads/yolof_head.py
index e15d4d4a..8c9a4861 100644
--- a/mmdet/models/dense_heads/yolof_head.py
+++ b/mmdet/models/dense_heads/yolof_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.cnn import (ConvModule, bias_init_with_prob, constant_init, is_norm,
diff --git a/mmdet/models/dense_heads/yolox_head.py b/mmdet/models/dense_heads/yolox_head.py
index 857d4903..8ce0345b 100644
--- a/mmdet/models/dense_heads/yolox_head.py
+++ b/mmdet/models/dense_heads/yolox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch
@@ -6,6 +7,7 @@ import torch.nn.functional as F
 from mmcv.cnn import (ConvModule, DepthwiseSeparableConvModule,
                       bias_init_with_prob)
 from mmcv.ops.nms import batched_nms
+from mmcv.runner import force_fp32
 
 from mmdet.core import (MlvlPointGenerator, bbox_xyxy_to_cxcywh,
                         build_assigner, build_sampler, multi_apply)
@@ -121,6 +123,7 @@ class YOLOXHead(BaseDenseHead, BBoxTestMixin):
             sampler_cfg = dict(type='PseudoSampler')
             self.sampler = build_sampler(sampler_cfg, context=self)
 
+        self.fp16_enabled = False
         self._init_layers()
 
     def _init_layers(self):
@@ -274,7 +277,8 @@ class YOLOXHead(BaseDenseHead, BBoxTestMixin):
         flatten_bboxes = self._bbox_decode(flatten_priors, flatten_bbox_preds)
 
         if rescale:
-            flatten_bboxes[..., :4] /= flatten_bboxes.new_tensor(scale_factors)
+            flatten_bboxes[..., :4] /= flatten_bboxes.new_tensor(
+                scale_factors).unsqueeze(1)
 
         result_list = []
         for img_id in range(len(img_metas)):
@@ -313,6 +317,7 @@ class YOLOXHead(BaseDenseHead, BBoxTestMixin):
             dets, keep = batched_nms(bboxes, scores, labels, cfg.nms)
             return dets, labels[keep]
 
+    @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'objectnesses'))
     def loss(self,
              cls_scores,
              bbox_preds,
diff --git a/mmdet/models/detectors/__init__.py b/mmdet/models/detectors/__init__.py
index 6111e07c..08fad546 100644
--- a/mmdet/models/detectors/__init__.py
+++ b/mmdet/models/detectors/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .atss import ATSS
 from .autoassign import AutoAssign
 from .base import BaseDetector
@@ -19,12 +20,15 @@ from .mask_rcnn import MaskRCNN
 from .mask_scoring_rcnn import MaskScoringRCNN
 from .nasfcos import NASFCOS
 from .paa import PAA
+from .panoptic_fpn import PanopticFPN
+from .panoptic_two_stage_segmentor import TwoStagePanopticSegmentor
 from .point_rend import PointRend
 from .reppoints_detector import RepPointsDetector
 from .retinanet import RetinaNet
 from .rpn import RPN
 from .scnet import SCNet
 from .single_stage import SingleStageDetector
+from .solo import SOLO
 from .sparse_rcnn import SparseRCNN
 from .trident_faster_rcnn import TridentFasterRCNN
 from .two_stage import TwoStageDetector
@@ -40,6 +44,7 @@ __all__ = [
     'MaskRCNN', 'CascadeRCNN', 'HybridTaskCascade', 'RetinaNet', 'FCOS',
     'GridRCNN', 'MaskScoringRCNN', 'RepPointsDetector', 'FOVEA', 'FSAF',
     'NASFCOS', 'PointRend', 'GFL', 'CornerNet', 'PAA', 'YOLOV3', 'YOLACT',
-    'VFNet', 'DETR', 'TridentFasterRCNN', 'SparseRCNN', 'SCNet',
-    'DeformableDETR', 'AutoAssign', 'YOLOF', 'CenterNet', 'YOLOX'
+    'VFNet', 'DETR', 'TridentFasterRCNN', 'SparseRCNN', 'SCNet', 'SOLO',
+    'DeformableDETR', 'AutoAssign', 'YOLOF', 'CenterNet', 'YOLOX',
+    'TwoStagePanopticSegmentor', 'PanopticFPN'
 ]
diff --git a/mmdet/models/detectors/atss.py b/mmdet/models/detectors/atss.py
index e28f457c..00f1acd9 100644
--- a/mmdet/models/detectors/atss.py
+++ b/mmdet/models/detectors/atss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/autoassign.py b/mmdet/models/detectors/autoassign.py
index 1bc03091..30ab7207 100644
--- a/mmdet/models/detectors/autoassign.py
+++ b/mmdet/models/detectors/autoassign.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/base.py b/mmdet/models/detectors/base.py
index c323a6b8..0f7b9f11 100644
--- a/mmdet/models/detectors/base.py
+++ b/mmdet/models/detectors/base.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 from collections import OrderedDict
 
diff --git a/mmdet/models/detectors/cascade_rcnn.py b/mmdet/models/detectors/cascade_rcnn.py
index 8a417895..d8c73827 100644
--- a/mmdet/models/detectors/cascade_rcnn.py
+++ b/mmdet/models/detectors/cascade_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/centernet.py b/mmdet/models/detectors/centernet.py
index f7c3ecd7..e1e3fd3c 100644
--- a/mmdet/models/detectors/centernet.py
+++ b/mmdet/models/detectors/centernet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import bbox2result
diff --git a/mmdet/models/detectors/cornernet.py b/mmdet/models/detectors/cornernet.py
index b6dc6033..ce921cc3 100644
--- a/mmdet/models/detectors/cornernet.py
+++ b/mmdet/models/detectors/cornernet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import bbox2result, bbox_mapping_back
diff --git a/mmdet/models/detectors/deformable_detr.py b/mmdet/models/detectors/deformable_detr.py
index 947550fb..b1f16422 100644
--- a/mmdet/models/detectors/deformable_detr.py
+++ b/mmdet/models/detectors/deformable_detr.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .detr import DETR
 
diff --git a/mmdet/models/detectors/detr.py b/mmdet/models/detectors/detr.py
index 0cedb692..06d76913 100644
--- a/mmdet/models/detectors/detr.py
+++ b/mmdet/models/detectors/detr.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch
diff --git a/mmdet/models/detectors/fast_rcnn.py b/mmdet/models/detectors/fast_rcnn.py
index 4dd56199..7aebe151 100644
--- a/mmdet/models/detectors/fast_rcnn.py
+++ b/mmdet/models/detectors/fast_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/faster_rcnn.py b/mmdet/models/detectors/faster_rcnn.py
index f6a7244d..70fb662f 100644
--- a/mmdet/models/detectors/faster_rcnn.py
+++ b/mmdet/models/detectors/faster_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/fcos.py b/mmdet/models/detectors/fcos.py
index df1d0bc5..d985bd02 100644
--- a/mmdet/models/detectors/fcos.py
+++ b/mmdet/models/detectors/fcos.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/fovea.py b/mmdet/models/detectors/fovea.py
index f7c75621..6fd908c7 100644
--- a/mmdet/models/detectors/fovea.py
+++ b/mmdet/models/detectors/fovea.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/fsaf.py b/mmdet/models/detectors/fsaf.py
index b859c729..81ed1bde 100644
--- a/mmdet/models/detectors/fsaf.py
+++ b/mmdet/models/detectors/fsaf.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/gfl.py b/mmdet/models/detectors/gfl.py
index 29bdb6b5..4628e2e7 100644
--- a/mmdet/models/detectors/gfl.py
+++ b/mmdet/models/detectors/gfl.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/grid_rcnn.py b/mmdet/models/detectors/grid_rcnn.py
index 1bd35947..bba7873b 100644
--- a/mmdet/models/detectors/grid_rcnn.py
+++ b/mmdet/models/detectors/grid_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/htc.py b/mmdet/models/detectors/htc.py
index d9efdf42..f7c95338 100644
--- a/mmdet/models/detectors/htc.py
+++ b/mmdet/models/detectors/htc.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .cascade_rcnn import CascadeRCNN
 
diff --git a/mmdet/models/detectors/kd_one_stage.py b/mmdet/models/detectors/kd_one_stage.py
index 671ec190..d7001950 100644
--- a/mmdet/models/detectors/kd_one_stage.py
+++ b/mmdet/models/detectors/kd_one_stage.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 from mmcv.runner import load_checkpoint
diff --git a/mmdet/models/detectors/mask_rcnn.py b/mmdet/models/detectors/mask_rcnn.py
index 29ea62d3..c68489f9 100644
--- a/mmdet/models/detectors/mask_rcnn.py
+++ b/mmdet/models/detectors/mask_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/mask_scoring_rcnn.py b/mmdet/models/detectors/mask_scoring_rcnn.py
index 86c6053c..5f55656f 100644
--- a/mmdet/models/detectors/mask_scoring_rcnn.py
+++ b/mmdet/models/detectors/mask_scoring_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/nasfcos.py b/mmdet/models/detectors/nasfcos.py
index 6f3446f3..a34c2280 100644
--- a/mmdet/models/detectors/nasfcos.py
+++ b/mmdet/models/detectors/nasfcos.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/paa.py b/mmdet/models/detectors/paa.py
index afc80590..f5cb8372 100644
--- a/mmdet/models/detectors/paa.py
+++ b/mmdet/models/detectors/paa.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/panoptic_fpn.py b/mmdet/models/detectors/panoptic_fpn.py
new file mode 100644
index 00000000..f8ac751f
--- /dev/null
+++ b/mmdet/models/detectors/panoptic_fpn.py
@@ -0,0 +1,34 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from ..builder import DETECTORS
+from .panoptic_two_stage_segmentor import TwoStagePanopticSegmentor
+
+
+@DETECTORS.register_module()
+class PanopticFPN(TwoStagePanopticSegmentor):
+    r"""Implementation of `Panoptic feature pyramid
+    networks <https://arxiv.org/pdf/1901.02446>`_"""
+
+    def __init__(
+            self,
+            backbone,
+            neck=None,
+            rpn_head=None,
+            roi_head=None,
+            train_cfg=None,
+            test_cfg=None,
+            pretrained=None,
+            init_cfg=None,
+            # for panoptic segmentation
+            semantic_head=None,
+            panoptic_fusion_head=None):
+        super(PanopticFPN, self).__init__(
+            backbone=backbone,
+            neck=neck,
+            rpn_head=rpn_head,
+            roi_head=roi_head,
+            train_cfg=train_cfg,
+            test_cfg=test_cfg,
+            pretrained=pretrained,
+            init_cfg=init_cfg,
+            semantic_head=semantic_head,
+            panoptic_fusion_head=panoptic_fusion_head)
diff --git a/mmdet/models/detectors/panoptic_two_stage_segmentor.py b/mmdet/models/detectors/panoptic_two_stage_segmentor.py
new file mode 100644
index 00000000..9b93e31e
--- /dev/null
+++ b/mmdet/models/detectors/panoptic_two_stage_segmentor.py
@@ -0,0 +1,203 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import torch
+
+from mmdet.core import bbox2roi, multiclass_nms
+from ..builder import DETECTORS, build_head
+from ..roi_heads.mask_heads.fcn_mask_head import _do_paste_mask
+from .two_stage import TwoStageDetector
+
+
+@DETECTORS.register_module()
+class TwoStagePanopticSegmentor(TwoStageDetector):
+    """Base class of Two-stage Panoptic Segmentor.
+
+    As well as the components in TwoStageDetector, Panoptic Segmentor has extra
+    semantic_head and panoptic_fusion_head.
+    """
+
+    def __init__(
+            self,
+            backbone,
+            neck=None,
+            rpn_head=None,
+            roi_head=None,
+            train_cfg=None,
+            test_cfg=None,
+            pretrained=None,
+            init_cfg=None,
+            # for panoptic segmentation
+            semantic_head=None,
+            panoptic_fusion_head=None):
+        super(TwoStagePanopticSegmentor,
+              self).__init__(backbone, neck, rpn_head, roi_head, train_cfg,
+                             test_cfg, pretrained, init_cfg)
+        if semantic_head is not None:
+            self.semantic_head = build_head(semantic_head)
+        if panoptic_fusion_head is not None:
+            panoptic_cfg = test_cfg.panoptic if test_cfg is not None else None
+            panoptic_fusion_head_ = panoptic_fusion_head.deepcopy()
+            panoptic_fusion_head_.update(test_cfg=panoptic_cfg)
+            self.panoptic_fusion_head = build_head(panoptic_fusion_head_)
+
+            self.num_things_classes = self.panoptic_fusion_head.\
+                num_things_classes
+            self.num_stuff_classes = self.panoptic_fusion_head.\
+                num_stuff_classes
+            self.num_classes = self.panoptic_fusion_head.num_classes
+
+    @property
+    def with_semantic_head(self):
+        return hasattr(self,
+                       'semantic_head') and self.semantic_head is not None
+
+    @property
+    def with_panoptic_fusion_head(self):
+        return hasattr(self, 'panoptic_fusion_heads') and \
+               self.panoptic_fusion_head is not None
+
+    def forward_dummy(self, img):
+        """Used for computing network flops.
+
+        See `mmdetection/tools/get_flops.py`
+        """
+        raise NotImplementedError(
+            f'`forward_dummy` is not implemented in {self.__class__.__name__}')
+
+    def forward_train(self,
+                      img,
+                      img_metas,
+                      gt_bboxes,
+                      gt_labels,
+                      gt_bboxes_ignore=None,
+                      gt_masks=None,
+                      gt_semantic_seg=None,
+                      proposals=None,
+                      **kwargs):
+        x = self.extract_feat(img)
+        losses = dict()
+
+        # RPN forward and loss
+        if self.with_rpn:
+            proposal_cfg = self.train_cfg.get('rpn_proposal',
+                                              self.test_cfg.rpn)
+            rpn_losses, proposal_list = self.rpn_head.forward_train(
+                x,
+                img_metas,
+                gt_bboxes,
+                gt_labels=None,
+                gt_bboxes_ignore=gt_bboxes_ignore,
+                proposal_cfg=proposal_cfg)
+            losses.update(rpn_losses)
+        else:
+            proposal_list = proposals
+
+        roi_losses = self.roi_head.forward_train(x, img_metas, proposal_list,
+                                                 gt_bboxes, gt_labels,
+                                                 gt_bboxes_ignore, gt_masks,
+                                                 **kwargs)
+        losses.update(roi_losses)
+
+        semantic_loss = self.semantic_head.forward_train(x, gt_semantic_seg)
+        losses.update(semantic_loss)
+
+        return losses
+
+    def simple_test_mask(self,
+                         x,
+                         img_metas,
+                         det_bboxes,
+                         det_labels,
+                         rescale=False):
+        """Simple test for mask head without augmentation."""
+        img_shapes = tuple(meta['ori_shape']
+                           for meta in img_metas) if rescale else tuple(
+                               meta['pad_shape'] for meta in img_metas)
+        scale_factors = tuple(meta['scale_factor'] for meta in img_metas)
+
+        if all(det_bbox.shape[0] == 0 for det_bbox in det_bboxes):
+            masks = []
+            for img_shape in img_shapes:
+                out_shape = (0, self.roi_head.bbox_head.num_classes) \
+                            + img_shape[:2]
+                masks.append(det_bboxes[0].new_zeros(out_shape))
+            mask_pred = det_bboxes[0].new_zeros((0, 80, 28, 28))
+            mask_results = dict(
+                masks=masks, mask_pred=mask_pred, mask_feats=None)
+            return mask_results
+
+        _bboxes = [det_bboxes[i][:, :4] for i in range(len(det_bboxes))]
+        if rescale:
+            if not isinstance(scale_factors[0], float):
+                scale_factors = [
+                    det_bboxes[0].new_tensor(scale_factor)
+                    for scale_factor in scale_factors
+                ]
+            _bboxes = [
+                _bboxes[i] * scale_factors[i] for i in range(len(_bboxes))
+            ]
+
+        mask_rois = bbox2roi(_bboxes)
+        mask_results = self.roi_head._mask_forward(x, mask_rois)
+        mask_pred = mask_results['mask_pred']
+        # split batch mask prediction back to each image
+        num_mask_roi_per_img = [len(det_bbox) for det_bbox in det_bboxes]
+        mask_preds = mask_pred.split(num_mask_roi_per_img, 0)
+
+        # resize the mask_preds to (K, H, W)
+        masks = []
+        for i in range(len(_bboxes)):
+            det_bbox = det_bboxes[i][:, :4]
+            det_label = det_labels[i]
+
+            mask_pred = mask_preds[i].sigmoid()
+
+            box_inds = torch.arange(mask_pred.shape[0])
+            mask_pred = mask_pred[box_inds, det_label][:, None]
+
+            img_h, img_w, _ = img_shapes[i]
+            mask_pred, _ = _do_paste_mask(
+                mask_pred, det_bbox, img_h, img_w, skip_empty=False)
+            masks.append(mask_pred)
+
+        mask_results['masks'] = masks
+
+        return mask_results
+
+    def simple_test(self, img, img_metas, proposals=None, rescale=False):
+        """Test without Augmentation."""
+        x = self.extract_feat(img)
+
+        if proposals is None:
+            proposal_list = self.rpn_head.simple_test_rpn(x, img_metas)
+        else:
+            proposal_list = proposals
+
+        bboxes, scores = self.roi_head.simple_test_bboxes(
+            x, img_metas, proposal_list, None, rescale=rescale)
+
+        pan_cfg = self.test_cfg.panoptic
+        # class-wise predictions
+        det_bboxes = []
+        det_labels = []
+        for bboxe, score in zip(bboxes, scores):
+            det_bbox, det_label = multiclass_nms(bboxe, score,
+                                                 pan_cfg.score_thr,
+                                                 pan_cfg.nms,
+                                                 pan_cfg.max_per_img)
+            det_bboxes.append(det_bbox)
+            det_labels.append(det_label)
+
+        mask_results = self.simple_test_mask(
+            x, img_metas, det_bboxes, det_labels, rescale=rescale)
+        masks = mask_results['masks']
+
+        seg_preds = self.semantic_head.simple_test(x, img_metas, rescale)
+
+        results = []
+        for i in range(len(det_bboxes)):
+            pan_results = self.panoptic_fusion_head.simple_test(
+                det_bboxes[i], det_labels[i], masks[i], seg_preds[i])
+            pan_results = pan_results.int().detach().cpu().numpy()
+            result = dict(pan_results=pan_results)
+            results.append(result)
+        return results
diff --git a/mmdet/models/detectors/point_rend.py b/mmdet/models/detectors/point_rend.py
index 72c4bacf..90eb4d40 100644
--- a/mmdet/models/detectors/point_rend.py
+++ b/mmdet/models/detectors/point_rend.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/reppoints_detector.py b/mmdet/models/detectors/reppoints_detector.py
index 3636a602..f1986cdc 100644
--- a/mmdet/models/detectors/reppoints_detector.py
+++ b/mmdet/models/detectors/reppoints_detector.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/retinanet.py b/mmdet/models/detectors/retinanet.py
index 6aa29f25..c28545ab 100644
--- a/mmdet/models/detectors/retinanet.py
+++ b/mmdet/models/detectors/retinanet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/rpn.py b/mmdet/models/detectors/rpn.py
index f92d9092..c70ede2b 100644
--- a/mmdet/models/detectors/rpn.py
+++ b/mmdet/models/detectors/rpn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import mmcv
@@ -151,4 +152,4 @@ class RPN(BaseDetector):
         Returns:
             np.ndarray: The image with bboxes drawn on it.
         """
-        mmcv.imshow_bboxes(data, result, top_k=top_k)
+        mmcv.imshow_bboxes(data, result, top_k=top_k, **kwargs)
diff --git a/mmdet/models/detectors/scnet.py b/mmdet/models/detectors/scnet.py
index 04a2347c..a361d81c 100644
--- a/mmdet/models/detectors/scnet.py
+++ b/mmdet/models/detectors/scnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .cascade_rcnn import CascadeRCNN
 
diff --git a/mmdet/models/detectors/single_stage.py b/mmdet/models/detectors/single_stage.py
index c0536009..f832ed61 100644
--- a/mmdet/models/detectors/single_stage.py
+++ b/mmdet/models/detectors/single_stage.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch
diff --git a/mmdet/models/detectors/single_stage_instance_seg.py b/mmdet/models/detectors/single_stage_instance_seg.py
new file mode 100644
index 00000000..9d9d6d28
--- /dev/null
+++ b/mmdet/models/detectors/single_stage_instance_seg.py
@@ -0,0 +1,363 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import copy
+import warnings
+
+import mmcv
+import numpy as np
+import torch
+
+from mmdet.core.visualization.image import imshow_det_bboxes
+from ..builder import DETECTORS, build_backbone, build_head, build_neck
+from .base import BaseDetector
+
+INF = 1e8
+
+
+@DETECTORS.register_module()
+class SingleStageInstanceSegmentor(BaseDetector):
+    """Base class for single-stage instance segmentors."""
+
+    def __init__(self,
+                 backbone,
+                 neck=None,
+                 bbox_head=None,
+                 mask_head=None,
+                 train_cfg=None,
+                 test_cfg=None,
+                 pretrained=None,
+                 init_cfg=None):
+
+        if pretrained:
+            warnings.warn('DeprecationWarning: pretrained is deprecated, '
+                          'please use "init_cfg" instead')
+            backbone.pretrained = pretrained
+        super(SingleStageInstanceSegmentor, self).__init__(init_cfg=init_cfg)
+        self.backbone = build_backbone(backbone)
+        if neck is not None:
+            self.neck = build_neck(neck)
+        else:
+            self.neck = None
+        if bbox_head is not None:
+            bbox_head.update(train_cfg=copy.deepcopy(train_cfg))
+            bbox_head.update(test_cfg=copy.deepcopy(test_cfg))
+            self.bbox_head = build_head(bbox_head)
+        else:
+            self.bbox_head = None
+
+        assert mask_head, f'`mask_head` must ' \
+                          f'be implemented in {self.__class__.__name__}'
+        mask_head.update(train_cfg=copy.deepcopy(train_cfg))
+        mask_head.update(test_cfg=copy.deepcopy(test_cfg))
+        self.mask_head = build_head(mask_head)
+
+        self.train_cfg = train_cfg
+        self.test_cfg = test_cfg
+
+    def extract_feat(self, img):
+        """Directly extract features from the backbone and neck."""
+        x = self.backbone(img)
+        if self.with_neck:
+            x = self.neck(x)
+        return x
+
+    def forward_dummy(self, img):
+        """Used for computing network flops.
+
+        See `mmdetection/tools/analysis_tools/get_flops.py`
+        """
+        raise NotImplementedError(
+            f'`forward_dummy` is not implemented in {self.__class__.__name__}')
+
+    def forward_train(self,
+                      img,
+                      img_metas,
+                      gt_masks,
+                      gt_labels,
+                      gt_bboxes=None,
+                      gt_bboxes_ignore=None,
+                      **kwargs):
+        """
+        Args:
+            img (Tensor): Input images of shape (B, C, H, W).
+                Typically these should be mean centered and std scaled.
+            img_metas (list[dict]): A List of image info dict where each dict
+                has: 'img_shape', 'scale_factor', 'flip', and may also contain
+                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.
+                For details on the values of these keys see
+                :class:`mmdet.datasets.pipelines.Collect`.
+            gt_masks (list[:obj:`BitmapMasks`] | None) : The segmentation
+                masks for each box.
+            gt_labels (list[Tensor]): Class indices corresponding to each box
+            gt_bboxes (list[Tensor]): Each item is the truth boxes
+                of each image in [tl_x, tl_y, br_x, br_y] format.
+                Default: None.
+            gt_bboxes_ignore (list[Tensor] | None): Specify which bounding
+                boxes can be ignored when computing the loss.
+
+        Returns:
+            dict[str, Tensor]: A dictionary of loss components.
+        """
+
+        gt_masks = [
+            gt_mask.to_tensor(dtype=torch.bool, device=img.device)
+            for gt_mask in gt_masks
+        ]
+        x = self.extract_feat(img)
+        losses = dict()
+
+        # CondInst and YOLACT have bbox_head
+        if self.bbox_head:
+            # bbox_head_preds is a tuple
+            bbox_head_preds = self.bbox_head(x)
+            # positive_infos is a list of obj:`InstanceData`
+            # It contains the information about the positive samples
+            # CondInst, YOLACT
+            det_losses, positive_infos = self.bbox_head.loss(
+                *bbox_head_preds,
+                gt_bboxes=gt_bboxes,
+                gt_labels=gt_labels,
+                gt_masks=gt_masks,
+                img_metas=img_metas,
+                gt_bboxes_ignore=gt_bboxes_ignore,
+                **kwargs)
+            losses.update(det_losses)
+        else:
+            positive_infos = None
+
+        mask_loss = self.mask_head.forward_train(
+            x,
+            gt_labels,
+            gt_masks,
+            img_metas,
+            positive_infos=positive_infos,
+            gt_bboxes=gt_bboxes,
+            gt_bboxes_ignore=gt_bboxes_ignore,
+            **kwargs)
+        # avoid loss override
+        assert not set(mask_loss.keys()) & set(losses.keys())
+
+        losses.update(mask_loss)
+        return losses
+
+    def simple_test(self, img, img_metas, rescale=False):
+        """Test function without test-time augmentation.
+
+        Args:
+            img (torch.Tensor): Images with shape (B, C, H, W).
+            img_metas (list[dict]): List of image information.
+            rescale (bool, optional): Whether to rescale the results.
+                Defaults to False.
+
+        Returns:
+            list(tuple): Formatted bbox and mask results of multiple \
+                images. The outer list corresponds to each image. \
+                Each tuple contains two type of results of single image:
+
+                - bbox_results (list[np.ndarray]): BBox results of
+                  single image. The list corresponds to each class.
+                  each ndarray has a shape (N, 5), N is the number of
+                  bboxes with this category, and last dimension
+                  5 arrange as (x1, y1, x2, y2, scores).
+                - mask_results (list[np.ndarray]): Mask results of
+                  single image. The list corresponds to each class.
+                  each ndarray has a shape (N, img_h, img_w), N
+                  is the number of masks with this category.
+        """
+        feat = self.extract_feat(img)
+        if self.bbox_head:
+            outs = self.bbox_head(feat)
+            # results_list is list[obj:`InstanceData`]
+            results_list = self.bbox_head.get_results(
+                *outs, img_metas=img_metas, cfg=self.test_cfg, rescale=rescale)
+        else:
+            results_list = None
+
+        results_list = self.mask_head.simple_test(
+            feat, img_metas, rescale=rescale, instances_list=results_list)
+
+        format_results_list = []
+        for results in results_list:
+            format_results_list.append(self.format_results(results))
+
+        return format_results_list
+
+    def format_results(self, results):
+        """Format the model predictions according to the interface with
+        dataset.
+
+        Args:
+            results (:obj:`InstanceData`): Processed
+                results of single images. Usually contains
+                following keys.
+
+                - scores (Tensor): Classification scores, has shape
+                  (num_instance,)
+                - labels (Tensor): Has shape (num_instances,).
+                - masks (Tensor): Processed mask results, has
+                  shape (num_instances, h, w).
+
+        Returns:
+            tuple: Formated bbox and mask results.. It contains two items:
+
+                - bbox_results (list[np.ndarray]): BBox results of
+                  single image. The list corresponds to each class.
+                  each ndarray has a shape (N, 5), N is the number of
+                  bboxes with this category, and last dimension
+                  5 arrange as (x1, y1, x2, y2, scores).
+                - mask_results (list[np.ndarray]): Mask results of
+                  single image. The list corresponds to each class.
+                  each ndarray has shape (N, img_h, img_w), N
+                  is the number of masks with this category.
+        """
+        data_keys = results.keys()
+        assert 'scores' in data_keys
+        assert 'labels' in data_keys
+
+        assert 'masks' in data_keys, \
+            'results should contain ' \
+            'masks when format the results '
+        mask_results = [[] for _ in range(self.mask_head.num_classes)]
+
+        num_masks = len(results)
+
+        if num_masks == 0:
+            bbox_results = [
+                np.zeros((0, 5), dtype=np.float32)
+                for _ in range(self.mask_head.num_classes)
+            ]
+            return bbox_results, mask_results
+
+        labels = results.labels.detach().cpu().numpy()
+
+        if 'bboxes' not in results:
+            # creat dummy bbox results to store the scores
+            results.bboxes = results.scores.new_zeros(len(results), 4)
+
+        det_bboxes = torch.cat([results.bboxes, results.scores[:, None]],
+                               dim=-1)
+        det_bboxes = det_bboxes.detach().cpu().numpy()
+        bbox_results = [
+            det_bboxes[labels == i, :]
+            for i in range(self.mask_head.num_classes)
+        ]
+
+        masks = results.masks.detach().cpu().numpy()
+
+        for idx in range(num_masks):
+            mask = masks[idx]
+            mask_results[labels[idx]].append(mask)
+
+        return bbox_results, mask_results
+
+    def aug_test(self, imgs, img_metas, rescale=False):
+        raise NotImplementedError
+
+    def show_result(self,
+                    img,
+                    result,
+                    score_thr=0.3,
+                    bbox_color=(72, 101, 241),
+                    text_color=(72, 101, 241),
+                    mask_color=None,
+                    thickness=2,
+                    font_size=13,
+                    win_name='',
+                    show=False,
+                    wait_time=0,
+                    out_file=None):
+        """Draw `result` over `img`.
+
+        Args:
+            img (str or Tensor): The image to be displayed.
+            result (tuple): Format bbox and mask results.
+                It contains two items:
+
+                - bbox_results (list[np.ndarray]): BBox results of
+                  single image. The list corresponds to each class.
+                  each ndarray has a shape (N, 5), N is the number of
+                  bboxes with this category, and last dimension
+                  5 arrange as (x1, y1, x2, y2, scores).
+                - mask_results (list[np.ndarray]): Mask results of
+                  single image. The list corresponds to each class.
+                  each ndarray has shape (N, img_h, img_w), N
+                  is the number of masks with this category.
+
+            score_thr (float, optional): Minimum score of bboxes to be shown.
+                Default: 0.3.
+            bbox_color (str or tuple(int) or :obj:`Color`):Color of bbox lines.
+               The tuple of color should be in BGR order. Default: 'green'
+            text_color (str or tuple(int) or :obj:`Color`):Color of texts.
+               The tuple of color should be in BGR order. Default: 'green'
+            mask_color (None or str or tuple(int) or :obj:`Color`):
+               Color of masks. The tuple of color should be in BGR order.
+               Default: None
+            thickness (int): Thickness of lines. Default: 2
+            font_size (int): Font size of texts. Default: 13
+            win_name (str): The window name. Default: ''
+            wait_time (float): Value of waitKey param.
+                Default: 0.
+            show (bool): Whether to show the image.
+                Default: False.
+            out_file (str or None): The filename to write the image.
+                Default: None.
+
+        Returns:
+            img (Tensor): Only if not `show` or `out_file`
+        """
+
+        assert isinstance(result, tuple)
+        bbox_result, mask_result = result
+        bboxes = np.vstack(bbox_result)
+        img = mmcv.imread(img)
+        img = img.copy()
+        labels = [
+            np.full(bbox.shape[0], i, dtype=np.int32)
+            for i, bbox in enumerate(bbox_result)
+        ]
+        labels = np.concatenate(labels)
+        if len(labels) == 0:
+            bboxes = np.zeros([0, 5])
+            masks = np.zeros([0, 0, 0])
+        # draw segmentation masks
+        else:
+            masks = mmcv.concat_list(mask_result)
+
+            if isinstance(masks[0], torch.Tensor):
+                masks = torch.stack(masks, dim=0).detach().cpu().numpy()
+            else:
+                masks = np.stack(masks, axis=0)
+            # dummy bboxes
+            if bboxes[:, :4].sum() == 0:
+                num_masks = len(bboxes)
+                x_any = masks.any(axis=1)
+                y_any = masks.any(axis=2)
+                for idx in range(num_masks):
+                    x = np.where(x_any[idx, :])[0]
+                    y = np.where(y_any[idx, :])[0]
+                    if len(x) > 0 and len(y) > 0:
+                        bboxes[idx, :4] = np.array(
+                            [x[0], y[0], x[-1] + 1, y[-1] + 1],
+                            dtype=np.float32)
+        # if out_file specified, do not show image in window
+        if out_file is not None:
+            show = False
+        # draw bounding boxes
+        img = imshow_det_bboxes(
+            img,
+            bboxes,
+            labels,
+            masks,
+            class_names=self.CLASSES,
+            score_thr=score_thr,
+            bbox_color=bbox_color,
+            text_color=text_color,
+            mask_color=mask_color,
+            thickness=thickness,
+            font_size=font_size,
+            win_name=win_name,
+            show=show,
+            wait_time=wait_time,
+            out_file=out_file)
+
+        if not (show or out_file):
+            return img
diff --git a/mmdet/models/detectors/solo.py b/mmdet/models/detectors/solo.py
new file mode 100644
index 00000000..9f45d314
--- /dev/null
+++ b/mmdet/models/detectors/solo.py
@@ -0,0 +1,29 @@
+from ..builder import DETECTORS
+from .single_stage_instance_seg import SingleStageInstanceSegmentor
+
+
+@DETECTORS.register_module()
+class SOLO(SingleStageInstanceSegmentor):
+    """`SOLO: Segmenting Objects by Locations
+    <https://arxiv.org/abs/1912.04488>`_
+
+    """
+
+    def __init__(self,
+                 backbone,
+                 neck=None,
+                 bbox_head=None,
+                 mask_head=None,
+                 train_cfg=None,
+                 test_cfg=None,
+                 init_cfg=None,
+                 pretrained=None):
+        super().__init__(
+            backbone=backbone,
+            neck=neck,
+            bbox_head=bbox_head,
+            mask_head=mask_head,
+            train_cfg=train_cfg,
+            test_cfg=test_cfg,
+            init_cfg=init_cfg,
+            pretrained=pretrained)
diff --git a/mmdet/models/detectors/sparse_rcnn.py b/mmdet/models/detectors/sparse_rcnn.py
index 0dbd0250..ae5d90e9 100644
--- a/mmdet/models/detectors/sparse_rcnn.py
+++ b/mmdet/models/detectors/sparse_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .two_stage import TwoStageDetector
 
diff --git a/mmdet/models/detectors/trident_faster_rcnn.py b/mmdet/models/detectors/trident_faster_rcnn.py
index c72065e5..fb26168c 100644
--- a/mmdet/models/detectors/trident_faster_rcnn.py
+++ b/mmdet/models/detectors/trident_faster_rcnn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .faster_rcnn import FasterRCNN
 
@@ -40,7 +41,8 @@ class TridentFasterRCNN(FasterRCNN):
             proposal_list = self.rpn_head.simple_test_rpn(x, trident_img_metas)
         else:
             proposal_list = proposals
-
+        # TODO： Fix trident_img_metas undefined errors
+        #  when proposals is specified
         return self.roi_head.simple_test(
             x, proposal_list, trident_img_metas, rescale=rescale)
 
diff --git a/mmdet/models/detectors/two_stage.py b/mmdet/models/detectors/two_stage.py
index 48b303b0..7b11f700 100644
--- a/mmdet/models/detectors/two_stage.py
+++ b/mmdet/models/detectors/two_stage.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch
diff --git a/mmdet/models/detectors/vfnet.py b/mmdet/models/detectors/vfnet.py
index cd34d714..38ddcdab 100644
--- a/mmdet/models/detectors/vfnet.py
+++ b/mmdet/models/detectors/vfnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/yolact.py b/mmdet/models/detectors/yolact.py
index bca776fc..3dfb7d8e 100644
--- a/mmdet/models/detectors/yolact.py
+++ b/mmdet/models/detectors/yolact.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import bbox2result
diff --git a/mmdet/models/detectors/yolo.py b/mmdet/models/detectors/yolo.py
index bd1f89e1..0ccd4177 100644
--- a/mmdet/models/detectors/yolo.py
+++ b/mmdet/models/detectors/yolo.py
@@ -1,4 +1,6 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # Copyright (c) 2019 Western Digital Corporation or its affiliates.
+import torch
 
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
@@ -17,3 +19,24 @@ class YOLOV3(SingleStageDetector):
                  init_cfg=None):
         super(YOLOV3, self).__init__(backbone, neck, bbox_head, train_cfg,
                                      test_cfg, pretrained, init_cfg)
+
+    def onnx_export(self, img, img_metas):
+        """Test function for exporting to ONNX, without test time augmentation.
+
+        Args:
+            img (torch.Tensor): input images.
+            img_metas (list[dict]): List of image information.
+
+        Returns:
+            tuple[Tensor, Tensor]: dets of shape [N, num_det, 5]
+                and class labels of shape [N, num_det].
+        """
+        x = self.extract_feat(img)
+        outs = self.bbox_head.forward(x)
+        # get shape as tensor
+        img_shape = torch._shape_as_tensor(img)[2:]
+        img_metas[0]['img_shape_for_onnx'] = img_shape
+
+        det_bboxes, det_labels = self.bbox_head.onnx_export(*outs, img_metas)
+
+        return det_bboxes, det_labels
diff --git a/mmdet/models/detectors/yolof.py b/mmdet/models/detectors/yolof.py
index dc7b3adf..6d08d16d 100644
--- a/mmdet/models/detectors/yolof.py
+++ b/mmdet/models/detectors/yolof.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/detectors/yolox.py b/mmdet/models/detectors/yolox.py
index 759a4eb9..6182175f 100644
--- a/mmdet/models/detectors/yolox.py
+++ b/mmdet/models/detectors/yolox.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import DETECTORS
 from .single_stage import SingleStageDetector
 
diff --git a/mmdet/models/losses/__init__.py b/mmdet/models/losses/__init__.py
index 3c64b1e2..068a54d6 100644
--- a/mmdet/models/losses/__init__.py
+++ b/mmdet/models/losses/__init__.py
@@ -1,8 +1,10 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .accuracy import Accuracy, accuracy
 from .ae_loss import AssociativeEmbeddingLoss
 from .balanced_l1_loss import BalancedL1Loss, balanced_l1_loss
 from .cross_entropy_loss import (CrossEntropyLoss, binary_cross_entropy,
                                  cross_entropy, mask_cross_entropy)
+from .dice_loss import DiceLoss
 from .focal_loss import FocalLoss, sigmoid_focal_loss
 from .gaussian_focal_loss import GaussianFocalLoss
 from .gfocal_loss import DistributionFocalLoss, QualityFocalLoss
@@ -26,5 +28,5 @@ __all__ = [
     'GHMR', 'reduce_loss', 'weight_reduce_loss', 'weighted_loss', 'L1Loss',
     'l1_loss', 'isr_p', 'carl_loss', 'AssociativeEmbeddingLoss',
     'GaussianFocalLoss', 'QualityFocalLoss', 'DistributionFocalLoss',
-    'VarifocalLoss', 'KnowledgeDistillationKLDivLoss', 'SeesawLoss'
+    'VarifocalLoss', 'KnowledgeDistillationKLDivLoss', 'SeesawLoss', 'DiceLoss'
 ]
diff --git a/mmdet/models/losses/accuracy.py b/mmdet/models/losses/accuracy.py
index 789a2240..fe765a39 100644
--- a/mmdet/models/losses/accuracy.py
+++ b/mmdet/models/losses/accuracy.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch.nn as nn
 
diff --git a/mmdet/models/losses/ae_loss.py b/mmdet/models/losses/ae_loss.py
index cff472aa..5c6da22a 100644
--- a/mmdet/models/losses/ae_loss.py
+++ b/mmdet/models/losses/ae_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 import torch.nn as nn
diff --git a/mmdet/models/losses/balanced_l1_loss.py b/mmdet/models/losses/balanced_l1_loss.py
index 7bcd13ff..8500345f 100644
--- a/mmdet/models/losses/balanced_l1_loss.py
+++ b/mmdet/models/losses/balanced_l1_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import numpy as np
 import torch
@@ -37,7 +38,10 @@ def balanced_l1_loss(pred,
         torch.Tensor: The calculated loss
     """
     assert beta > 0
-    assert pred.size() == target.size() and target.numel() > 0
+    if target.numel() == 0:
+        return pred.sum() * 0
+
+    assert pred.size() == target.size()
 
     diff = torch.abs(pred - target)
     b = np.e**(gamma / alpha) - 1
diff --git a/mmdet/models/losses/cross_entropy_loss.py b/mmdet/models/losses/cross_entropy_loss.py
index ec8f6aeb..f3aca80f 100644
--- a/mmdet/models/losses/cross_entropy_loss.py
+++ b/mmdet/models/losses/cross_entropy_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/losses/dice_loss.py b/mmdet/models/losses/dice_loss.py
new file mode 100644
index 00000000..0551d143
--- /dev/null
+++ b/mmdet/models/losses/dice_loss.py
@@ -0,0 +1,123 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import torch
+import torch.nn as nn
+
+from ..builder import LOSSES
+from .utils import weight_reduce_loss
+
+
+def dice_loss(pred,
+              target,
+              weight=None,
+              eps=1e-3,
+              reduction='mean',
+              avg_factor=None):
+    """Calculate dice loss, which is proposed in
+    `V-Net: Fully Convolutional Neural Networks for Volumetric
+    Medical Image Segmentation <https://arxiv.org/abs/1606.04797>`_.
+
+    Args:
+        pred (torch.Tensor): The prediction, has a shape (n, *)
+        target (torch.Tensor): The learning label of the prediction,
+            shape (n, *), same shape of pred.
+        weight (torch.Tensor, optional): The weight of loss for each
+            prediction, has a shape (n,). Defaults to None.
+        eps (float): Avoid dividing by zero. Default: 1e-3.
+        reduction (str, optional): The method used to reduce the loss into
+            a scalar. Defaults to 'mean'.
+            Options are "none", "mean" and "sum".
+        avg_factor (int, optional): Average factor that is used to average
+            the loss. Defaults to None.
+    """
+
+    input = pred.reshape(pred.size()[0], -1)
+    target = target.reshape(target.size()[0], -1).float()
+
+    a = torch.sum(input * target, 1)
+    b = torch.sum(input * input, 1) + eps
+    c = torch.sum(target * target, 1) + eps
+    d = (2 * a) / (b + c)
+    loss = 1 - d
+    if weight is not None:
+        assert weight.ndim == loss.ndim
+        assert len(weight) == len(pred)
+    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)
+    return loss
+
+
+@LOSSES.register_module()
+class DiceLoss(nn.Module):
+
+    def __init__(self,
+                 use_sigmoid=True,
+                 activate=True,
+                 reduction='mean',
+                 loss_weight=1.0,
+                 eps=1e-3):
+        """`Dice Loss, which is proposed in
+        `V-Net: Fully Convolutional Neural Networks for Volumetric
+         Medical Image Segmentation <https://arxiv.org/abs/1606.04797>`_.
+
+        Args:
+            use_sigmoid (bool, optional): Whether to the prediction is
+                used for sigmoid or softmax. Defaults to True.
+            activate (bool): Whether to activate the predictions inside,
+                this will disable the inside sigmoid operation.
+                Defaults to True.
+            reduction (str, optional): The method used
+                to reduce the loss. Options are "none",
+                "mean" and "sum". Defaults to 'mean'.
+            loss_weight (float, optional): Weight of loss. Defaults to 1.0.
+            eps (float): Avoid dividing by zero. Defaults to 1e-3.
+        """
+
+        super(DiceLoss, self).__init__()
+        self.use_sigmoid = use_sigmoid
+        self.reduction = reduction
+        self.loss_weight = loss_weight
+        self.eps = eps
+        self.activate = activate
+
+    def forward(self,
+                pred,
+                target,
+                weight=None,
+                reduction_override=None,
+                avg_factor=None):
+        """Forward function.
+
+        Args:
+            pred (torch.Tensor): The prediction, has a shape (n, *).
+            target (torch.Tensor): The label of the prediction,
+                shape (n, *), same shape of pred.
+            weight (torch.Tensor, optional): The weight of loss for each
+                prediction, has a shape (n,). Defaults to None.
+            avg_factor (int, optional): Average factor that is used to average
+                the loss. Defaults to None.
+            reduction_override (str, optional): The reduction method used to
+                override the original reduction method of the loss.
+                Options are "none", "mean" and "sum".
+
+        Returns:
+            torch.Tensor: The calculated loss
+        """
+
+        assert reduction_override in (None, 'none', 'mean', 'sum')
+        reduction = (
+            reduction_override if reduction_override else self.reduction)
+
+        if self.activate:
+            if self.use_sigmoid:
+                pred = pred.sigmoid()
+            else:
+                raise NotImplementedError
+
+        loss = self.loss_weight * dice_loss(
+            pred,
+            target,
+            weight,
+            eps=self.eps,
+            reduction=reduction,
+            avg_factor=avg_factor)
+
+        return loss
diff --git a/mmdet/models/losses/focal_loss.py b/mmdet/models/losses/focal_loss.py
index 493907c6..92909117 100644
--- a/mmdet/models/losses/focal_loss.py
+++ b/mmdet/models/losses/focal_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
@@ -82,8 +83,8 @@ def sigmoid_focal_loss(pred,
     """
     # Function.apply does not accept keyword arguments, so the decorator
     # "weighted_loss" is not applicable
-    loss = _sigmoid_focal_loss(pred.contiguous(), target, gamma, alpha, None,
-                               'none')
+    loss = _sigmoid_focal_loss(pred.contiguous(), target.contiguous(), gamma,
+                               alpha, None, 'none')
     if weight is not None:
         if weight.shape != loss.shape:
             if weight.size(0) == loss.size(0):
diff --git a/mmdet/models/losses/gaussian_focal_loss.py b/mmdet/models/losses/gaussian_focal_loss.py
index e45506a3..7abcb691 100644
--- a/mmdet/models/losses/gaussian_focal_loss.py
+++ b/mmdet/models/losses/gaussian_focal_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch.nn as nn
 
diff --git a/mmdet/models/losses/gfocal_loss.py b/mmdet/models/losses/gfocal_loss.py
index 9d3b8833..a7a1b765 100644
--- a/mmdet/models/losses/gfocal_loss.py
+++ b/mmdet/models/losses/gfocal_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/losses/ghm_loss.py b/mmdet/models/losses/ghm_loss.py
index e6626662..a4df9fe8 100644
--- a/mmdet/models/losses/ghm_loss.py
+++ b/mmdet/models/losses/ghm_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/losses/iou_loss.py b/mmdet/models/losses/iou_loss.py
index e6115155..bf1ed04e 100644
--- a/mmdet/models/losses/iou_loss.py
+++ b/mmdet/models/losses/iou_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 import warnings
 
@@ -87,8 +88,9 @@ def bounded_iou_loss(pred, target, beta=0.2, eps=1e-3):
                             (target_w + eps))
     loss_dh = 1 - torch.min(target_h / (pred_h + eps), pred_h /
                             (target_h + eps))
+    # view(..., -1) does not work for empty tensor
     loss_comb = torch.stack([loss_dx, loss_dy, loss_dw, loss_dh],
-                            dim=-1).view(loss_dx.size(0), -1)
+                            dim=-1).flatten(1)
 
     loss = torch.where(loss_comb < beta, 0.5 * loss_comb * loss_comb / beta,
                        loss_comb - 0.5 * beta)
@@ -226,9 +228,12 @@ def ciou_loss(pred, target, eps=1e-7):
     factor = 4 / math.pi**2
     v = factor * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)
 
+    with torch.no_grad():
+        alpha = (ious > 0.5).float() * v / (1 - ious + v)
+
     # CIoU
-    cious = ious - (rho2 / c2 + v**2 / (1 - ious + v))
-    loss = 1 - cious
+    cious = ious - (rho2 / c2 + alpha * v)
+    loss = 1 - cious.clamp(min=-1.0, max=1.0)
     return loss
 
 
diff --git a/mmdet/models/losses/kd_loss.py b/mmdet/models/losses/kd_loss.py
index f3abb68d..75c19355 100644
--- a/mmdet/models/losses/kd_loss.py
+++ b/mmdet/models/losses/kd_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/losses/mse_loss.py b/mmdet/models/losses/mse_loss.py
index 54b5db74..4a622f86 100644
--- a/mmdet/models/losses/mse_loss.py
+++ b/mmdet/models/losses/mse_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.nn.functional as F
 
diff --git a/mmdet/models/losses/pisa_loss.py b/mmdet/models/losses/pisa_loss.py
index 4a48adfc..6afea0e5 100644
--- a/mmdet/models/losses/pisa_loss.py
+++ b/mmdet/models/losses/pisa_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/mmdet/models/losses/seesaw_loss.py b/mmdet/models/losses/seesaw_loss.py
index 558cc9fd..01040472 100644
--- a/mmdet/models/losses/seesaw_loss.py
+++ b/mmdet/models/losses/seesaw_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/losses/smooth_l1_loss.py b/mmdet/models/losses/smooth_l1_loss.py
index ec9c98a5..55117467 100644
--- a/mmdet/models/losses/smooth_l1_loss.py
+++ b/mmdet/models/losses/smooth_l1_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 import torch.nn as nn
@@ -21,7 +22,10 @@ def smooth_l1_loss(pred, target, beta=1.0):
         torch.Tensor: Calculated loss
     """
     assert beta > 0
-    assert pred.size() == target.size() and target.numel() > 0
+    if target.numel() == 0:
+        return pred.sum() * 0
+
+    assert pred.size() == target.size()
     diff = torch.abs(pred - target)
     loss = torch.where(diff < beta, 0.5 * diff * diff / beta,
                        diff - 0.5 * beta)
@@ -40,7 +44,10 @@ def l1_loss(pred, target):
     Returns:
         torch.Tensor: Calculated loss
     """
-    assert pred.size() == target.size() and target.numel() > 0
+    if target.numel() == 0:
+        return pred.sum() * 0
+
+    assert pred.size() == target.size()
     loss = torch.abs(pred - target)
     return loss
 
diff --git a/mmdet/models/losses/utils.py b/mmdet/models/losses/utils.py
index 4756d7fc..686dfb6d 100644
--- a/mmdet/models/losses/utils.py
+++ b/mmdet/models/losses/utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import functools
 
 import mmcv
diff --git a/mmdet/models/losses/varifocal_loss.py b/mmdet/models/losses/varifocal_loss.py
index 7f00bd69..42f0eef9 100644
--- a/mmdet/models/losses/varifocal_loss.py
+++ b/mmdet/models/losses/varifocal_loss.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/necks/__init__.py b/mmdet/models/necks/__init__.py
index 9720b69d..fac8397c 100644
--- a/mmdet/models/necks/__init__.py
+++ b/mmdet/models/necks/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .bfp import BFP
 from .channel_mapper import ChannelMapper
 from .ct_resnet_neck import CTResNetNeck
diff --git a/mmdet/models/necks/bfp.py b/mmdet/models/necks/bfp.py
index 9f8ee072..9fdfa036 100644
--- a/mmdet/models/necks/bfp.py
+++ b/mmdet/models/necks/bfp.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
 from mmcv.cnn.bricks import NonLocal2d
diff --git a/mmdet/models/necks/channel_mapper.py b/mmdet/models/necks/channel_mapper.py
index 9c4c541f..774bdb1d 100644
--- a/mmdet/models/necks/channel_mapper.py
+++ b/mmdet/models/necks/channel_mapper.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmcv.runner import BaseModule
diff --git a/mmdet/models/necks/ct_resnet_neck.py b/mmdet/models/necks/ct_resnet_neck.py
index 899a6547..40eb2685 100644
--- a/mmdet/models/necks/ct_resnet_neck.py
+++ b/mmdet/models/necks/ct_resnet_neck.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch.nn as nn
diff --git a/mmdet/models/necks/dilated_encoder.py b/mmdet/models/necks/dilated_encoder.py
index e97d5ccc..6679835b 100644
--- a/mmdet/models/necks/dilated_encoder.py
+++ b/mmdet/models/necks/dilated_encoder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import (ConvModule, caffe2_xavier_init, constant_init, is_norm,
                       normal_init)
diff --git a/mmdet/models/necks/fpg.py b/mmdet/models/necks/fpg.py
index 2b65dba3..b301d921 100644
--- a/mmdet/models/necks/fpg.py
+++ b/mmdet/models/necks/fpg.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
diff --git a/mmdet/models/necks/fpn.py b/mmdet/models/necks/fpn.py
index 5644c615..2f065ed0 100644
--- a/mmdet/models/necks/fpn.py
+++ b/mmdet/models/necks/fpn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
diff --git a/mmdet/models/necks/fpn_carafe.py b/mmdet/models/necks/fpn_carafe.py
index ccc78ec3..160d2ce0 100644
--- a/mmdet/models/necks/fpn_carafe.py
+++ b/mmdet/models/necks/fpn_carafe.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule, build_upsample_layer, xavier_init
 from mmcv.ops.carafe import CARAFEPack
diff --git a/mmdet/models/necks/hrfpn.py b/mmdet/models/necks/hrfpn.py
index 135128fe..ca15be6b 100644
--- a/mmdet/models/necks/hrfpn.py
+++ b/mmdet/models/necks/hrfpn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/necks/nas_fpn.py b/mmdet/models/necks/nas_fpn.py
index fca3496e..f055b229 100644
--- a/mmdet/models/necks/nas_fpn.py
+++ b/mmdet/models/necks/nas_fpn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmcv.ops.merge_cells import GlobalPoolingCell, SumCell
diff --git a/mmdet/models/necks/nasfcos_fpn.py b/mmdet/models/necks/nasfcos_fpn.py
index 77a3ffd8..e6cdca51 100644
--- a/mmdet/models/necks/nasfcos_fpn.py
+++ b/mmdet/models/necks/nasfcos_fpn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule, caffe2_xavier_init
diff --git a/mmdet/models/necks/pafpn.py b/mmdet/models/necks/pafpn.py
index ba56ccae..8d5e32f0 100644
--- a/mmdet/models/necks/pafpn.py
+++ b/mmdet/models/necks/pafpn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
diff --git a/mmdet/models/necks/rfp.py b/mmdet/models/necks/rfp.py
index 200e2434..6976f4da 100644
--- a/mmdet/models/necks/rfp.py
+++ b/mmdet/models/necks/rfp.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/necks/ssd_neck.py b/mmdet/models/necks/ssd_neck.py
index 6ca11c2f..179d575e 100644
--- a/mmdet/models/necks/ssd_neck.py
+++ b/mmdet/models/necks/ssd_neck.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.cnn import ConvModule, DepthwiseSeparableConvModule
diff --git a/mmdet/models/necks/yolo_neck.py b/mmdet/models/necks/yolo_neck.py
index 999fb483..c8eeb573 100644
--- a/mmdet/models/necks/yolo_neck.py
+++ b/mmdet/models/necks/yolo_neck.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # Copyright (c) 2019 Western Digital Corporation or its affiliates.
 
 import torch
diff --git a/mmdet/models/necks/yolox_pafpn.py b/mmdet/models/necks/yolox_pafpn.py
index b35c8071..b0f6f706 100644
--- a/mmdet/models/necks/yolox_pafpn.py
+++ b/mmdet/models/necks/yolox_pafpn.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch
diff --git a/mmdet/models/plugins/__init__.py b/mmdet/models/plugins/__init__.py
index 5ca1eaab..a4368551 100644
--- a/mmdet/models/plugins/__init__.py
+++ b/mmdet/models/plugins/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .dropblock import DropBlock
 
 __all__ = ['DropBlock']
diff --git a/mmdet/models/plugins/dropblock.py b/mmdet/models/plugins/dropblock.py
index 25683449..bb00ade7 100644
--- a/mmdet/models/plugins/dropblock.py
+++ b/mmdet/models/plugins/dropblock.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
@@ -78,3 +79,7 @@ class DropBlock(nn.Module):
         factor = (1.0 if self.iter_cnt > self.warmup_iters else self.iter_cnt /
                   self.warmup_iters)
         return gamma * factor
+
+    def extra_repr(self):
+        return (f'drop_prob={self.drop_prob}, block_size={self.block_size}, '
+                f'warmup_iters={self.warmup_iters}')
diff --git a/mmdet/models/roi_heads/__init__.py b/mmdet/models/roi_heads/__init__.py
index c7c6ca2d..baae2a05 100644
--- a/mmdet/models/roi_heads/__init__.py
+++ b/mmdet/models/roi_heads/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .base_roi_head import BaseRoIHead
 from .bbox_heads import (BBoxHead, ConvFCBBoxHead, DIIHead,
                          DoubleConvFCBBoxHead, SABLHead, SCNetBBoxHead,
diff --git a/mmdet/models/roi_heads/base_roi_head.py b/mmdet/models/roi_heads/base_roi_head.py
index 423af25c..4adbdef8 100644
--- a/mmdet/models/roi_heads/base_roi_head.py
+++ b/mmdet/models/roi_heads/base_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 
 from mmcv.runner import BaseModule
diff --git a/mmdet/models/roi_heads/bbox_heads/__init__.py b/mmdet/models/roi_heads/bbox_heads/__init__.py
index bc5d29ec..d1207dbe 100644
--- a/mmdet/models/roi_heads/bbox_heads/__init__.py
+++ b/mmdet/models/roi_heads/bbox_heads/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .bbox_head import BBoxHead
 from .convfc_bbox_head import (ConvFCBBoxHead, Shared2FCBBoxHead,
                                Shared4Conv1FCBBoxHead)
diff --git a/mmdet/models/roi_heads/bbox_heads/bbox_head.py b/mmdet/models/roi_heads/bbox_heads/bbox_head.py
index 66188a47..6e0427e6 100644
--- a/mmdet/models/roi_heads/bbox_heads/bbox_head.py
+++ b/mmdet/models/roi_heads/bbox_heads/bbox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
@@ -107,8 +108,13 @@ class BBoxHead(BaseModule):
     @auto_fp16()
     def forward(self, x):
         if self.with_avg_pool:
-            x = self.avg_pool(x)
-        x = x.view(x.size(0), -1)
+            if x.numel() > 0:
+                x = self.avg_pool(x)
+                x = x.view(x.size(0), -1)
+            else:
+                # avg_pool does not support empty tensor,
+                # so use torch.mean instead it
+                x = torch.mean(x, dim=(-1, -2))
         cls_score = self.fc_cls(x) if self.with_cls else None
         bbox_pred = self.fc_reg(x) if self.with_reg else None
         return cls_score, bbox_pred
@@ -357,7 +363,6 @@ class BBoxHead(BaseModule):
                 bboxes[:, [1, 3]].clamp_(min=0, max=img_shape[0])
 
         if rescale and bboxes.size(0) > 0:
-
             scale_factor = bboxes.new_tensor(scale_factor)
             bboxes = (bboxes.view(bboxes.size(0), -1, 4) / scale_factor).view(
                 bboxes.size()[0], -1)
@@ -455,9 +460,15 @@ class BBoxHead(BaseModule):
         """Regress the bbox for the predicted class. Used in Cascade R-CNN.
 
         Args:
-            rois (Tensor): shape (n, 4) or (n, 5)
-            label (Tensor): shape (n, )
-            bbox_pred (Tensor): shape (n, 4*(#class)) or (n, 4)
+            rois (Tensor): Rois from `rpn_head` or last stage
+                `bbox_head`, has shape (num_proposals, 4) or
+                (num_proposals, 5).
+            label (Tensor): Only used when `self.reg_class_agnostic`
+                is False, has shape (num_proposals, ).
+            bbox_pred (Tensor): Regression prediction of
+                current stage `bbox_head`. When `self.reg_class_agnostic`
+                is False, it has shape (n, num_classes * 4), otherwise
+                it has shape (n, 4).
             img_meta (dict): Image meta info.
 
         Returns:
diff --git a/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py b/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py
index 6f9f5ec6..921a00b0 100644
--- a/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py
+++ b/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 
diff --git a/mmdet/models/roi_heads/bbox_heads/dii_head.py b/mmdet/models/roi_heads/bbox_heads/dii_head.py
index cf708eb0..0bb899c5 100644
--- a/mmdet/models/roi_heads/bbox_heads/dii_head.py
+++ b/mmdet/models/roi_heads/bbox_heads/dii_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.cnn import (bias_init_with_prob, build_activation_layer,
diff --git a/mmdet/models/roi_heads/bbox_heads/double_bbox_head.py b/mmdet/models/roi_heads/bbox_heads/double_bbox_head.py
index 26687e04..2a38d591 100644
--- a/mmdet/models/roi_heads/bbox_heads/double_bbox_head.py
+++ b/mmdet/models/roi_heads/bbox_heads/double_bbox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmcv.runner import BaseModule, ModuleList
diff --git a/mmdet/models/roi_heads/bbox_heads/sabl_head.py b/mmdet/models/roi_heads/bbox_heads/sabl_head.py
index 07c542ef..bf5a1e2c 100644
--- a/mmdet/models/roi_heads/bbox_heads/sabl_head.py
+++ b/mmdet/models/roi_heads/bbox_heads/sabl_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
diff --git a/mmdet/models/roi_heads/bbox_heads/scnet_bbox_head.py b/mmdet/models/roi_heads/bbox_heads/scnet_bbox_head.py
index 35758f4f..cf39ebef 100644
--- a/mmdet/models/roi_heads/bbox_heads/scnet_bbox_head.py
+++ b/mmdet/models/roi_heads/bbox_heads/scnet_bbox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmdet.models.builder import HEADS
 from .convfc_bbox_head import ConvFCBBoxHead
 
diff --git a/mmdet/models/roi_heads/cascade_roi_head.py b/mmdet/models/roi_heads/cascade_roi_head.py
index 1529801a..e17313f2 100644
--- a/mmdet/models/roi_heads/cascade_roi_head.py
+++ b/mmdet/models/roi_heads/cascade_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
@@ -270,6 +271,11 @@ class CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):
                     if self.bbox_head[i].custom_activation:
                         cls_score = self.bbox_head[i].loss_cls.get_activation(
                             cls_score)
+
+                    # Empty proposal.
+                    if cls_score.numel() == 0:
+                        break
+
                     roi_labels = torch.where(
                         roi_labels == self.bbox_head[i].num_classes,
                         cls_score[:, :-1].argmax(1), roi_labels)
@@ -280,7 +286,28 @@ class CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):
         return losses
 
     def simple_test(self, x, proposal_list, img_metas, rescale=False):
-        """Test without augmentation."""
+        """Test without augmentation.
+
+        Args:
+            x (tuple[Tensor]): Features from upstream network. Each
+                has shape (batch_size, c, h, w).
+            proposal_list (list(Tensor)): Proposals from rpn head.
+                Each has shape (num_proposals, 5), last dimension
+                5 represent (x1, y1, x2, y2, score).
+            img_metas (list[dict]): Meta information of images.
+            rescale (bool): Whether to rescale the results to
+                the original image. Default: True.
+
+        Returns:
+            list[list[np.ndarray]] or list[tuple]: When no mask branch,
+            it is bbox results of each image and classes with type
+            `list[list[np.ndarray]]`. The outer list
+            corresponds to each image. The inner list
+            corresponds to each class. When the model has mask branch,
+            it contains bbox results and mask results.
+            The outer list corresponds to each image, and first element
+            of tuple is bbox results, second element is mask results.
+        """
         assert self.with_bbox, 'Bbox head must be implemented.'
         num_imgs = len(proposal_list)
         img_shapes = tuple(meta['img_shape'] for meta in img_metas)
@@ -340,7 +367,7 @@ class CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):
                     if rois[j].shape[0] > 0:
                         bbox_label = cls_score[j][:, :-1].argmax(dim=1)
                         refined_rois = self.bbox_head[i].regress_by_class(
-                            rois[j], bbox_label[j], bbox_pred[j], img_metas[j])
+                            rois[j], bbox_label, bbox_pred[j], img_metas[j])
                         refine_rois_list.append(refined_rois)
                 rois = torch.cat(refine_rois_list)
 
diff --git a/mmdet/models/roi_heads/double_roi_head.py b/mmdet/models/roi_heads/double_roi_head.py
index a1aa6c82..895b5d30 100644
--- a/mmdet/models/roi_heads/double_roi_head.py
+++ b/mmdet/models/roi_heads/double_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from ..builder import HEADS
 from .standard_roi_head import StandardRoIHead
 
diff --git a/mmdet/models/roi_heads/dynamic_roi_head.py b/mmdet/models/roi_heads/dynamic_roi_head.py
index 89427a93..4c2b6cda 100644
--- a/mmdet/models/roi_heads/dynamic_roi_head.py
+++ b/mmdet/models/roi_heads/dynamic_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
diff --git a/mmdet/models/roi_heads/grid_roi_head.py b/mmdet/models/roi_heads/grid_roi_head.py
index 0332b418..333f6297 100644
--- a/mmdet/models/roi_heads/grid_roi_head.py
+++ b/mmdet/models/roi_heads/grid_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
diff --git a/mmdet/models/roi_heads/htc_roi_head.py b/mmdet/models/roi_heads/htc_roi_head.py
index a9b2624c..08bc1dbf 100644
--- a/mmdet/models/roi_heads/htc_roi_head.py
+++ b/mmdet/models/roi_heads/htc_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn.functional as F
@@ -5,6 +6,7 @@ import torch.nn.functional as F
 from mmdet.core import (bbox2result, bbox2roi, bbox_mapping, merge_aug_bboxes,
                         merge_aug_masks, multiclass_nms)
 from ..builder import HEADS, build_head, build_roi_extractor
+from ..utils.brick_wrappers import adaptive_avg_pool2d
 from .cascade_roi_head import CascadeRoIHead
 
 
@@ -163,7 +165,7 @@ class HybridTaskCascadeRoIHead(CascadeRoIHead):
             bbox_semantic_feat = self.semantic_roi_extractor([semantic_feat],
                                                              rois)
             if bbox_semantic_feat.shape[-2:] != bbox_feats.shape[-2:]:
-                bbox_semantic_feat = F.adaptive_avg_pool2d(
+                bbox_semantic_feat = adaptive_avg_pool2d(
                     bbox_semantic_feat, bbox_feats.shape[-2:])
             bbox_feats += bbox_semantic_feat
         cls_score, bbox_pred = bbox_head(bbox_feats)
@@ -326,7 +328,28 @@ class HybridTaskCascadeRoIHead(CascadeRoIHead):
         return losses
 
     def simple_test(self, x, proposal_list, img_metas, rescale=False):
-        """Test without augmentation."""
+        """Test without augmentation.
+
+        Args:
+            x (tuple[Tensor]): Features from upstream network. Each
+                has shape (batch_size, c, h, w).
+            proposal_list (list(Tensor)): Proposals from rpn head.
+                Each has shape (num_proposals, 5), last dimension
+                5 represent (x1, y1, x2, y2, score).
+            img_metas (list[dict]): Meta information of images.
+            rescale (bool): Whether to rescale the results to
+                the original image. Default: True.
+
+        Returns:
+            list[list[np.ndarray]] or list[tuple]: When no mask branch,
+            it is bbox results of each image and classes with type
+            `list[list[np.ndarray]]`. The outer list
+            corresponds to each image. The inner list
+            corresponds to each class. When the model has mask branch,
+            it contains bbox results and mask results.
+            The outer list corresponds to each image, and first element
+            of tuple is bbox results, second element is mask results.
+        """
         if self.with_semantic:
             _, semantic_feat = self.semantic_head(x)
         else:
@@ -381,7 +404,7 @@ class HybridTaskCascadeRoIHead(CascadeRoIHead):
                     if rois[j].shape[0] > 0:
                         bbox_label = cls_score[j][:, :-1].argmax(dim=1)
                         refine_rois = bbox_head.regress_by_class(
-                            rois[j], bbox_label[j], bbox_pred[j], img_metas[j])
+                            rois[j], bbox_label, bbox_pred[j], img_metas[j])
                         refine_rois_list.append(refine_rois)
                 rois = torch.cat(refine_rois_list)
 
@@ -552,9 +575,8 @@ class HybridTaskCascadeRoIHead(CascadeRoIHead):
 
         if self.with_mask:
             if det_bboxes.shape[0] == 0:
-                segm_result = [[[]
-                                for _ in range(self.mask_head[-1].num_classes)]
-                               ]
+                segm_result = [[]
+                               for _ in range(self.mask_head[-1].num_classes)]
             else:
                 aug_masks = []
                 aug_img_metas = []
diff --git a/mmdet/models/roi_heads/mask_heads/__init__.py b/mmdet/models/roi_heads/mask_heads/__init__.py
index abfbe262..a9ab9b5a 100644
--- a/mmdet/models/roi_heads/mask_heads/__init__.py
+++ b/mmdet/models/roi_heads/mask_heads/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .coarse_mask_head import CoarseMaskHead
 from .fcn_mask_head import FCNMaskHead
 from .feature_relay_head import FeatureRelayHead
diff --git a/mmdet/models/roi_heads/mask_heads/coarse_mask_head.py b/mmdet/models/roi_heads/mask_heads/coarse_mask_head.py
index e58cae1b..946254cb 100644
--- a/mmdet/models/roi_heads/mask_heads/coarse_mask_head.py
+++ b/mmdet/models/roi_heads/mask_heads/coarse_mask_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.cnn import ConvModule, Linear
 from mmcv.runner import ModuleList, auto_fp16
 
diff --git a/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py b/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
index b9c829a5..8874cad1 100644
--- a/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
+++ b/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from warnings import warn
 
 import numpy as np
diff --git a/mmdet/models/roi_heads/mask_heads/feature_relay_head.py b/mmdet/models/roi_heads/mask_heads/feature_relay_head.py
index b4cd3820..452f37af 100644
--- a/mmdet/models/roi_heads/mask_heads/feature_relay_head.py
+++ b/mmdet/models/roi_heads/mask_heads/feature_relay_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.runner import BaseModule, auto_fp16
 
diff --git a/mmdet/models/roi_heads/mask_heads/fused_semantic_head.py b/mmdet/models/roi_heads/mask_heads/fused_semantic_head.py
index 85a64fb6..8494f7e4 100644
--- a/mmdet/models/roi_heads/mask_heads/fused_semantic_head.py
+++ b/mmdet/models/roi_heads/mask_heads/fused_semantic_head.py
@@ -1,9 +1,12 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import warnings
+
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
 from mmcv.runner import BaseModule, auto_fp16, force_fp32
 
-from mmdet.models.builder import HEADS
+from mmdet.models.builder import HEADS, build_loss
 
 
 @HEADS.register_module()
@@ -30,10 +33,14 @@ class FusedSemanticHead(BaseModule):
                  in_channels=256,
                  conv_out_channels=256,
                  num_classes=183,
-                 ignore_label=255,
-                 loss_weight=0.2,
                  conv_cfg=None,
                  norm_cfg=None,
+                 ignore_label=None,
+                 loss_weight=None,
+                 loss_seg=dict(
+                     type='CrossEntropyLoss',
+                     ignore_index=255,
+                     loss_weight=0.2),
                  init_cfg=dict(
                      type='Kaiming', override=dict(name='conv_logits'))):
         super(FusedSemanticHead, self).__init__(init_cfg)
@@ -43,8 +50,6 @@ class FusedSemanticHead(BaseModule):
         self.in_channels = in_channels
         self.conv_out_channels = conv_out_channels
         self.num_classes = num_classes
-        self.ignore_label = ignore_label
-        self.loss_weight = loss_weight
         self.conv_cfg = conv_cfg
         self.norm_cfg = norm_cfg
         self.fp16_enabled = False
@@ -78,8 +83,15 @@ class FusedSemanticHead(BaseModule):
             conv_cfg=self.conv_cfg,
             norm_cfg=self.norm_cfg)
         self.conv_logits = nn.Conv2d(conv_out_channels, self.num_classes, 1)
-
-        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_label)
+        if ignore_label:
+            loss_seg['ignore_index'] = ignore_label
+        if loss_weight:
+            loss_seg['loss_weight'] = loss_weight
+        if ignore_label or loss_weight:
+            warnings.warn('``ignore_label`` and ``loss_weight`` would be '
+                          'deprecated soon. Please set ``ingore_index`` and '
+                          '``loss_weight`` in ``loss_seg`` instead.')
+        self.criterion = build_loss(loss_seg)
 
     @auto_fp16()
     def forward(self, feats):
@@ -102,5 +114,4 @@ class FusedSemanticHead(BaseModule):
     def loss(self, mask_pred, labels):
         labels = labels.squeeze(1).long()
         loss_semantic_seg = self.criterion(mask_pred, labels)
-        loss_semantic_seg *= self.loss_weight
         return loss_semantic_seg
diff --git a/mmdet/models/roi_heads/mask_heads/global_context_head.py b/mmdet/models/roi_heads/mask_heads/global_context_head.py
index 4f619940..af76a174 100644
--- a/mmdet/models/roi_heads/mask_heads/global_context_head.py
+++ b/mmdet/models/roi_heads/mask_heads/global_context_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmcv.runner import BaseModule, auto_fp16, force_fp32
diff --git a/mmdet/models/roi_heads/mask_heads/grid_head.py b/mmdet/models/roi_heads/mask_heads/grid_head.py
index 2d6ef67d..0c0702d2 100644
--- a/mmdet/models/roi_heads/mask_heads/grid_head.py
+++ b/mmdet/models/roi_heads/mask_heads/grid_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
diff --git a/mmdet/models/roi_heads/mask_heads/htc_mask_head.py b/mmdet/models/roi_heads/mask_heads/htc_mask_head.py
index 0f435ecf..7ad8592b 100644
--- a/mmdet/models/roi_heads/mask_heads/htc_mask_head.py
+++ b/mmdet/models/roi_heads/mask_heads/htc_mask_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.cnn import ConvModule
 
 from mmdet.models.builder import HEADS
diff --git a/mmdet/models/roi_heads/mask_heads/mask_point_head.py b/mmdet/models/roi_heads/mask_heads/mask_point_head.py
index 16ed8b60..120b8ffa 100644
--- a/mmdet/models/roi_heads/mask_heads/mask_point_head.py
+++ b/mmdet/models/roi_heads/mask_heads/mask_point_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # Modified from https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend/point_head/point_head.py  # noqa
 
 import torch
diff --git a/mmdet/models/roi_heads/mask_heads/maskiou_head.py b/mmdet/models/roi_heads/mask_heads/maskiou_head.py
index fc117ff7..a7ff7c7c 100644
--- a/mmdet/models/roi_heads/mask_heads/maskiou_head.py
+++ b/mmdet/models/roi_heads/mask_heads/maskiou_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn as nn
diff --git a/mmdet/models/roi_heads/mask_heads/scnet_mask_head.py b/mmdet/models/roi_heads/mask_heads/scnet_mask_head.py
index 983a2d9d..ca624866 100644
--- a/mmdet/models/roi_heads/mask_heads/scnet_mask_head.py
+++ b/mmdet/models/roi_heads/mask_heads/scnet_mask_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmdet.models.builder import HEADS
 from mmdet.models.utils import ResLayer, SimplifiedBasicBlock
 from .fcn_mask_head import FCNMaskHead
diff --git a/mmdet/models/roi_heads/mask_heads/scnet_semantic_head.py b/mmdet/models/roi_heads/mask_heads/scnet_semantic_head.py
index df85a011..2b8c5c32 100644
--- a/mmdet/models/roi_heads/mask_heads/scnet_semantic_head.py
+++ b/mmdet/models/roi_heads/mask_heads/scnet_semantic_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmdet.models.builder import HEADS
 from mmdet.models.utils import ResLayer, SimplifiedBasicBlock
 from .fused_semantic_head import FusedSemanticHead
diff --git a/mmdet/models/roi_heads/mask_scoring_roi_head.py b/mmdet/models/roi_heads/mask_scoring_roi_head.py
index e12700cd..4617988e 100644
--- a/mmdet/models/roi_heads/mask_scoring_roi_head.py
+++ b/mmdet/models/roi_heads/mask_scoring_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import bbox2roi
diff --git a/mmdet/models/roi_heads/pisa_roi_head.py b/mmdet/models/roi_heads/pisa_roi_head.py
index e0111362..92a51186 100644
--- a/mmdet/models/roi_heads/pisa_roi_head.py
+++ b/mmdet/models/roi_heads/pisa_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmdet.core import bbox2roi
 from ..builder import HEADS
 from ..losses.pisa_loss import carl_loss, isr_p
diff --git a/mmdet/models/roi_heads/point_rend_roi_head.py b/mmdet/models/roi_heads/point_rend_roi_head.py
index c8e73d57..9f667793 100644
--- a/mmdet/models/roi_heads/point_rend_roi_head.py
+++ b/mmdet/models/roi_heads/point_rend_roi_head.py
@@ -1,6 +1,7 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 # Modified from https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend  # noqa
-import logging
 import os
+import warnings
 
 import numpy as np
 import torch
@@ -12,8 +13,6 @@ from .. import builder
 from ..builder import HEADS
 from .standard_roi_head import StandardRoIHead
 
-logger = logging.getLogger(__name__)
-
 
 @HEADS.register_module()
 class PointRendRoIHead(StandardRoIHead):
@@ -164,7 +163,7 @@ class PointRendRoIHead(StandardRoIHead):
         scale_factors = tuple(meta['scale_factor'] for meta in img_metas)
 
         if isinstance(scale_factors[0], float):
-            logger.warning(
+            warnings.warn(
                 'Scale factor in img_metas should be a '
                 'ndarray with shape (4,) '
                 'arrange as (factor_w, factor_h, factor_w, factor_h), '
diff --git a/mmdet/models/roi_heads/roi_extractors/__init__.py b/mmdet/models/roi_heads/roi_extractors/__init__.py
index 59e2d6d2..0f602149 100644
--- a/mmdet/models/roi_heads/roi_extractors/__init__.py
+++ b/mmdet/models/roi_heads/roi_extractors/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .base_roi_extractor import BaseRoIExtractor
 from .generic_roi_extractor import GenericRoIExtractor
 from .single_level_roi_extractor import SingleRoIExtractor
diff --git a/mmdet/models/roi_heads/roi_extractors/base_roi_extractor.py b/mmdet/models/roi_heads/roi_extractors/base_roi_extractor.py
index 704ccf20..82629757 100644
--- a/mmdet/models/roi_heads/roi_extractors/base_roi_extractor.py
+++ b/mmdet/models/roi_heads/roi_extractors/base_roi_extractor.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
 
 import torch
diff --git a/mmdet/models/roi_heads/roi_extractors/generic_roi_extractor.py b/mmdet/models/roi_heads/roi_extractors/generic_roi_extractor.py
index 80c25bb8..566d3de8 100644
--- a/mmdet/models/roi_heads/roi_extractors/generic_roi_extractor.py
+++ b/mmdet/models/roi_heads/roi_extractors/generic_roi_extractor.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.cnn.bricks import build_plugin_layer
 from mmcv.runner import force_fp32
 
diff --git a/mmdet/models/roi_heads/roi_extractors/single_level_roi_extractor.py b/mmdet/models/roi_heads/roi_extractors/single_level_roi_extractor.py
index 6c7c1d54..1b569ce1 100644
--- a/mmdet/models/roi_heads/roi_extractors/single_level_roi_extractor.py
+++ b/mmdet/models/roi_heads/roi_extractors/single_level_roi_extractor.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmcv.runner import force_fp32
 
diff --git a/mmdet/models/roi_heads/scnet_roi_head.py b/mmdet/models/roi_heads/scnet_roi_head.py
index c231e1a6..705430a2 100644
--- a/mmdet/models/roi_heads/scnet_roi_head.py
+++ b/mmdet/models/roi_heads/scnet_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 import torch.nn.functional as F
@@ -5,6 +6,7 @@ import torch.nn.functional as F
 from mmdet.core import (bbox2result, bbox2roi, bbox_mapping, merge_aug_bboxes,
                         merge_aug_masks, multiclass_nms)
 from ..builder import HEADS, build_head, build_roi_extractor
+from ..utils.brick_wrappers import adaptive_avg_pool2d
 from .cascade_roi_head import CascadeRoIHead
 
 
@@ -106,7 +108,7 @@ class SCNetRoIHead(CascadeRoIHead):
             bbox_semantic_feat = self.semantic_roi_extractor([semantic_feat],
                                                              rois)
             if bbox_semantic_feat.shape[-2:] != bbox_feats.shape[-2:]:
-                bbox_semantic_feat = F.adaptive_avg_pool2d(
+                bbox_semantic_feat = adaptive_avg_pool2d(
                     bbox_semantic_feat, bbox_feats.shape[-2:])
             bbox_feats += bbox_semantic_feat
         if self.with_glbctx and glbctx_feat is not None:
@@ -213,26 +215,19 @@ class SCNetRoIHead(CascadeRoIHead):
         """
         Args:
             x (list[Tensor]): list of multi-level img features.
-
             img_metas (list[dict]): list of image info dict where each dict
                 has: 'img_shape', 'scale_factor', 'flip', and may also contain
                 'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.
                 For details on the values of these keys see
                 `mmdet/datasets/pipelines/formatting.py:Collect`.
-
             proposal_list (list[Tensors]): list of region proposals.
-
             gt_bboxes (list[Tensor]): Ground truth bboxes for each image with
                 shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
-
             gt_labels (list[Tensor]): class indices corresponding to each box
-
             gt_bboxes_ignore (None, list[Tensor]): specify which bounding
                 boxes can be ignored when computing the loss.
-
             gt_masks (None, Tensor) : true segmentation masks for each box
                 used if the architecture supports a segmentation task.
-
             gt_semantic_seg (None, list[Tensor]): semantic segmentation masks
                 used if the architecture supports semantic segmentation task.
 
@@ -317,7 +312,28 @@ class SCNetRoIHead(CascadeRoIHead):
         return losses
 
     def simple_test(self, x, proposal_list, img_metas, rescale=False):
-        """Test without augmentation."""
+        """Test without augmentation.
+
+        Args:
+            x (tuple[Tensor]): Features from upstream network. Each
+                has shape (batch_size, c, h, w).
+            proposal_list (list(Tensor)): Proposals from rpn head.
+                Each has shape (num_proposals, 5), last dimension
+                5 represent (x1, y1, x2, y2, score).
+            img_metas (list[dict]): Meta information of images.
+            rescale (bool): Whether to rescale the results to
+                the original image. Default: True.
+
+        Returns:
+            list[list[np.ndarray]] or list[tuple]: When no mask branch,
+            it is bbox results of each image and classes with type
+            `list[list[np.ndarray]]`. The outer list
+            corresponds to each image. The inner list
+            corresponds to each class. When the model has mask branch,
+            it contains bbox results and mask results.
+            The outer list corresponds to each image, and first element
+            of tuple is bbox results, second element is mask results.
+        """
         if self.with_semantic:
             _, semantic_feat = self.semantic_head(x)
         else:
@@ -379,7 +395,7 @@ class SCNetRoIHead(CascadeRoIHead):
                     if rois[j].shape[0] > 0:
                         bbox_label = cls_score[j][:, :-1].argmax(dim=1)
                         refine_rois = bbox_head.regress_by_class(
-                            rois[j], bbox_label[j], bbox_pred[j], img_metas[j])
+                            rois[j], bbox_label, bbox_pred[j], img_metas[j])
                         refine_rois_list.append(refine_rois)
                 rois = torch.cat(refine_rois_list)
 
diff --git a/mmdet/models/roi_heads/shared_heads/__init__.py b/mmdet/models/roi_heads/shared_heads/__init__.py
index bbe70145..d56636ab 100644
--- a/mmdet/models/roi_heads/shared_heads/__init__.py
+++ b/mmdet/models/roi_heads/shared_heads/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .res_layer import ResLayer
 
 __all__ = ['ResLayer']
diff --git a/mmdet/models/roi_heads/shared_heads/res_layer.py b/mmdet/models/roi_heads/shared_heads/res_layer.py
index 01d6cb7f..bef00a05 100644
--- a/mmdet/models/roi_heads/shared_heads/res_layer.py
+++ b/mmdet/models/roi_heads/shared_heads/res_layer.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 import torch.nn as nn
@@ -48,7 +49,7 @@ class ResLayer(BaseModule):
         self.add_module(f'layer{stage + 1}', res_layer)
 
         assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be setting at the same time'
+            'init_cfg and pretrained cannot be specified at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is a deprecated, '
                           'please use "init_cfg" instead')
diff --git a/mmdet/models/roi_heads/sparse_roi_head.py b/mmdet/models/roi_heads/sparse_roi_head.py
index 01c81a66..8c3c75de 100644
--- a/mmdet/models/roi_heads/sparse_roi_head.py
+++ b/mmdet/models/roi_heads/sparse_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
diff --git a/mmdet/models/roi_heads/standard_roi_head.py b/mmdet/models/roi_heads/standard_roi_head.py
index 1d0288f6..9fdb19d2 100644
--- a/mmdet/models/roi_heads/standard_roi_head.py
+++ b/mmdet/models/roi_heads/standard_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import bbox2result, bbox2roi, build_assigner, build_sampler
@@ -224,7 +225,28 @@ class StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):
                     img_metas,
                     proposals=None,
                     rescale=False):
-        """Test without augmentation."""
+        """Test without augmentation.
+
+        Args:
+            x (tuple[Tensor]): Features from upstream network. Each
+                has shape (batch_size, c, h, w).
+            proposal_list (list(Tensor)): Proposals from rpn head.
+                Each has shape (num_proposals, 5), last dimension
+                5 represent (x1, y1, x2, y2, score).
+            img_metas (list[dict]): Meta information of images.
+            rescale (bool): Whether to rescale the results to
+                the original image. Default: True.
+
+        Returns:
+            list[list[np.ndarray]] or list[tuple]: When no mask branch,
+            it is bbox results of each image and classes with type
+            `list[list[np.ndarray]]`. The outer list
+            corresponds to each image. The inner list
+            corresponds to each class. When the model has mask branch,
+            it contains bbox results and mask results.
+            The outer list corresponds to each image, and first element
+            of tuple is bbox results, second element is mask results.
+        """
         assert self.with_bbox, 'Bbox head must be implemented.'
 
         det_bboxes, det_labels = self.simple_test_bboxes(
diff --git a/mmdet/models/roi_heads/test_mixins.py b/mmdet/models/roi_heads/test_mixins.py
index c5848d0e..ae6e79ae 100644
--- a/mmdet/models/roi_heads/test_mixins.py
+++ b/mmdet/models/roi_heads/test_mixins.py
@@ -1,5 +1,6 @@
-import logging
+# Copyright (c) OpenMMLab. All rights reserved.
 import sys
+import warnings
 
 import numpy as np
 import torch
@@ -7,7 +8,6 @@ import torch
 from mmdet.core import (bbox2roi, bbox_mapping, merge_aug_bboxes,
                         merge_aug_masks, multiclass_nms)
 
-logger = logging.getLogger(__name__)
 if sys.version_info >= (3, 7):
     from mmdet.utils.contextmanagers import completed
 
@@ -233,7 +233,7 @@ class MaskTestMixin:
         scale_factors = tuple(meta['scale_factor'] for meta in img_metas)
 
         if isinstance(scale_factors[0], float):
-            logger.warning(
+            warnings.warn(
                 'Scale factor in img_metas should be a '
                 'ndarray with shape (4,) '
                 'arrange as (factor_w, factor_h, factor_w, factor_h), '
diff --git a/mmdet/models/roi_heads/trident_roi_head.py b/mmdet/models/roi_heads/trident_roi_head.py
index 245569e5..09758792 100644
--- a/mmdet/models/roi_heads/trident_roi_head.py
+++ b/mmdet/models/roi_heads/trident_roi_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmcv.ops import batched_nms
 
diff --git a/mmdet/models/seg_heads/__init__.py b/mmdet/models/seg_heads/__init__.py
new file mode 100644
index 00000000..b489a905
--- /dev/null
+++ b/mmdet/models/seg_heads/__init__.py
@@ -0,0 +1,3 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from .panoptic_fpn_head import PanopticFPNHead  # noqa: F401,F403
+from .panoptic_fusion_heads import *  # noqa: F401,F403
diff --git a/mmdet/models/seg_heads/base_semantic_head.py b/mmdet/models/seg_heads/base_semantic_head.py
new file mode 100644
index 00000000..2b6ca145
--- /dev/null
+++ b/mmdet/models/seg_heads/base_semantic_head.py
@@ -0,0 +1,86 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from abc import ABCMeta, abstractmethod
+
+import torch.nn.functional as F
+from mmcv.runner import BaseModule, force_fp32
+
+from ..builder import build_loss
+from ..utils import interpolate_as
+
+
+class BaseSemanticHead(BaseModule, metaclass=ABCMeta):
+    """Base module of Semantic Head.
+
+    Args:
+        num_classes (int): the number of classes.
+        init_cfg (dict): the initialization config.
+        loss_seg (dict): the loss of the semantic head.
+    """
+
+    def __init__(self,
+                 num_classes,
+                 init_cfg=None,
+                 loss_seg=dict(
+                     type='CrossEntropyLoss',
+                     ignore_index=255,
+                     loss_weight=1.0)):
+        super(BaseSemanticHead, self).__init__(init_cfg)
+        self.loss_seg = build_loss(loss_seg)
+        self.num_classes = num_classes
+
+    @force_fp32(apply_to=('seg_preds', ))
+    def loss(self, seg_preds, gt_semantic_seg):
+        """Get the loss of semantic head.
+
+        Args:
+            seg_preds (Tensor): The input logits with the shape (N, C, H, W).
+            gt_semantic_seg: The ground truth of semantic segmentation with
+                the shape (N, H, W).
+            label_bias: The starting number of the semantic label.
+                Default: 1.
+
+        Returns:
+            dict: the loss of semantic head.
+        """
+        if seg_preds.shape[-2:] != gt_semantic_seg.shape[-2:]:
+            seg_preds = interpolate_as(seg_preds, gt_semantic_seg)
+        seg_preds = seg_preds.permute((0, 2, 3, 1))
+
+        loss_seg = self.loss_seg(
+            seg_preds.reshape(-1, self.num_classes),  # => [NxHxW, C]
+            gt_semantic_seg.reshape(-1).long())
+        return dict(loss_seg=loss_seg)
+
+    @abstractmethod
+    def forward(self, x):
+        """Placeholder of forward function.
+
+        Returns:
+            dict[str, Tensor]: A dictionary, including features
+                and predicted scores. Required keys: 'seg_preds'
+                and 'feats'.
+        """
+        pass
+
+    def forward_train(self, x, gt_semantic_seg):
+        output = self.forward(x)
+        seg_preds = output['seg_preds']
+        return self.loss(seg_preds, gt_semantic_seg)
+
+    def simple_test(self, x, img_metas, rescale=False):
+        output = self.forward(x)
+        seg_preds = output['seg_preds']
+        seg_preds = F.interpolate(
+            seg_preds,
+            size=img_metas[0]['pad_shape'][:2],
+            mode='bilinear',
+            align_corners=False)
+
+        if rescale:
+            h, w, _ = img_metas[0]['img_shape']
+            seg_preds = seg_preds[:, :, :h, :w]
+
+            h, w, _ = img_metas[0]['ori_shape']
+            seg_preds = F.interpolate(
+                seg_preds, size=(h, w), mode='bilinear', align_corners=False)
+        return seg_preds
diff --git a/mmdet/models/seg_heads/panoptic_fpn_head.py b/mmdet/models/seg_heads/panoptic_fpn_head.py
new file mode 100644
index 00000000..67c30db0
--- /dev/null
+++ b/mmdet/models/seg_heads/panoptic_fpn_head.py
@@ -0,0 +1,118 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import torch
+import torch.nn as nn
+from mmcv.runner import ModuleList
+
+from ..builder import HEADS
+from ..utils import ConvUpsample
+from .base_semantic_head import BaseSemanticHead
+
+
+@HEADS.register_module()
+class PanopticFPNHead(BaseSemanticHead):
+    """PanopticFPNHead used in Panoptic FPN.
+
+    Arg:
+        num_classes (int): Number of classes, including all stuff
+            classes and one thing class.
+        in_channels (int): Number of channels in the input feature
+            map.
+        inner_channels (int): Number of channels in inner features.
+        start_level (int): The start level of the input features
+            used in PanopticFPN.
+        end_level (int): The end level of the used features, the
+            `end_level`-th layer will not be used.
+        fg_range (tuple): Range of the foreground classes.
+        bg_range (tuple): Range of the background classes.
+        conv_cfg (dict): Dictionary to construct and config
+            conv layer. Default: None.
+        norm_cfg (dict): Dictionary to construct and config norm layer.
+            Use ``GN`` by default.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+        loss_seg (dict): the loss of the semantic head.
+    """
+
+    def __init__(self,
+                 num_classes,
+                 in_channels=256,
+                 inner_channels=128,
+                 start_level=0,
+                 end_level=4,
+                 fg_range=(0, 79),
+                 bg_range=(80, 132),
+                 conv_cfg=None,
+                 norm_cfg=dict(type='GN', num_groups=32, requires_grad=True),
+                 init_cfg=None,
+                 loss_seg=dict(
+                     type='CrossEntropyLoss', ignore_index=-1,
+                     loss_weight=1.0)):
+        super(PanopticFPNHead, self).__init__(num_classes, init_cfg, loss_seg)
+        self.fg_range = fg_range
+        self.bg_range = bg_range
+        self.fg_nums = self.fg_range[1] - self.fg_range[0] + 1
+        self.bg_nums = self.bg_range[1] - self.bg_range[0] + 1
+        # Used feature layers are [start_level, end_level)
+        self.start_level = start_level
+        self.end_level = end_level
+        self.num_stages = end_level - start_level
+        self.inner_channels = inner_channels
+
+        self.conv_upsample_layers = ModuleList()
+        for i in range(start_level, end_level):
+            self.conv_upsample_layers.append(
+                ConvUpsample(
+                    in_channels,
+                    inner_channels,
+                    num_layers=i if i > 0 else 1,
+                    num_upsample=i if i > 0 else 0,
+                    conv_cfg=conv_cfg,
+                    norm_cfg=norm_cfg,
+                ))
+        self.conv_logits = nn.Conv2d(inner_channels, num_classes, 1)
+
+    def _set_things_to_void(self, gt_semantic_seg):
+        """Merge thing classes to one class.
+
+        In PanopticFPN, the background labels will be reset from `0` to
+        `self.bg_nums-1`, the foreground labels will merged to `self.bg_nums`.
+        """
+        gt_semantic_seg = gt_semantic_seg.int()
+        fg_mask = (gt_semantic_seg >= self.fg_range[0]) * (
+            gt_semantic_seg <= self.fg_range[1])
+        bg_mask = (gt_semantic_seg >= self.bg_range[0]) * (
+            gt_semantic_seg <= self.bg_range[1])
+
+        new_gt_seg = torch.clone(gt_semantic_seg)
+        new_gt_seg = torch.where(bg_mask, gt_semantic_seg - self.fg_nums,
+                                 new_gt_seg)
+        new_gt_seg = torch.where(fg_mask,
+                                 fg_mask.int() * self.bg_nums, new_gt_seg)
+        return new_gt_seg
+
+    def loss(self, seg_preds, gt_semantic_seg):
+        """The loss of PanopticFPN head.
+
+        Things classes will be merged to one class in PanopticFPN.
+        """
+        gt_semantic_seg = self._set_things_to_void(gt_semantic_seg)
+        return super().loss(seg_preds, gt_semantic_seg)
+
+    def init_weights(self):
+        super().init_weights()
+        nn.init.normal_(self.conv_logits.weight.data, 0, 0.01)
+        self.conv_logits.bias.data.zero_()
+
+    def forward(self, x):
+        # the number of subnets must be not more than
+        # the length of features.
+        assert self.num_stages <= len(x)
+
+        feats = []
+        for i, layer in enumerate(self.conv_upsample_layers):
+            f = layer(x[self.start_level + i])
+            feats.append(f)
+
+        feats = torch.sum(torch.stack(feats, dim=0), dim=0)
+        seg_preds = self.conv_logits(feats)
+        out = dict(seg_preds=seg_preds, feats=feats)
+        return out
diff --git a/mmdet/models/seg_heads/panoptic_fusion_heads/__init__.py b/mmdet/models/seg_heads/panoptic_fusion_heads/__init__.py
new file mode 100644
index 00000000..d14a33c3
--- /dev/null
+++ b/mmdet/models/seg_heads/panoptic_fusion_heads/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from .base_panoptic_fusion_head import \
+    BasePanopticFusionHead  # noqa: F401,F403
+from .heuristic_fusion_head import HeuristicFusionHead  # noqa: F401,F403
diff --git a/mmdet/models/seg_heads/panoptic_fusion_heads/base_panoptic_fusion_head.py b/mmdet/models/seg_heads/panoptic_fusion_heads/base_panoptic_fusion_head.py
new file mode 100644
index 00000000..a38ac1c6
--- /dev/null
+++ b/mmdet/models/seg_heads/panoptic_fusion_heads/base_panoptic_fusion_head.py
@@ -0,0 +1,48 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from abc import ABCMeta, abstractmethod
+
+from mmcv.runner import BaseModule
+
+from ...builder import build_loss
+
+
+class BasePanopticFusionHead(BaseModule, metaclass=ABCMeta):
+    """Base class for panoptic heads."""
+
+    def __init__(self,
+                 num_things_classes=80,
+                 num_stuff_classes=53,
+                 test_cfg=None,
+                 loss_panoptic=None,
+                 init_cfg=None,
+                 **kwargs):
+        super(BasePanopticFusionHead, self).__init__(init_cfg)
+        self.num_things_classes = num_things_classes
+        self.num_stuff_classes = num_stuff_classes
+        self.num_classes = num_things_classes + num_stuff_classes
+        self.test_cfg = test_cfg
+
+        if loss_panoptic:
+            self.loss_panoptic = build_loss(loss_panoptic)
+        else:
+            self.loss_panoptic = None
+
+    @property
+    def with_loss(self):
+        """bool: whether the panoptic head contains loss function."""
+        return self.loss_panoptic is not None
+
+    @abstractmethod
+    def forward_train(self, gt_masks=None, gt_semantic_seg=None, **kwargs):
+        """Forward function during training."""
+
+    @abstractmethod
+    def simple_test(self,
+                    img_metas,
+                    det_labels,
+                    mask_preds,
+                    seg_preds,
+                    det_bboxes,
+                    cfg=None,
+                    **kwargs):
+        """Test without augmentation."""
diff --git a/mmdet/models/seg_heads/panoptic_fusion_heads/heuristic_fusion_head.py b/mmdet/models/seg_heads/panoptic_fusion_heads/heuristic_fusion_head.py
new file mode 100644
index 00000000..fa625780
--- /dev/null
+++ b/mmdet/models/seg_heads/panoptic_fusion_heads/heuristic_fusion_head.py
@@ -0,0 +1,126 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import torch
+
+from mmdet.datasets.coco_panoptic import INSTANCE_OFFSET
+from mmdet.models.builder import HEADS
+from .base_panoptic_fusion_head import BasePanopticFusionHead
+
+
+@HEADS.register_module()
+class HeuristicFusionHead(BasePanopticFusionHead):
+    """Fusion Head with Heuristic method."""
+
+    def __init__(self,
+                 num_things_classes=80,
+                 num_stuff_classes=53,
+                 test_cfg=None,
+                 init_cfg=None,
+                 **kwargs):
+        super(HeuristicFusionHead,
+              self).__init__(num_things_classes, num_stuff_classes, test_cfg,
+                             None, init_cfg, **kwargs)
+
+    def forward_train(self, gt_masks=None, gt_semantic_seg=None, **kwargs):
+        """HeuristicFusionHead has no training loss."""
+        return dict()
+
+    def _lay_masks(self, bboxes, labels, masks, overlap_thr=0.5):
+        """Lay instance masks to a result map.
+
+        Args:
+            bboxes: The bboxes results, (K, 4).
+            labels: The labels of bboxes, (K, ).
+            masks: The instance masks, (K, H, W).
+            overlap_thr: Threshold to determine whether two masks overlap.
+                default: 0.5.
+
+        Returns:
+            Tensor: The result map, (H, W).
+        """
+        num_insts = bboxes.shape[0]
+        id_map = torch.zeros(
+            masks.shape[-2:], device=bboxes.device, dtype=torch.long)
+        if num_insts == 0:
+            return id_map, labels
+
+        scores, bboxes = bboxes[:, -1], bboxes[:, :4]
+
+        # Sort by score to use heuristic fusion
+        order = torch.argsort(-scores)
+        bboxes = bboxes[order]
+        labels = labels[order]
+        segm_masks = masks[order]
+
+        instance_id = 1
+        left_labels = []
+        for idx in range(bboxes.shape[0]):
+            _cls = labels[idx]
+            _mask = segm_masks[idx]
+            instance_id_map = torch.ones_like(
+                _mask, dtype=torch.long) * instance_id
+            area = _mask.sum()
+            if area == 0:
+                continue
+
+            pasted = id_map > 0
+            intersect = (_mask * pasted).sum()
+            if (intersect / (area + 1e-5)) > overlap_thr:
+                continue
+
+            _part = _mask * (~pasted)
+            id_map = torch.where(_part, instance_id_map, id_map)
+            left_labels.append(_cls)
+            instance_id += 1
+
+        if len(left_labels) > 0:
+            instance_labels = torch.stack(left_labels)
+        else:
+            instance_labels = bboxes.new_zeros((0, ), dtype=torch.long)
+        assert instance_id == (len(instance_labels) + 1)
+        return id_map, instance_labels
+
+    def simple_test(self, det_bboxes, det_labels, mask_preds, seg_preds,
+                    **kwargs):
+        """Fuse the results of instance and semantic segmentations.
+
+        Args:
+            det_bboxes: The bboxes results, (K, 4).
+            det_labels: The labels of bboxes, (K,).
+            mask_preds: The masks results, (K, H, W).
+            seg_preds: The semantic segmentation results,
+                (K, num_stuff + 1, H, W).
+
+        Returns:
+            Tensor : The panoptic segmentation result, (H, W).
+        """
+        mask_preds = mask_preds >= self.test_cfg.mask_thr_binary
+        id_map, labels = self._lay_masks(det_bboxes, det_labels, mask_preds,
+                                         self.test_cfg.mask_overlap)
+
+        seg_results = seg_preds.argmax(dim=0)
+        seg_results = seg_results + self.num_things_classes
+
+        pan_results = seg_results
+        instance_id = 1
+        for idx in range(det_labels.shape[0]):
+            _mask = id_map == (idx + 1)
+            if _mask.sum() == 0:
+                continue
+            _cls = labels[idx]
+            # simply trust detection
+            segment_id = _cls + instance_id * INSTANCE_OFFSET
+            pan_results[_mask] = segment_id
+            instance_id += 1
+
+        ids, counts = torch.unique(
+            pan_results % INSTANCE_OFFSET, return_counts=True)
+        stuff_ids = ids[ids >= self.num_things_classes]
+        stuff_counts = counts[ids >= self.num_things_classes]
+        ignore_stuff_ids = stuff_ids[
+            stuff_counts < self.test_cfg.stuff_area_limit]
+
+        assert pan_results.ndim == 2
+        pan_results[(pan_results.unsqueeze(2) == ignore_stuff_ids.reshape(
+            1, 1, -1)).any(dim=2)] = self.num_classes
+
+        return pan_results
diff --git a/mmdet/models/utils/__init__.py b/mmdet/models/utils/__init__.py
index 348a6f42..0b394136 100644
--- a/mmdet/models/utils/__init__.py
+++ b/mmdet/models/utils/__init__.py
@@ -1,4 +1,7 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from .brick_wrappers import AdaptiveAvgPool2d, adaptive_avg_pool2d
 from .builder import build_linear_layer, build_transformer
+from .ckpt_convert import pvt_convert
 from .conv_upsample import ConvUpsample
 from .csp_layer import CSPLayer
 from .gaussian_target import gaussian_radius, gen_gaussian_target
@@ -11,7 +14,8 @@ from .positional_encoding import (LearnedPositionalEncoding,
 from .res_layer import ResLayer, SimplifiedBasicBlock
 from .se_layer import SELayer
 from .transformer import (DetrTransformerDecoder, DetrTransformerDecoderLayer,
-                          DynamicConv, Transformer)
+                          DynamicConv, PatchEmbed, Transformer, nchw_to_nlc,
+                          nlc_to_nchw)
 
 __all__ = [
     'ResLayer', 'gaussian_radius', 'gen_gaussian_target',
@@ -19,5 +23,7 @@ __all__ = [
     'build_transformer', 'build_linear_layer', 'SinePositionalEncoding',
     'LearnedPositionalEncoding', 'DynamicConv', 'SimplifiedBasicBlock',
     'NormedLinear', 'NormedConv2d', 'make_divisible', 'InvertedResidual',
-    'SELayer', 'interpolate_as', 'ConvUpsample', 'CSPLayer'
+    'SELayer', 'interpolate_as', 'ConvUpsample', 'CSPLayer',
+    'adaptive_avg_pool2d', 'AdaptiveAvgPool2d', 'PatchEmbed', 'nchw_to_nlc',
+    'nlc_to_nchw', 'pvt_convert'
 ]
diff --git a/mmdet/models/utils/brick_wrappers.py b/mmdet/models/utils/brick_wrappers.py
new file mode 100644
index 00000000..b95a0996
--- /dev/null
+++ b/mmdet/models/utils/brick_wrappers.py
@@ -0,0 +1,50 @@
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from mmcv.cnn.bricks.wrappers import NewEmptyTensorOp, obsolete_torch_version
+
+if torch.__version__ == 'parrots':
+    TORCH_VERSION = torch.__version__
+else:
+    # torch.__version__ could be 1.3.1+cu92, we only need the first two
+    # for comparison
+    TORCH_VERSION = tuple(int(x) for x in torch.__version__.split('.')[:2])
+
+
+def adaptive_avg_pool2d(input, output_size):
+    """Handle empty batch dimension to adaptive_avg_pool2d.
+
+    Args:
+        input (tensor): 4D tensor.
+        output_size (int, tuple[int,int]): the target output size.
+    """
+    if input.numel() == 0 and obsolete_torch_version(TORCH_VERSION, (1, 9)):
+        if isinstance(output_size, int):
+            output_size = [output_size, output_size]
+        output_size = [*input.shape[:2], *output_size]
+        empty = NewEmptyTensorOp.apply(input, output_size)
+        return empty
+    else:
+        return F.adaptive_avg_pool2d(input, output_size)
+
+
+class AdaptiveAvgPool2d(nn.AdaptiveAvgPool2d):
+    """Handle empty batch dimension to AdaptiveAvgPool2d."""
+
+    def forward(self, x):
+        # PyTorch 1.9 does not support empty tensor inference yet
+        if x.numel() == 0 and obsolete_torch_version(TORCH_VERSION, (1, 9)):
+            output_size = self.output_size
+            if isinstance(output_size, int):
+                output_size = [output_size, output_size]
+            else:
+                output_size = [
+                    v if v is not None else d
+                    for v, d in zip(output_size,
+                                    x.size()[-2:])
+                ]
+            output_size = [*x.shape[:2], *output_size]
+            empty = NewEmptyTensorOp.apply(x, output_size)
+            return empty
+
+        return super().forward(x)
diff --git a/mmdet/models/utils/builder.py b/mmdet/models/utils/builder.py
index fdcff090..20fe7a6d 100644
--- a/mmdet/models/utils/builder.py
+++ b/mmdet/models/utils/builder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 from mmcv.utils import Registry, build_from_cfg
 
diff --git a/mmdet/models/utils/ckpt_convert.py b/mmdet/models/utils/ckpt_convert.py
new file mode 100644
index 00000000..4d660c4e
--- /dev/null
+++ b/mmdet/models/utils/ckpt_convert.py
@@ -0,0 +1,137 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+
+# This script consists of several convert functions which
+# can modify the weights of model in original repo to be
+# pre-trained weights.
+
+from collections import OrderedDict
+
+import torch
+
+
+def pvt_convert(ckpt):
+    new_ckpt = OrderedDict()
+    # Process the concat between q linear weights and kv linear weights
+    use_abs_pos_embed = False
+    use_conv_ffn = False
+    for k in ckpt.keys():
+        if k.startswith('pos_embed'):
+            use_abs_pos_embed = True
+        if k.find('dwconv') >= 0:
+            use_conv_ffn = True
+    for k, v in ckpt.items():
+        if k.startswith('head'):
+            continue
+        if k.startswith('norm.'):
+            continue
+        if k.startswith('cls_token'):
+            continue
+        if k.startswith('pos_embed'):
+            stage_i = int(k.replace('pos_embed', ''))
+            new_k = k.replace(f'pos_embed{stage_i}',
+                              f'layers.{stage_i - 1}.1.0.pos_embed')
+            if stage_i == 4 and v.size(1) == 50:  # 1 (cls token) + 7 * 7
+                new_v = v[:, 1:, :]  # remove cls token
+            else:
+                new_v = v
+        elif k.startswith('patch_embed'):
+            stage_i = int(k.split('.')[0].replace('patch_embed', ''))
+            new_k = k.replace(f'patch_embed{stage_i}',
+                              f'layers.{stage_i - 1}.0')
+            new_v = v
+            if 'proj.' in new_k:
+                new_k = new_k.replace('proj.', 'projection.')
+        elif k.startswith('block'):
+            stage_i = int(k.split('.')[0].replace('block', ''))
+            layer_i = int(k.split('.')[1])
+            new_layer_i = layer_i + use_abs_pos_embed
+            new_k = k.replace(f'block{stage_i}.{layer_i}',
+                              f'layers.{stage_i - 1}.1.{new_layer_i}')
+            new_v = v
+            if 'attn.q.' in new_k:
+                sub_item_k = k.replace('q.', 'kv.')
+                new_k = new_k.replace('q.', 'attn.in_proj_')
+                new_v = torch.cat([v, ckpt[sub_item_k]], dim=0)
+            elif 'attn.kv.' in new_k:
+                continue
+            elif 'attn.proj.' in new_k:
+                new_k = new_k.replace('proj.', 'attn.out_proj.')
+            elif 'attn.sr.' in new_k:
+                new_k = new_k.replace('sr.', 'sr.')
+            elif 'mlp.' in new_k:
+                string = f'{new_k}-'
+                new_k = new_k.replace('mlp.', 'ffn.layers.')
+                if 'fc1.weight' in new_k or 'fc2.weight' in new_k:
+                    new_v = v.reshape((*v.shape, 1, 1))
+                new_k = new_k.replace('fc1.', '0.')
+                new_k = new_k.replace('dwconv.dwconv.', '1.')
+                if use_conv_ffn:
+                    new_k = new_k.replace('fc2.', '4.')
+                else:
+                    new_k = new_k.replace('fc2.', '3.')
+                string += f'{new_k} {v.shape}-{new_v.shape}'
+        elif k.startswith('norm'):
+            stage_i = int(k[4])
+            new_k = k.replace(f'norm{stage_i}', f'layers.{stage_i - 1}.2')
+            new_v = v
+        else:
+            new_k = k
+            new_v = v
+        new_ckpt[new_k] = new_v
+
+    return new_ckpt
+
+
+def swin_converter(ckpt):
+
+    new_ckpt = OrderedDict()
+
+    def correct_unfold_reduction_order(x):
+        out_channel, in_channel = x.shape
+        x = x.reshape(out_channel, 4, in_channel // 4)
+        x = x[:, [0, 2, 1, 3], :].transpose(1,
+                                            2).reshape(out_channel, in_channel)
+        return x
+
+    def correct_unfold_norm_order(x):
+        in_channel = x.shape[0]
+        x = x.reshape(4, in_channel // 4)
+        x = x[[0, 2, 1, 3], :].transpose(0, 1).reshape(in_channel)
+        return x
+
+    for k, v in ckpt.items():
+        if k.startswith('head'):
+            continue
+        elif k.startswith('layers'):
+            new_v = v
+            if 'attn.' in k:
+                new_k = k.replace('attn.', 'attn.w_msa.')
+            elif 'mlp.' in k:
+                if 'mlp.fc1.' in k:
+                    new_k = k.replace('mlp.fc1.', 'ffn.layers.0.0.')
+                elif 'mlp.fc2.' in k:
+                    new_k = k.replace('mlp.fc2.', 'ffn.layers.1.')
+                else:
+                    new_k = k.replace('mlp.', 'ffn.')
+            elif 'downsample' in k:
+                new_k = k
+                if 'reduction.' in k:
+                    new_v = correct_unfold_reduction_order(v)
+                elif 'norm.' in k:
+                    new_v = correct_unfold_norm_order(v)
+            else:
+                new_k = k
+            new_k = new_k.replace('layers', 'stages', 1)
+        elif k.startswith('patch_embed'):
+            new_v = v
+            if 'proj' in k:
+                new_k = k.replace('proj', 'projection')
+            else:
+                new_k = k
+        else:
+            new_v = v
+            new_k = k
+
+        new_ckpt['backbone.' + new_k] = new_v
+
+    return new_ckpt
diff --git a/mmdet/models/utils/conv_upsample.py b/mmdet/models/utils/conv_upsample.py
index 9fed8340..bb5ba767 100644
--- a/mmdet/models/utils/conv_upsample.py
+++ b/mmdet/models/utils/conv_upsample.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
 from mmcv.runner import BaseModule, ModuleList
diff --git a/mmdet/models/utils/csp_layer.py b/mmdet/models/utils/csp_layer.py
index 5aad7519..5760b014 100644
--- a/mmdet/models/utils/csp_layer.py
+++ b/mmdet/models/utils/csp_layer.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 from mmcv.cnn import ConvModule, DepthwiseSeparableConvModule
diff --git a/mmdet/models/utils/gaussian_target.py b/mmdet/models/utils/gaussian_target.py
index 2e6d8b81..cd52faef 100644
--- a/mmdet/models/utils/gaussian_target.py
+++ b/mmdet/models/utils/gaussian_target.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from math import sqrt
 
 import torch
diff --git a/mmdet/models/utils/inverted_residual.py b/mmdet/models/utils/inverted_residual.py
index deb139d8..45faef04 100644
--- a/mmdet/models/utils/inverted_residual.py
+++ b/mmdet/models/utils/inverted_residual.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch.utils.checkpoint as cp
 from mmcv.cnn import ConvModule
 from mmcv.runner import BaseModule
diff --git a/mmdet/models/utils/make_divisible.py b/mmdet/models/utils/make_divisible.py
index 75ad7560..ed42c2ee 100644
--- a/mmdet/models/utils/make_divisible.py
+++ b/mmdet/models/utils/make_divisible.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 def make_divisible(value, divisor, min_value=None, min_ratio=0.9):
     """Make divisible function.
 
diff --git a/mmdet/models/utils/misc.py b/mmdet/models/utils/misc.py
index bd20054b..87643850 100644
--- a/mmdet/models/utils/misc.py
+++ b/mmdet/models/utils/misc.py
@@ -1,12 +1,12 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from torch.nn import functional as F
 
 
 def interpolate_as(source, target, mode='bilinear', align_corners=False):
-    """Interpolate the source to the shape of the target.
+    """Interpolate the `source` to the shape of the `target`.
 
-    Interpolate the source to the shape of target. The input must be a
-    Tensor, but the target can be a Tensor or a np.ndarray with the shape
-    (..., target_h, target_w).
+    The `source` must be a Tensor, but the `target` can be a Tensor or a
+    np.ndarray with the shape (..., target_h, target_w).
 
     Args:
         source (Tensor): A 3D/4D Tensor with the shape (N, H, W) or
@@ -23,7 +23,7 @@ def interpolate_as(source, target, mode='bilinear', align_corners=False):
     assert len(target.shape) >= 2
 
     def _interpolate_as(source, target, mode='bilinear', align_corners=False):
-        """Interpolate the source (4D) to the shape of the target."""
+        """Interpolate the `source` (4D) to the shape of the `target`."""
         target_h, target_w = target.shape[-2:]
         source_h, source_w = source.shape[-2:]
         if target_h != source_h or target_w != source_w:
diff --git a/mmdet/models/utils/normed_predictor.py b/mmdet/models/utils/normed_predictor.py
index 3b1b2312..f0eeef7d 100644
--- a/mmdet/models/utils/normed_predictor.py
+++ b/mmdet/models/utils/normed_predictor.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
diff --git a/mmdet/models/utils/positional_encoding.py b/mmdet/models/utils/positional_encoding.py
index 19cb83b6..dd29cd65 100644
--- a/mmdet/models/utils/positional_encoding.py
+++ b/mmdet/models/utils/positional_encoding.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 
 import torch
diff --git a/mmdet/models/utils/res_layer.py b/mmdet/models/utils/res_layer.py
index 825880d7..5c3e89fb 100644
--- a/mmdet/models/utils/res_layer.py
+++ b/mmdet/models/utils/res_layer.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.cnn import build_conv_layer, build_norm_layer
 from mmcv.runner import BaseModule, Sequential
 from torch import nn as nn
diff --git a/mmdet/models/utils/se_layer.py b/mmdet/models/utils/se_layer.py
index 877346cb..8e55a9e4 100644
--- a/mmdet/models/utils/se_layer.py
+++ b/mmdet/models/utils/se_layer.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch.nn as nn
 from mmcv.cnn import ConvModule
diff --git a/mmdet/models/utils/transformer.py b/mmdet/models/utils/transformer.py
index f81a5827..f0443a9e 100644
--- a/mmdet/models/utils/transformer.py
+++ b/mmdet/models/utils/transformer.py
@@ -1,15 +1,20 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import math
 import warnings
+from typing import Sequence
 
 import torch
 import torch.nn as nn
-from mmcv.cnn import build_activation_layer, build_norm_layer, xavier_init
+import torch.nn.functional as F
+from mmcv.cnn import (build_activation_layer, build_conv_layer,
+                      build_norm_layer, xavier_init)
 from mmcv.cnn.bricks.registry import (TRANSFORMER_LAYER,
                                       TRANSFORMER_LAYER_SEQUENCE)
 from mmcv.cnn.bricks.transformer import (BaseTransformerLayer,
                                          TransformerLayerSequence,
                                          build_transformer_layer_sequence)
 from mmcv.runner.base_module import BaseModule
+from mmcv.utils import to_2tuple
 from torch.nn.init import normal_
 
 from mmdet.models.utils.builder import TRANSFORMER
@@ -24,6 +29,362 @@ except ImportError:
     from mmcv.cnn.bricks.transformer import MultiScaleDeformableAttention
 
 
+def nlc_to_nchw(x, hw_shape):
+    """Convert [N, L, C] shape tensor to [N, C, H, W] shape tensor.
+
+    Args:
+        x (Tensor): The input tensor of shape [N, L, C] before convertion.
+        hw_shape (Sequence[int]): The height and width of output feature map.
+
+    Returns:
+        Tensor: The output tensor of shape [N, C, H, W] after convertion.
+    """
+    H, W = hw_shape
+    assert len(x.shape) == 3
+    B, L, C = x.shape
+    assert L == H * W, 'The seq_len does not match H, W'
+    return x.transpose(1, 2).reshape(B, C, H, W).contiguous()
+
+
+def nchw_to_nlc(x):
+    """Flatten [N, C, H, W] shape tensor to [N, L, C] shape tensor.
+
+    Args:
+        x (Tensor): The input tensor of shape [N, C, H, W] before convertion.
+
+    Returns:
+        Tensor: The output tensor of shape [N, L, C] after convertion.
+    """
+    assert len(x.shape) == 4
+    return x.flatten(2).transpose(1, 2).contiguous()
+
+
+class AdaptivePadding(nn.Module):
+    """Applies padding to input (if needed) so that input can get fully covered
+    by filter you specified. It support two modes "same" and "corner". The
+    "same" mode is same with "SAME" padding mode in TensorFlow, pad zero around
+    input. The "corner"  mode would pad zero to bottom right.
+
+    Args:
+        kernel_size (int | tuple): Size of the kernel:
+        stride (int | tuple): Stride of the filter. Default: 1:
+        dilation (int | tuple): Spacing between kernel elements.
+            Default: 1
+        padding (str): Support "same" and "corner", "corner" mode
+            would pad zero to bottom right, and "same" mode would
+            pad zero around input. Default: "corner".
+    Example:
+        >>> kernel_size = 16
+        >>> stride = 16
+        >>> dilation = 1
+        >>> input = torch.rand(1, 1, 15, 17)
+        >>> adap_pad = AdaptivePadding(
+        >>>     kernel_size=kernel_size,
+        >>>     stride=stride,
+        >>>     dilation=dilation,
+        >>>     padding="corner")
+        >>> out = adap_pad(input)
+        >>> assert (out.shape[2], out.shape[3]) == (16, 32)
+        >>> input = torch.rand(1, 1, 16, 17)
+        >>> out = adap_pad(input)
+        >>> assert (out.shape[2], out.shape[3]) == (16, 32)
+    """
+
+    def __init__(self, kernel_size=1, stride=1, dilation=1, padding='corner'):
+
+        super(AdaptivePadding, self).__init__()
+
+        assert padding in ('same', 'corner')
+
+        kernel_size = to_2tuple(kernel_size)
+        stride = to_2tuple(stride)
+        padding = to_2tuple(padding)
+        dilation = to_2tuple(dilation)
+
+        self.padding = padding
+        self.kernel_size = kernel_size
+        self.stride = stride
+        self.dilation = dilation
+
+    def get_pad_shape(self, input_shape):
+        input_h, input_w = input_shape
+        kernel_h, kernel_w = self.kernel_size
+        stride_h, stride_w = self.stride
+        output_h = math.ceil(input_h / stride_h)
+        output_w = math.ceil(input_w / stride_w)
+        pad_h = max((output_h - 1) * stride_h +
+                    (kernel_h - 1) * self.dilation[0] + 1 - input_h, 0)
+        pad_w = max((output_w - 1) * stride_w +
+                    (kernel_w - 1) * self.dilation[1] + 1 - input_w, 0)
+        return pad_h, pad_w
+
+    def forward(self, x):
+        pad_h, pad_w = self.get_pad_shape(x.size()[-2:])
+        if pad_h > 0 or pad_w > 0:
+            if self.padding == 'corner':
+                x = F.pad(x, [0, pad_w, 0, pad_h])
+            elif self.padding == 'same':
+                x = F.pad(x, [
+                    pad_w // 2, pad_w - pad_w // 2, pad_h // 2,
+                    pad_h - pad_h // 2
+                ])
+        return x
+
+
+class PatchEmbed(BaseModule):
+    """Image to Patch Embedding.
+
+    We use a conv layer to implement PatchEmbed.
+
+    Args:
+        in_channels (int): The num of input channels. Default: 3
+        embed_dims (int): The dimensions of embedding. Default: 768
+        conv_type (str): The config dict for embedding
+            conv layer type selection. Default: "Conv2d.
+        kernel_size (int): The kernel_size of embedding conv. Default: 16.
+        stride (int): The slide stride of embedding conv.
+            Default: None (Would be set as `kernel_size`).
+        padding (int | tuple | string ): The padding length of
+            embedding conv. When it is a string, it means the mode
+            of adaptive padding, support "same" and "corner" now.
+            Default: "corner".
+        dilation (int): The dilation rate of embedding conv. Default: 1.
+        bias (bool): Bias of embed conv. Default: True.
+        norm_cfg (dict, optional): Config dict for normalization layer.
+            Default: None.
+        input_size (int | tuple | None): The size of input, which will be
+            used to calculate the out size. Only work when `dynamic_size`
+            is False. Default: None.
+        init_cfg (`mmcv.ConfigDict`, optional): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(
+        self,
+        in_channels=3,
+        embed_dims=768,
+        conv_type='Conv2d',
+        kernel_size=16,
+        stride=16,
+        padding='corner',
+        dilation=1,
+        bias=True,
+        norm_cfg=None,
+        input_size=None,
+        init_cfg=None,
+    ):
+        super(PatchEmbed, self).__init__(init_cfg=init_cfg)
+
+        self.embed_dims = embed_dims
+        if stride is None:
+            stride = kernel_size
+
+        kernel_size = to_2tuple(kernel_size)
+        stride = to_2tuple(stride)
+        dilation = to_2tuple(dilation)
+
+        if isinstance(padding, str):
+            self.adap_padding = AdaptivePadding(
+                kernel_size=kernel_size,
+                stride=stride,
+                dilation=dilation,
+                padding=padding)
+            # disable the padding of conv
+            padding = 0
+        else:
+            self.adap_padding = None
+        padding = to_2tuple(padding)
+
+        self.projection = build_conv_layer(
+            dict(type=conv_type),
+            in_channels=in_channels,
+            out_channels=embed_dims,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        if norm_cfg is not None:
+            self.norm = build_norm_layer(norm_cfg, embed_dims)[1]
+        else:
+            self.norm = None
+
+        if input_size:
+            input_size = to_2tuple(input_size)
+            # `init_out_size` would be used outside to
+            # calculate the num_patches
+            # when `use_abs_pos_embed` outside
+            self.init_input_size = input_size
+            if self.adap_padding:
+                pad_h, pad_w = self.adap_padding.get_pad_shape(input_size)
+                input_h, input_w = input_size
+                input_h = input_h + pad_h
+                input_w = input_w + pad_w
+                input_size = (input_h, input_w)
+
+            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html
+            h_out = (input_size[0] + 2 * padding[0] - dilation[0] *
+                     (kernel_size[0] - 1) - 1) // stride[0] + 1
+            w_out = (input_size[1] + 2 * padding[1] - dilation[1] *
+                     (kernel_size[1] - 1) - 1) // stride[1] + 1
+            self.init_out_size = (h_out, w_out)
+        else:
+            self.init_input_size = None
+            self.init_out_size = None
+
+    def forward(self, x):
+        """
+        Args:
+            x (Tensor): Has shape (B, C, H, W). In most case, C is 3.
+
+        Returns:
+            tuple: Contains merged results and its spatial shape.
+
+                - x (Tensor): Has shape (B, out_h * out_w, embed_dims)
+                - out_size (tuple[int]): Spatial shape of x, arrange as
+                    (out_h, out_w).
+        """
+
+        if self.adap_padding:
+            x = self.adap_padding(x)
+
+        x = self.projection(x)
+        out_size = (x.shape[2], x.shape[3])
+        x = x.flatten(2).transpose(1, 2)
+        if self.norm is not None:
+            x = self.norm(x)
+        return x, out_size
+
+
+class PatchMerging(BaseModule):
+    """Merge patch feature map.
+
+    This layer groups feature map by kernel_size, and applies norm and linear
+    layers to the grouped feature map. Our implementation uses `nn.Unfold` to
+    merge patch, which is about 25% faster than original implementation.
+    Instead, we need to modify pretrained models for compatibility.
+
+    Args:
+        in_channels (int): The num of input channels.
+            to gets fully covered by filter and stride you specified..
+            Default: True.
+        out_channels (int): The num of output channels.
+        kernel_size (int | tuple, optional): the kernel size in the unfold
+            layer. Defaults to 2.
+        stride (int | tuple, optional): the stride of the sliding blocks in the
+            unfold layer. Default: None. (Would be set as `kernel_size`)
+        padding (int | tuple | string ): The padding length of
+            embedding conv. When it is a string, it means the mode
+            of adaptive padding, support "same" and "corner" now.
+            Default: "corner".
+        dilation (int | tuple, optional): dilation parameter in the unfold
+            layer. Default: 1.
+        bias (bool, optional): Whether to add bias in linear layer or not.
+            Defaults: False.
+        norm_cfg (dict, optional): Config dict for normalization layer.
+            Default: dict(type='LN').
+        init_cfg (dict, optional): The extra config for initialization.
+            Default: None.
+    """
+
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 kernel_size=2,
+                 stride=None,
+                 padding='corner',
+                 dilation=1,
+                 bias=False,
+                 norm_cfg=dict(type='LN'),
+                 init_cfg=None):
+        super().__init__(init_cfg=init_cfg)
+        self.in_channels = in_channels
+        self.out_channels = out_channels
+        if stride:
+            stride = stride
+        else:
+            stride = kernel_size
+
+        kernel_size = to_2tuple(kernel_size)
+        stride = to_2tuple(stride)
+        dilation = to_2tuple(dilation)
+
+        if isinstance(padding, str):
+            self.adap_padding = AdaptivePadding(
+                kernel_size=kernel_size,
+                stride=stride,
+                dilation=dilation,
+                padding=padding)
+            # disable the padding of unfold
+            padding = 0
+        else:
+            self.adap_padding = None
+
+        padding = to_2tuple(padding)
+        self.sampler = nn.Unfold(
+            kernel_size=kernel_size,
+            dilation=dilation,
+            padding=padding,
+            stride=stride)
+
+        sample_dim = kernel_size[0] * kernel_size[1] * in_channels
+
+        if norm_cfg is not None:
+            self.norm = build_norm_layer(norm_cfg, sample_dim)[1]
+        else:
+            self.norm = None
+
+        self.reduction = nn.Linear(sample_dim, out_channels, bias=bias)
+
+    def forward(self, x, input_size):
+        """
+        Args:
+            x (Tensor): Has shape (B, H*W, C_in).
+            input_size (tuple[int]): The spatial shape of x, arrange as (H, W).
+                Default: None.
+
+        Returns:
+            tuple: Contains merged results and its spatial shape.
+
+                - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)
+                - out_size (tuple[int]): Spatial shape of x, arrange as
+                    (Merged_H, Merged_W).
+        """
+        B, L, C = x.shape
+        assert isinstance(input_size, Sequence), f'Expect ' \
+                                                 f'input_size is ' \
+                                                 f'`Sequence` ' \
+                                                 f'but get {input_size}'
+
+        H, W = input_size
+        assert L == H * W, 'input feature has wrong size'
+
+        x = x.view(B, H, W, C).permute([0, 3, 1, 2])  # B, C, H, W
+        # Use nn.Unfold to merge patch. About 25% faster than original method,
+        # but need to modify pretrained model for compatibility
+
+        if self.adap_padding:
+            x = self.adap_padding(x)
+            H, W = x.shape[-2:]
+
+        x = self.sampler(x)
+        # if kernel_size=2 and stride=2, x should has shape (B, 4*C, H/2*W/2)
+
+        out_h = (H + 2 * self.sampler.padding[0] - self.sampler.dilation[0] *
+                 (self.sampler.kernel_size[0] - 1) -
+                 1) // self.sampler.stride[0] + 1
+        out_w = (W + 2 * self.sampler.padding[1] - self.sampler.dilation[1] *
+                 (self.sampler.kernel_size[1] - 1) -
+                 1) // self.sampler.stride[1] + 1
+
+        output_size = (out_h, out_w)
+        x = x.transpose(1, 2)  # B, H/2*W/2, 4*C
+        x = self.norm(x) if self.norm else x
+        x = self.reduction(x)
+        return x, output_size
+
+
 def inverse_sigmoid(x, eps=1e-5):
     """Inverse function of sigmoid.
 
diff --git a/mmdet/utils/__init__.py b/mmdet/utils/__init__.py
index ac489e2d..3f155805 100644
--- a/mmdet/utils/__init__.py
+++ b/mmdet/utils/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .collect_env import collect_env
 from .logger import get_root_logger
 
diff --git a/mmdet/utils/collect_env.py b/mmdet/utils/collect_env.py
index 89c064ac..97e25c0e 100644
--- a/mmdet/utils/collect_env.py
+++ b/mmdet/utils/collect_env.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmcv.utils import collect_env as collect_base_env
 from mmcv.utils import get_git_hash
 
diff --git a/mmdet/utils/contextmanagers.py b/mmdet/utils/contextmanagers.py
index 38a63926..fa12bfca 100644
--- a/mmdet/utils/contextmanagers.py
+++ b/mmdet/utils/contextmanagers.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import asyncio
 import contextlib
 import logging
diff --git a/mmdet/utils/logger.py b/mmdet/utils/logger.py
index 6fc6e6b4..7e66fb6b 100644
--- a/mmdet/utils/logger.py
+++ b/mmdet/utils/logger.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import logging
 
 from mmcv.utils import get_logger
diff --git a/mmdet/utils/profiling.py b/mmdet/utils/profiling.py
index 4be9222c..2f53f456 100644
--- a/mmdet/utils/profiling.py
+++ b/mmdet/utils/profiling.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import contextlib
 import sys
 import time
diff --git a/mmdet/utils/util_mixins.py b/mmdet/utils/util_mixins.py
index 9aed0153..b83b6617 100644
--- a/mmdet/utils/util_mixins.py
+++ b/mmdet/utils/util_mixins.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """This module defines the :class:`NiceRepr` mixin class, which defines a
 ``__repr__`` and ``__str__`` method that only depend on a custom ``__nice__``
 method, which you must define. This means you only have to overload one
diff --git a/mmdet/utils/util_random.py b/mmdet/utils/util_random.py
index e313e994..dc1ecb6c 100644
--- a/mmdet/utils/util_random.py
+++ b/mmdet/utils/util_random.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """Helpers for random number generators."""
 import numpy as np
 
diff --git a/mmdet/version.py b/mmdet/version.py
index 3c6f47c4..22605ac9 100644
--- a/mmdet/version.py
+++ b/mmdet/version.py
@@ -1,6 +1,6 @@
-# Copyright (c) Open-MMLab. All rights reserved.
+# Copyright (c) OpenMMLab. All rights reserved.
 
-__version__ = '2.15.1'
+__version__ = '2.17.0'
 short_version = __version__
 
 
diff --git a/model-index.yml b/model-index.yml
index aa66f7ce..23131003 100644
--- a/model-index.yml
+++ b/model-index.yml
@@ -38,6 +38,7 @@ Import:
   - configs/nas_fpn/metafile.yml
   - configs/paa/metafile.yml
   - configs/pafpn/metafile.yml
+  - configs/pvt/metafile.yml
   - configs/pisa/metafile.yml
   - configs/point_rend/metafile.yml
   - configs/regnet/metafile.yml
@@ -49,6 +50,7 @@ Import:
   - configs/scnet/metafile.yml
   - configs/scratch/metafile.yml
   - configs/sparse_rcnn/metafile.yml
+  - configs/solo/metafile.yml
   - configs/ssd/metafile.yml
   - configs/tridentnet/metafile.yml
   - configs/vfnet/metafile.yml
@@ -56,3 +58,5 @@ Import:
   - configs/yolo/metafile.yml
   - configs/yolof/metafile.yml
   - configs/yolox/metafile.yml
+  - configs/swin/metafile.yml
+  - configs/strong_baselines/metafile.yml
diff --git a/requirements/docs.txt b/requirements/docs.txt
index 89fbf86c..45885d9b 100644
--- a/requirements/docs.txt
+++ b/requirements/docs.txt
@@ -1,4 +1,7 @@
+docutils==0.16.0
+-e git+https://github.com/open-mmlab/pytorch_sphinx_theme.git#egg=pytorch_sphinx_theme
 recommonmark
-sphinx
+sphinx==4.0.2
+sphinx-copybutton
 sphinx_markdown_tables
-sphinx_rtd_theme
+sphinx_rtd_theme==0.5.2
diff --git a/requirements/optional.txt b/requirements/optional.txt
index ac9688b0..da554cf3 100644
--- a/requirements/optional.txt
+++ b/requirements/optional.txt
@@ -1,4 +1,3 @@
-albumentations>=0.3.2
 cityscapesscripts
 imagecorruptions
 scipy
diff --git a/requirements/tests.txt b/requirements/tests.txt
index fe1fe0d7..124a750b 100644
--- a/requirements/tests.txt
+++ b/requirements/tests.txt
@@ -7,7 +7,7 @@ isort==4.3.21
 kwarray
 mmtrack
 onnx==1.7.0
-onnxruntime==1.5.1
+onnxruntime>=1.8.0
 pytest
 ubelt
 xdoctest>=0.10.0
diff --git a/setup.cfg b/setup.cfg
index 84dfbddd..950b4dc8 100644
--- a/setup.cfg
+++ b/setup.cfg
@@ -3,7 +3,7 @@ line_length = 79
 multi_line_output = 0
 known_standard_library = setuptools
 known_first_party = mmdet
-known_third_party = PIL,asynctest,cityscapesscripts,cv2,gather_models,matplotlib,mmcv,numpy,onnx,onnxruntime,pycocotools,pytest,seaborn,six,terminaltables,torch,ts,yaml
+known_third_party = PIL,asynctest,cityscapesscripts,cv2,gather_models,matplotlib,mmcv,numpy,onnx,onnxruntime,pycocotools,pytest,pytorch_sphinx_theme,requests,scipy,seaborn,six,terminaltables,torch,ts,yaml
 no_lines_before = STDLIB,LOCALFOLDER
 default_section = THIRDPARTY
 
diff --git a/setup.py b/setup.py
index dfd3db89..16466b28 100755
--- a/setup.py
+++ b/setup.py
@@ -1,4 +1,5 @@
 #!/usr/bin/env python
+# Copyright (c) OpenMMLab. All rights reserved.
 import os
 import os.path as osp
 import shutil
@@ -129,7 +130,7 @@ def parse_requirements(fname='requirements.txt', with_version=True):
     return packages
 
 
-def add_mim_extention():
+def add_mim_extension():
     """Add extra files that are required to support MIM into the package.
 
     These files will be added by creating a symlink to the originals if the
@@ -178,14 +179,14 @@ def add_mim_extention():
 
 
 if __name__ == '__main__':
-    add_mim_extention()
+    add_mim_extension()
     setup(
         name='mmdet',
         version=get_version(),
         description='OpenMMLab Detection Toolbox and Benchmark',
         long_description=readme(),
         long_description_content_type='text/markdown',
-        author='OpenMMLab',
+        author='MMDetection Contributors',
         author_email='openmmlab@gmail.com',
         keywords='computer vision, object detection',
         url='https://github.com/open-mmlab/mmdetection',
@@ -199,6 +200,7 @@ if __name__ == '__main__':
             'Programming Language :: Python :: 3.6',
             'Programming Language :: Python :: 3.7',
             'Programming Language :: Python :: 3.8',
+            'Programming Language :: Python :: 3.9',
         ],
         license='Apache License 2.0',
         setup_requires=parse_requirements('requirements/build.txt'),
diff --git a/tests/data/configs_mmtrack/faster_rcnn_r50_dc5.py b/tests/data/configs_mmtrack/faster_rcnn_r50_dc5.py
index 0d77a29e..d0d2dc21 100644
--- a/tests/data/configs_mmtrack/faster_rcnn_r50_dc5.py
+++ b/tests/data/configs_mmtrack/faster_rcnn_r50_dc5.py
@@ -1,10 +1,10 @@
 model = dict(
     detector=dict(
         type='FasterRCNN',
-        pretrained='torchvision://resnet50',
         backbone=dict(
             type='ResNet',
-            depth=50,
+            depth=18,
+            base_channels=2,
             num_stages=4,
             out_indices=(3, ),
             strides=(1, 2, 2, 1),
@@ -15,13 +15,13 @@ model = dict(
             style='pytorch'),
         neck=dict(
             type='ChannelMapper',
-            in_channels=[2048],
-            out_channels=512,
+            in_channels=[16],
+            out_channels=16,
             kernel_size=3),
         rpn_head=dict(
             type='RPNHead',
-            in_channels=512,
-            feat_channels=512,
+            in_channels=16,
+            feat_channels=16,
             anchor_generator=dict(
                 type='AnchorGenerator',
                 scales=[4, 8, 16, 32],
@@ -41,12 +41,12 @@ model = dict(
                 type='SingleRoIExtractor',
                 roi_layer=dict(
                     type='RoIAlign', output_size=7, sampling_ratio=2),
-                out_channels=512,
+                out_channels=16,
                 featmap_strides=[16]),
             bbox_head=dict(
                 type='Shared2FCBBoxHead',
-                in_channels=512,
-                fc_out_channels=1024,
+                in_channels=16,
+                fc_out_channels=32,
                 roi_feat_size=7,
                 num_classes=30,
                 bbox_coder=dict(
diff --git a/tests/data/configs_mmtrack/faster_rcnn_r50_fpn.py b/tests/data/configs_mmtrack/faster_rcnn_r50_fpn.py
index 688dd0f6..09de2164 100644
--- a/tests/data/configs_mmtrack/faster_rcnn_r50_fpn.py
+++ b/tests/data/configs_mmtrack/faster_rcnn_r50_fpn.py
@@ -1,10 +1,10 @@
 model = dict(
     detector=dict(
         type='FasterRCNN',
-        pretrained='torchvision://resnet50',
         backbone=dict(
             type='ResNet',
-            depth=50,
+            depth=18,
+            base_channels=2,
             num_stages=4,
             out_indices=(0, 1, 2, 3),
             frozen_stages=1,
@@ -12,14 +12,12 @@ model = dict(
             norm_eval=True,
             style='pytorch'),
         neck=dict(
-            type='FPN',
-            in_channels=[256, 512, 1024, 2048],
-            out_channels=256,
+            type='FPN', in_channels=[2, 4, 8, 16], out_channels=16,
             num_outs=5),
         rpn_head=dict(
             type='RPNHead',
-            in_channels=256,
-            feat_channels=256,
+            in_channels=16,
+            feat_channels=16,
             anchor_generator=dict(
                 type='AnchorGenerator',
                 scales=[8],
@@ -39,12 +37,12 @@ model = dict(
                 type='SingleRoIExtractor',
                 roi_layer=dict(
                     type='RoIAlign', output_size=7, sampling_ratio=0),
-                out_channels=256,
+                out_channels=16,
                 featmap_strides=[4, 8, 16, 32]),
             bbox_head=dict(
                 type='Shared2FCBBoxHead',
-                in_channels=256,
-                fc_out_channels=1024,
+                in_channels=16,
+                fc_out_channels=32,
                 roi_feat_size=7,
                 num_classes=80,
                 bbox_coder=dict(
diff --git a/tests/data/configs_mmtrack/selsa_faster_rcnn_r101_dc5_1x.py b/tests/data/configs_mmtrack/selsa_faster_rcnn_r101_dc5_1x.py
index a69d1546..a0109d75 100644
--- a/tests/data/configs_mmtrack/selsa_faster_rcnn_r101_dc5_1x.py
+++ b/tests/data/configs_mmtrack/selsa_faster_rcnn_r101_dc5_1x.py
@@ -6,8 +6,7 @@ model = dict(
     type='SELSA',
     pretrains=None,
     detector=dict(
-        pretrained='torchvision://resnet101',
-        backbone=dict(depth=101),
+        backbone=dict(depth=18, base_channels=2),
         roi_head=dict(
             type='SelsaRoIHead',
             bbox_head=dict(
@@ -15,7 +14,7 @@ model = dict(
                 num_shared_fcs=2,
                 aggregator=dict(
                     type='SelsaAggregator',
-                    in_channels=1024,
+                    in_channels=32,
                     num_attention_blocks=16)))))
 
 # dataset settings
diff --git a/tests/data/configs_mmtrack/tracktor_faster-rcnn_r50_fpn_4e.py b/tests/data/configs_mmtrack/tracktor_faster-rcnn_r50_fpn_4e.py
index b8cb582f..e7d61117 100644
--- a/tests/data/configs_mmtrack/tracktor_faster-rcnn_r50_fpn_4e.py
+++ b/tests/data/configs_mmtrack/tracktor_faster-rcnn_r50_fpn_4e.py
@@ -19,7 +19,8 @@ model = dict(
         type='BaseReID',
         backbone=dict(
             type='ResNet',
-            depth=50,
+            depth=18,
+            base_channels=2,
             num_stages=4,
             out_indices=(3, ),
             style='pytorch'),
@@ -27,10 +28,10 @@ model = dict(
         head=dict(
             type='LinearReIDHead',
             num_fcs=1,
-            in_channels=2048,
-            fc_channels=1024,
-            out_channels=128,
-            num_classes=378,
+            in_channels=16,
+            fc_channels=32,
+            out_channels=16,
+            num_classes=8,
             loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
             loss_pairwise=dict(
                 type='TripletLoss', margin=0.3, loss_weight=1.0),
diff --git a/tests/data/custom_dataset/images/000001.jpg b/tests/data/custom_dataset/images/000001.jpg
new file mode 100644
index 00000000..8f96ee5d
Binary files /dev/null and b/tests/data/custom_dataset/images/000001.jpg differ
diff --git a/tests/data/custom_dataset/images/000001.xml b/tests/data/custom_dataset/images/000001.xml
new file mode 100644
index 00000000..795d3983
--- /dev/null
+++ b/tests/data/custom_dataset/images/000001.xml
@@ -0,0 +1,44 @@
+<annotation>
+	<folder>VOC2007</folder>
+	<filename>000001.jpg</filename>
+	<source>
+		<database>The VOC2007 Database</database>
+		<annotation>PASCAL VOC2007</annotation>
+		<image>flickr</image>
+		<flickrid>341012865</flickrid>
+	</source>
+	<owner>
+		<flickrid>Fried Camels</flickrid>
+		<name>Jinky the Fruit Bat</name>
+	</owner>
+	<size>
+		<width>353</width>
+		<height>500</height>
+		<depth>3</depth>
+	</size>
+	<segmented>0</segmented>
+	<object>
+		<name>dog</name>
+		<pose>Left</pose>
+		<truncated>1</truncated>
+		<difficult>0</difficult>
+		<bndbox>
+			<xmin>48</xmin>
+			<ymin>240</ymin>
+			<xmax>195</xmax>
+			<ymax>371</ymax>
+		</bndbox>
+	</object>
+	<object>
+		<name>person</name>
+		<pose>Left</pose>
+		<truncated>1</truncated>
+		<difficult>0</difficult>
+		<bndbox>
+			<xmin>8</xmin>
+			<ymin>12</ymin>
+			<xmax>352</xmax>
+			<ymax>498</ymax>
+		</bndbox>
+	</object>
+</annotation>
diff --git a/tests/data/custom_dataset/test.txt b/tests/data/custom_dataset/test.txt
new file mode 100644
index 00000000..a12b836b
--- /dev/null
+++ b/tests/data/custom_dataset/test.txt
@@ -0,0 +1 @@
+000001
diff --git a/tests/data/custom_dataset/trainval.txt b/tests/data/custom_dataset/trainval.txt
new file mode 100644
index 00000000..a12b836b
--- /dev/null
+++ b/tests/data/custom_dataset/trainval.txt
@@ -0,0 +1 @@
+000001
diff --git a/tests/test_data/test_datasets/test_coco_dataset.py b/tests/test_data/test_datasets/test_coco_dataset.py
index 13b6c7f3..77edfdfd 100644
--- a/tests/test_data/test_datasets/test_coco_dataset.py
+++ b/tests/test_data/test_datasets/test_coco_dataset.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import tempfile
 
diff --git a/tests/test_data/test_datasets/test_common.py b/tests/test_data/test_datasets/test_common.py
index 6642be9f..36278834 100644
--- a/tests/test_data/test_datasets/test_common.py
+++ b/tests/test_data/test_datasets/test_common.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import logging
 import os
diff --git a/tests/test_data/test_datasets/test_custom_dataset.py b/tests/test_data/test_datasets/test_custom_dataset.py
index bda44996..b9207be3 100644
--- a/tests/test_data/test_datasets/test_custom_dataset.py
+++ b/tests/test_data/test_datasets/test_custom_dataset.py
@@ -1,3 +1,6 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import os
+import unittest
 from unittest.mock import MagicMock, patch
 
 import pytest
@@ -86,3 +89,51 @@ def test_custom_classes_override_default(dataset):
     assert custom_dataset.CLASSES != original_classes
     assert custom_dataset.CLASSES == ['bus', 'car']
     print(custom_dataset)
+
+
+class CustomDatasetTests(unittest.TestCase):
+
+    def setUp(self):
+        super().setUp()
+        self.data_dir = os.path.join(
+            os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
+            'data')
+        self.dataset_class = DATASETS.get('XMLDataset')
+
+    def test_data_infos__default_db_directories(self):
+        """Test correct data read having a Pacal-VOC directory structure."""
+        test_dataset_root = os.path.join(self.data_dir, 'VOCdevkit', 'VOC2007')
+        custom_ds = self.dataset_class(
+            data_root=test_dataset_root,
+            ann_file=os.path.join(test_dataset_root, 'ImageSets', 'Main',
+                                  'trainval.txt'),
+            pipeline=[],
+            classes=('person', 'dog'),
+            test_mode=True)
+
+        self.assertListEqual([{
+            'id': '000001',
+            'filename': 'JPEGImages/000001.jpg',
+            'width': 353,
+            'height': 500
+        }], custom_ds.data_infos)
+
+    def test_data_infos__overridden_db_subdirectories(self):
+        """Test correct data read having a customized directory structure."""
+        test_dataset_root = os.path.join(self.data_dir, 'custom_dataset')
+        custom_ds = self.dataset_class(
+            data_root=test_dataset_root,
+            ann_file=os.path.join(test_dataset_root, 'trainval.txt'),
+            pipeline=[],
+            classes=('person', 'dog'),
+            test_mode=True,
+            img_prefix='',
+            img_subdir='images',
+            ann_subdir='images')
+
+        self.assertListEqual([{
+            'id': '000001',
+            'filename': 'images/000001.jpg',
+            'width': 353,
+            'height': 500
+        }], custom_ds.data_infos)
diff --git a/tests/test_data/test_datasets/test_dataset_wrapper.py b/tests/test_data/test_datasets/test_dataset_wrapper.py
index dfa5d58c..eb3b08a6 100644
--- a/tests/test_data/test_datasets/test_dataset_wrapper.py
+++ b/tests/test_data/test_datasets/test_dataset_wrapper.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import bisect
 import math
 from collections import defaultdict
diff --git a/tests/test_data/test_datasets/test_panoptic_dataset.py b/tests/test_data/test_datasets/test_panoptic_dataset.py
index dcfab7f1..44670f22 100644
--- a/tests/test_data/test_datasets/test_panoptic_dataset.py
+++ b/tests/test_data/test_datasets/test_panoptic_dataset.py
@@ -1,9 +1,16 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import tempfile
 
 import mmcv
+import numpy as np
 
-from mmdet.datasets import CocoPanopticDataset
+from mmdet.datasets.coco_panoptic import INSTANCE_OFFSET, CocoPanopticDataset
+
+try:
+    from panopticapi.utils import id2rgb
+except ImportError:
+    id2rgb = None
 
 
 def _create_panoptic_style_json(json_name):
@@ -140,3 +147,161 @@ def test_load_panoptic_style_json():
     assert ann['bboxes'].shape[0] == ann['labels'].shape[0] == 1
     assert ann['bboxes_ignore'].shape[0] == 1
     assert len(ann['masks']) == 3
+
+
+def _create_panoptic_gt_annotations(ann_file):
+    categories = [{
+        'id': 0,
+        'name': 'person',
+        'supercategory': 'person',
+        'isthing': 1
+    }, {
+        'id': 1,
+        'name': 'dog',
+        'supercategory': 'dog',
+        'isthing': 1
+    }, {
+        'id': 2,
+        'name': 'wall',
+        'supercategory': 'wall',
+        'isthing': 0
+    }]
+
+    images = [{
+        'id': 0,
+        'width': 80,
+        'height': 60,
+        'file_name': 'fake_name1.jpg',
+    }]
+
+    annotations = [{
+        'segments_info': [{
+            'id': 1,
+            'category_id': 0,
+            'area': 400,
+            'bbox': [10, 10, 10, 40],
+            'iscrowd': 0
+        }, {
+            'id': 2,
+            'category_id': 0,
+            'area': 400,
+            'bbox': [30, 10, 10, 40],
+            'iscrowd': 0
+        }, {
+            'id': 3,
+            'category_id': 1,
+            'iscrowd': 0,
+            'bbox': [50, 10, 10, 5],
+            'area': 50
+        }, {
+            'id': 4,
+            'category_id': 2,
+            'iscrowd': 0,
+            'bbox': [0, 0, 80, 60],
+            'area': 3950
+        }],
+        'file_name':
+        'fake_name1.png',
+        'image_id':
+        0
+    }]
+
+    gt_json = {
+        'images': images,
+        'annotations': annotations,
+        'categories': categories
+    }
+
+    # 4 is the id of the background class annotation.
+    gt = np.zeros((60, 80), dtype=np.int64) + 4
+    gt_bboxes = np.array([[10, 10, 10, 40], [30, 10, 10, 40], [50, 10, 10, 5]],
+                         dtype=np.int64)
+    for i in range(3):
+        x, y, w, h = gt_bboxes[i]
+        gt[y:y + h, x:x + w] = i + 1  # id starts from 1
+
+    gt = id2rgb(gt).astype(np.uint8)
+    img_path = osp.join(osp.dirname(ann_file), 'fake_name1.png')
+    mmcv.imwrite(gt[:, :, ::-1], img_path)
+
+    mmcv.dump(gt_json, ann_file)
+    return gt_json
+
+
+def test_panoptic_evaluation():
+    if id2rgb is None:
+        return
+
+    # TP for background class, IoU=3576/4324=0.827
+    # 2 the category id of the background class
+    pred = np.zeros((60, 80), dtype=np.int64) + 2
+    pred_bboxes = np.array(
+        [
+            [11, 11, 10, 40],  # TP IoU=351/449=0.78
+            [38, 10, 10, 40],  # FP
+            [51, 10, 10, 5]
+        ],  # TP IoU=45/55=0.818
+        dtype=np.int64)
+    pred_labels = np.array([0, 0, 1], dtype=np.int64)
+    for i in range(3):
+        x, y, w, h = pred_bboxes[i]
+        pred[y:y + h, x:x + w] = (i + 1) * INSTANCE_OFFSET + pred_labels[i]
+
+    tmp_dir = tempfile.TemporaryDirectory()
+    ann_file = osp.join(tmp_dir.name, 'panoptic.json')
+    gt_json = _create_panoptic_gt_annotations(ann_file)
+
+    results = [{'pan_results': pred}]
+
+    dataset = CocoPanopticDataset(
+        ann_file=ann_file,
+        seg_prefix=tmp_dir.name,
+        classes=[cat['name'] for cat in gt_json['categories']],
+        pipeline=[])
+
+    # For 'person', sq = 0.78 / 1, rq = 1 / 2( 1 tp + 0.5 * (1 fn + 1 fp))
+    # For 'dog', sq = 0.818, rq = 1 / 1
+    # For 'wall', sq = 0.827, rq = 1 / 1
+    # Here is the results for all classes:
+    # +--------+--------+--------+---------+------------+
+    # |        | PQ     | SQ     | RQ      | categories |
+    # +--------+--------+--------+---------+------------+
+    # | All    | 67.869 | 80.898 | 83.333  |      3     |
+    # | Things | 60.453 | 79.996 | 75.000  |      2     |
+    # | Stuff  | 82.701 | 82.701 | 100.000 |      1     |
+    # +--------+--------+--------+---------+------------+
+    parsed_results = dataset.evaluate(results)
+    assert np.isclose(parsed_results['PQ'], 67.869)
+    assert np.isclose(parsed_results['SQ'], 80.898)
+    assert np.isclose(parsed_results['RQ'], 83.333)
+    assert np.isclose(parsed_results['PQ_th'], 60.453)
+    assert np.isclose(parsed_results['SQ_th'], 79.996)
+    assert np.isclose(parsed_results['RQ_th'], 75.000)
+    assert np.isclose(parsed_results['PQ_st'], 82.701)
+    assert np.isclose(parsed_results['SQ_st'], 82.701)
+    assert np.isclose(parsed_results['RQ_st'], 100.000)
+
+    # test jsonfile_prefix
+    outfile_prefix = osp.join(tmp_dir.name, 'results')
+    parsed_results = dataset.evaluate(results, jsonfile_prefix=outfile_prefix)
+    assert np.isclose(parsed_results['PQ'], 67.869)
+    assert np.isclose(parsed_results['SQ'], 80.898)
+    assert np.isclose(parsed_results['RQ'], 83.333)
+    assert np.isclose(parsed_results['PQ_th'], 60.453)
+    assert np.isclose(parsed_results['SQ_th'], 79.996)
+    assert np.isclose(parsed_results['RQ_th'], 75.000)
+    assert np.isclose(parsed_results['PQ_st'], 82.701)
+    assert np.isclose(parsed_results['SQ_st'], 82.701)
+    assert np.isclose(parsed_results['RQ_st'], 100.000)
+
+    # test classwise
+    parsed_results = dataset.evaluate(results, classwise=True)
+    assert np.isclose(parsed_results['PQ'], 67.869)
+    assert np.isclose(parsed_results['SQ'], 80.898)
+    assert np.isclose(parsed_results['RQ'], 83.333)
+    assert np.isclose(parsed_results['PQ_th'], 60.453)
+    assert np.isclose(parsed_results['SQ_th'], 79.996)
+    assert np.isclose(parsed_results['RQ_th'], 75.000)
+    assert np.isclose(parsed_results['PQ_st'], 82.701)
+    assert np.isclose(parsed_results['SQ_st'], 82.701)
+    assert np.isclose(parsed_results['RQ_st'], 100.000)
diff --git a/tests/test_data/test_datasets/test_xml_dataset.py b/tests/test_data/test_datasets/test_xml_dataset.py
index ebdd9e6f..f72f13d4 100644
--- a/tests/test_data/test_datasets/test_xml_dataset.py
+++ b/tests/test_data/test_datasets/test_xml_dataset.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 
 from mmdet.datasets import DATASETS
diff --git a/tests/test_data/test_pipelines/test_formatting.py b/tests/test_data/test_pipelines/test_formatting.py
index 8a2a3757..2e228984 100644
--- a/tests/test_data/test_pipelines/test_formatting.py
+++ b/tests/test_data/test_pipelines/test_formatting.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 
 from mmcv.utils import build_from_cfg
diff --git a/tests/test_data/test_pipelines/test_loading.py b/tests/test_data/test_pipelines/test_loading.py
index e28c3b04..760b09a0 100644
--- a/tests/test_data/test_pipelines/test_loading.py
+++ b/tests/test_data/test_pipelines/test_loading.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import os.path as osp
 
diff --git a/tests/test_data/test_pipelines/test_sampler.py b/tests/test_data/test_pipelines/test_sampler.py
index 1ba5c562..a44c978e 100644
--- a/tests/test_data/test_pipelines/test_sampler.py
+++ b/tests/test_data/test_pipelines/test_sampler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core.bbox.assigners import MaxIoUAssigner
diff --git a/tests/test_data/test_pipelines/test_transform/__init__.py b/tests/test_data/test_pipelines/test_transform/__init__.py
new file mode 100644
index 00000000..d4990310
--- /dev/null
+++ b/tests/test_data/test_pipelines/test_transform/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from .utils import check_result_same, construct_toy_data, create_random_bboxes
+
+__all__ = ['create_random_bboxes', 'construct_toy_data', 'check_result_same']
diff --git a/tests/test_data/test_pipelines/test_transform/test_img_augment.py b/tests/test_data/test_pipelines/test_transform/test_img_augment.py
index 8f7dd9eb..2a65fc2e 100644
--- a/tests/test_data/test_pipelines/test_transform/test_img_augment.py
+++ b/tests/test_data/test_pipelines/test_transform/test_img_augment.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import mmcv
@@ -5,38 +6,8 @@ import numpy as np
 from mmcv.utils import build_from_cfg
 from numpy.testing import assert_array_equal
 
-from mmdet.core.mask import BitmapMasks, PolygonMasks
 from mmdet.datasets.builder import PIPELINES
-
-
-def construct_toy_data(poly2mask=True):
-    img = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.uint8)
-    img = np.stack([img, img, img], axis=-1)
-    results = dict()
-    # image
-    results['img'] = img
-    results['img_shape'] = img.shape
-    results['img_fields'] = ['img']
-    # bboxes
-    results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
-    results['gt_bboxes'] = np.array([[0., 0., 2., 1.]], dtype=np.float32)
-    results['gt_bboxes_ignore'] = np.array([[2., 0., 3., 1.]],
-                                           dtype=np.float32)
-    # labels
-    results['gt_labels'] = np.array([1], dtype=np.int64)
-    # masks
-    results['mask_fields'] = ['gt_masks']
-    if poly2mask:
-        gt_masks = np.array([[0, 1, 1, 0], [0, 1, 0, 0]],
-                            dtype=np.uint8)[None, :, :]
-        results['gt_masks'] = BitmapMasks(gt_masks, 2, 4)
-    else:
-        raw_masks = [[np.array([1, 0, 2, 0, 2, 1, 1, 1], dtype=np.float)]]
-        results['gt_masks'] = PolygonMasks(raw_masks, 2, 4)
-    # segmentations
-    results['seg_fields'] = ['gt_semantic_seg']
-    results['gt_semantic_seg'] = img[..., 0]
-    return results
+from .utils import construct_toy_data
 
 
 def test_adjust_color():
diff --git a/tests/test_data/test_pipelines/test_transform/test_models_aug_test.py b/tests/test_data/test_pipelines/test_transform/test_models_aug_test.py
index d1f34b20..5eba1eff 100644
--- a/tests/test_data/test_pipelines/test_transform/test_models_aug_test.py
+++ b/tests/test_data/test_pipelines/test_transform/test_models_aug_test.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 
 import mmcv
diff --git a/tests/test_data/test_pipelines/test_transform/test_rotate.py b/tests/test_data/test_pipelines/test_transform/test_rotate.py
index c440451a..4b6f2072 100644
--- a/tests/test_data/test_pipelines/test_transform/test_rotate.py
+++ b/tests/test_data/test_pipelines/test_transform/test_rotate.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import numpy as np
@@ -6,60 +7,7 @@ from mmcv.utils import build_from_cfg
 
 from mmdet.core.mask import BitmapMasks, PolygonMasks
 from mmdet.datasets.builder import PIPELINES
-
-
-def construct_toy_data(poly2mask=True):
-    img = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.uint8)
-    img = np.stack([img, img, img], axis=-1)
-    results = dict()
-    # image
-    results['img'] = img
-    results['img_shape'] = img.shape
-    results['img_fields'] = ['img']
-    # bboxes
-    results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
-    results['gt_bboxes'] = np.array([[0., 0., 2., 1.]], dtype=np.float32)
-    results['gt_bboxes_ignore'] = np.array([[2., 0., 3., 1.]],
-                                           dtype=np.float32)
-    # labels
-    results['gt_labels'] = np.array([1], dtype=np.int64)
-    # masks
-    results['mask_fields'] = ['gt_masks']
-    if poly2mask:
-        gt_masks = np.array([[0, 1, 1, 0], [0, 1, 0, 0]],
-                            dtype=np.uint8)[None, :, :]
-        results['gt_masks'] = BitmapMasks(gt_masks, 2, 4)
-    else:
-        raw_masks = [[np.array([0, 0, 2, 0, 2, 1, 0, 1], dtype=np.float)]]
-        results['gt_masks'] = PolygonMasks(raw_masks, 2, 4)
-    # segmentations
-    results['seg_fields'] = ['gt_semantic_seg']
-    results['gt_semantic_seg'] = img[..., 0]
-    return results
-
-
-def _check_fields(results, results_rotated, keys):
-    for key in keys:
-        if isinstance(results[key], (BitmapMasks, PolygonMasks)):
-            assert np.equal(results[key].to_ndarray(),
-                            results_rotated[key].to_ndarray()).all()
-        else:
-            assert np.equal(results[key], results_rotated[key]).all()
-
-
-def check_rotate(results, results_rotated):
-    # check image
-    _check_fields(results, results_rotated, results.get('img_fields', ['img']))
-    # check bboxes
-    _check_fields(results, results_rotated, results.get('bbox_fields', []))
-    # check masks
-    _check_fields(results, results_rotated, results.get('mask_fields', []))
-    # check segmentations
-    _check_fields(results, results_rotated, results.get('seg_fields', []))
-    # _check gt_labels
-    if 'gt_labels' in results:
-        assert np.equal(results['gt_labels'],
-                        results_rotated['gt_labels']).all()
+from .utils import check_result_same, construct_toy_data
 
 
 def test_rotate():
@@ -104,14 +52,14 @@ def test_rotate():
     )
     rotate_module = build_from_cfg(transform, PIPELINES)
     results_wo_rotate = rotate_module(copy.deepcopy(results))
-    check_rotate(results, results_wo_rotate)
+    check_result_same(results, results_wo_rotate)
 
     # test case when no rotate aug (prob<=0)
     transform = dict(
         type='Rotate', level=10, prob=0., img_fill_val=img_fill_val, scale=0.6)
     rotate_module = build_from_cfg(transform, PIPELINES)
     results_wo_rotate = rotate_module(copy.deepcopy(results))
-    check_rotate(results, results_wo_rotate)
+    check_result_same(results, results_wo_rotate)
 
     # test clockwise rotation with angle 90
     results = construct_toy_data()
@@ -139,14 +87,14 @@ def test_rotate():
     results_gt['gt_semantic_seg'] = np.array(
         [[255, 6, 2, 255], [255, 7, 3,
                             255]]).astype(results['gt_semantic_seg'].dtype)
-    check_rotate(results_gt, results_rotated)
+    check_result_same(results_gt, results_rotated)
 
     # test clockwise rotation with angle 90, PolygonMasks
     results = construct_toy_data(poly2mask=False)
     results_rotated = rotate_module(copy.deepcopy(results))
     gt_masks = [[np.array([2, 0, 2, 1, 1, 1, 1, 0], dtype=np.float)]]
     results_gt['gt_masks'] = PolygonMasks(gt_masks, 2, 4)
-    check_rotate(results_gt, results_rotated)
+    check_result_same(results_gt, results_rotated)
 
     # test counter-clockwise roatation with angle 90,
     # and specify the ratation center
@@ -182,7 +130,7 @@ def test_rotate():
     gt_seg = (np.ones((h, w)) * 255).astype(results['gt_semantic_seg'].dtype)
     gt_seg[0, 0], gt_seg[0, 1] = 1, 5
     results_gt['gt_semantic_seg'] = gt_seg
-    check_rotate(results_gt, results_rotated)
+    check_result_same(results_gt, results_rotated)
 
     transform = dict(
         type='Rotate',
@@ -194,7 +142,7 @@ def test_rotate():
         prob=1.)
     rotate_module = build_from_cfg(transform, PIPELINES)
     results_rotated = rotate_module(copy.deepcopy(results))
-    check_rotate(results_gt, results_rotated)
+    check_result_same(results_gt, results_rotated)
 
     # test counter-clockwise roatation with angle 90,
     # and specify the ratation center, PolygonMasks
@@ -202,7 +150,7 @@ def test_rotate():
     results_rotated = rotate_module(copy.deepcopy(results))
     gt_masks = [[np.array([0, 0, 0, 0, 1, 0, 1, 0], dtype=np.float)]]
     results_gt['gt_masks'] = PolygonMasks(gt_masks, 2, 4)
-    check_rotate(results_gt, results_rotated)
+    check_result_same(results_gt, results_rotated)
 
     # test AutoAugment equipped with Rotate
     policies = [[dict(type='Rotate', level=10, prob=1.)]]
diff --git a/tests/test_data/test_pipelines/test_transform/test_shear.py b/tests/test_data/test_pipelines/test_transform/test_shear.py
index 3d638125..215d9a3f 100644
--- a/tests/test_data/test_pipelines/test_transform/test_shear.py
+++ b/tests/test_data/test_pipelines/test_transform/test_shear.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import numpy as np
@@ -6,62 +7,7 @@ from mmcv.utils import build_from_cfg
 
 from mmdet.core.mask import BitmapMasks, PolygonMasks
 from mmdet.datasets.builder import PIPELINES
-
-
-def construct_toy_data(poly2mask=True):
-    img = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.uint8)
-    img = np.stack([img, img, img], axis=-1)
-    results = dict()
-    # image
-    results['img'] = img
-    results['img_shape'] = img.shape
-    results['img_fields'] = ['img']
-    # bboxes
-    results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
-    results['gt_bboxes'] = np.array([[0., 0., 2., 1.]], dtype=np.float32)
-    results['gt_bboxes_ignore'] = np.array([[2., 0., 3., 1.]],
-                                           dtype=np.float32)
-    # labels
-    results['gt_labels'] = np.array([1], dtype=np.int64)
-    # masks
-    results['mask_fields'] = ['gt_masks']
-    if poly2mask:
-        gt_masks = np.array([[0, 1, 1, 0], [0, 1, 0, 0]],
-                            dtype=np.uint8)[None, :, :]
-        results['gt_masks'] = BitmapMasks(gt_masks, 2, 4)
-    else:
-        raw_masks = [[np.array([1, 0, 2, 0, 2, 1, 1, 1], dtype=np.float)]]
-        results['gt_masks'] = PolygonMasks(raw_masks, 2, 4)
-
-    # segmentations
-    results['seg_fields'] = ['gt_semantic_seg']
-    results['gt_semantic_seg'] = img[..., 0]
-    return results
-
-
-def _check_fields(results, results_sheared, keys):
-    for key in keys:
-        if isinstance(results[key], (BitmapMasks, PolygonMasks)):
-            assert np.equal(results[key].to_ndarray(),
-                            results_sheared[key].to_ndarray()).all()
-        else:
-            assert np.equal(results[key], results_sheared[key]).all()
-
-
-def check_shear(results, results_sheared):
-    # _check_keys(results, results_sheared)
-    # check image
-    _check_fields(results, results_sheared, results.get('img_fields', ['img']))
-    # check bboxes
-    _check_fields(results, results_sheared, results.get('bbox_fields', []))
-    # check masks
-    _check_fields(results, results_sheared, results.get('mask_fields', []))
-    # check segmentations
-    _check_fields(results, results_sheared, results.get('seg_fields', []))
-    # check gt_labels
-    if 'gt_labels' in results:
-        assert np.equal(results['gt_labels'],
-                        results_sheared['gt_labels']).all()
+from .utils import check_result_same, construct_toy_data
 
 
 def test_shear():
@@ -93,7 +39,7 @@ def test_shear():
         direction='horizontal')
     shear_module = build_from_cfg(transform, PIPELINES)
     results_wo_shear = shear_module(copy.deepcopy(results))
-    check_shear(results, results_wo_shear)
+    check_result_same(results, results_wo_shear)
 
     # test case when no shear aug (level=0, direction='vertical')
     transform = dict(
@@ -105,7 +51,7 @@ def test_shear():
         direction='vertical')
     shear_module = build_from_cfg(transform, PIPELINES)
     results_wo_shear = shear_module(copy.deepcopy(results))
-    check_shear(results, results_wo_shear)
+    check_result_same(results, results_wo_shear)
 
     # test case when no shear aug (prob<=0)
     transform = dict(
@@ -116,7 +62,7 @@ def test_shear():
         direction='vertical')
     shear_module = build_from_cfg(transform, PIPELINES)
     results_wo_shear = shear_module(copy.deepcopy(results))
-    check_shear(results, results_wo_shear)
+    check_result_same(results, results_wo_shear)
 
     # test shear horizontally, magnitude=1
     transform = dict(
@@ -142,14 +88,15 @@ def test_shear():
     results_gt['gt_masks'] = BitmapMasks(gt_masks, 2, 4)
     results_gt['gt_semantic_seg'] = np.array(
         [[1, 2, 3, 4], [255, 5, 6, 7]], dtype=results['gt_semantic_seg'].dtype)
-    check_shear(results_gt, results_sheared)
+    check_result_same(results_gt, results_sheared)
 
     # test PolygonMasks with shear horizontally, magnitude=1
     results = construct_toy_data(poly2mask=False)
     results_sheared = shear_module(copy.deepcopy(results))
-    gt_masks = [[np.array([1, 0, 2, 0, 3, 1, 2, 1], dtype=np.float)]]
+    print(results_sheared['gt_masks'])
+    gt_masks = [[np.array([0, 0, 2, 0, 3, 1, 1, 1], dtype=np.float)]]
     results_gt['gt_masks'] = PolygonMasks(gt_masks, 2, 4)
-    check_shear(results_gt, results_sheared)
+    check_result_same(results_gt, results_sheared)
 
     # test shear vertically, magnitude=-1
     img_fill_val = 128
@@ -179,14 +126,14 @@ def test_shear():
     results_gt['gt_semantic_seg'] = np.array(
         [[1, 6, 255, 255], [5, 255, 255, 255]],
         dtype=results['gt_semantic_seg'].dtype)
-    check_shear(results_gt, results_sheared)
+    check_result_same(results_gt, results_sheared)
 
     # test PolygonMasks with shear vertically, magnitude=-1
     results = construct_toy_data(poly2mask=False)
     results_sheared = shear_module(copy.deepcopy(results))
-    gt_masks = [[np.array([1, 0, 2, 0, 2, 0, 1, 0], dtype=np.float)]]
+    gt_masks = [[np.array([0, 0, 2, 0, 2, 0, 0, 1], dtype=np.float)]]
     results_gt['gt_masks'] = PolygonMasks(gt_masks, 2, 4)
-    check_shear(results_gt, results_sheared)
+    check_result_same(results_gt, results_sheared)
 
     results = construct_toy_data()
     # same mask for BitmapMasks and PolygonMasks
@@ -195,7 +142,7 @@ def test_shear():
         4)
     results['gt_bboxes'] = np.array([[1., 0., 2., 1.]], dtype=np.float32)
     results_sheared_bitmap = shear_module(copy.deepcopy(results))
-    check_shear(results_sheared_bitmap, results_sheared)
+    check_result_same(results_sheared_bitmap, results_sheared)
 
     # test AutoAugment equipped with Shear
     policies = [[dict(type='Shear', level=10, prob=1.)]]
diff --git a/tests/test_data/test_pipelines/test_transform/test_transform.py b/tests/test_data/test_pipelines/test_transform/test_transform.py
index 6742f315..d57f063d 100644
--- a/tests/test_data/test_pipelines/test_transform/test_transform.py
+++ b/tests/test_data/test_pipelines/test_transform/test_transform.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import os.path as osp
 
@@ -9,6 +10,7 @@ from mmcv.utils import build_from_cfg
 
 from mmdet.core.evaluation.bbox_overlaps import bbox_overlaps
 from mmdet.datasets.builder import PIPELINES
+from .utils import create_random_bboxes
 
 
 def test_resize():
@@ -77,6 +79,29 @@ def test_resize():
     results = resize_module(results)
     assert np.equal(results['img'], results['img2']).all()
     assert results['img_shape'] == (800, 1280, 3)
+    assert results['img'].dtype == results['img'].dtype == np.uint8
+
+    results_seg = {
+        'img': img,
+        'img_shape': img.shape,
+        'ori_shape': img.shape,
+        'gt_semantic_seg': copy.deepcopy(img),
+        'gt_seg': copy.deepcopy(img),
+        'seg_fields': ['gt_semantic_seg', 'gt_seg']
+    }
+    transform = dict(
+        type='Resize',
+        img_scale=(640, 400),
+        multiscale_mode='value',
+        keep_ratio=False)
+    resize_module = build_from_cfg(transform, PIPELINES)
+    results_seg = resize_module(results_seg)
+    assert results_seg['gt_semantic_seg'].shape == results_seg['gt_seg'].shape
+    assert results_seg['img_shape'] == (400, 640, 3)
+    assert results_seg['img_shape'] != results_seg['ori_shape']
+    assert results_seg['gt_semantic_seg'].shape == results_seg['img_shape']
+    assert np.equal(results_seg['gt_semantic_seg'],
+                    results_seg['gt_seg']).all()
 
 
 def test_flip():
@@ -199,17 +224,10 @@ def test_random_crop():
     results['pad_shape'] = img.shape
     results['scale_factor'] = 1.0
 
-    def create_random_bboxes(num_bboxes, img_w, img_h):
-        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
-        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
-        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
-        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
-            np.int)
-        return bboxes
-
     h, w, _ = img.shape
     gt_bboxes = create_random_bboxes(8, w, h)
     gt_bboxes_ignore = create_random_bboxes(2, w, h)
+    results['gt_labels'] = np.ones(gt_bboxes.shape[0], dtype=np.int64)
     results['gt_bboxes'] = gt_bboxes
     results['gt_bboxes_ignore'] = gt_bboxes_ignore
     transform = dict(type='RandomCrop', crop_size=(h - 20, w - 20))
@@ -218,6 +236,9 @@ def test_random_crop():
     assert results['img'].shape[:2] == (h - 20, w - 20)
     # All bboxes should be reserved after crop
     assert results['img_shape'][:2] == (h - 20, w - 20)
+    assert results['gt_labels'].shape[0] == results['gt_bboxes'].shape[0]
+    assert results['gt_labels'].dtype == np.int64
+    assert results['gt_bboxes'].dtype == np.float32
     assert results['gt_bboxes'].shape[0] == 8
     assert results['gt_bboxes_ignore'].shape[0] == 2
 
@@ -226,6 +247,8 @@ def test_random_crop():
 
     assert (area(results['gt_bboxes']) <= area(gt_bboxes)).all()
     assert (area(results['gt_bboxes_ignore']) <= area(gt_bboxes_ignore)).all()
+    assert results['gt_bboxes'].dtype == np.float32
+    assert results['gt_bboxes_ignore'].dtype == np.float32
 
     # test assertion for invalid crop_type
     with pytest.raises(ValueError):
@@ -268,6 +291,8 @@ def test_random_crop():
     h, w = results_transformed['img_shape'][:2]
     assert int(2 * 0.3 + 0.5) <= h <= int(2 * 1 + 0.5)
     assert int(4 * 0.7 + 0.5) <= w <= int(4 * 1 + 0.5)
+    assert results_transformed['gt_bboxes'].dtype == np.float32
+    assert results_transformed['gt_bboxes_ignore'].dtype == np.float32
 
     # test crop_type "relative"
     transform = dict(
@@ -279,6 +304,8 @@ def test_random_crop():
     results_transformed = transform_module(copy.deepcopy(results))
     h, w = results_transformed['img_shape'][:2]
     assert h == int(2 * 0.3 + 0.5) and w == int(4 * 0.7 + 0.5)
+    assert results_transformed['gt_bboxes'].dtype == np.float32
+    assert results_transformed['gt_bboxes_ignore'].dtype == np.float32
 
     # test crop_type "absolute"
     transform = dict(
@@ -290,6 +317,8 @@ def test_random_crop():
     results_transformed = transform_module(copy.deepcopy(results))
     h, w = results_transformed['img_shape'][:2]
     assert h == 1 and w == 2
+    assert results_transformed['gt_bboxes'].dtype == np.float32
+    assert results_transformed['gt_bboxes_ignore'].dtype == np.float32
 
     # test crop_type "absolute_range"
     transform = dict(
@@ -301,18 +330,11 @@ def test_random_crop():
     results_transformed = transform_module(copy.deepcopy(results))
     h, w = results_transformed['img_shape'][:2]
     assert 1 <= h <= 2 and 1 <= w <= 4
+    assert results_transformed['gt_bboxes'].dtype == np.float32
+    assert results_transformed['gt_bboxes_ignore'].dtype == np.float32
 
 
 def test_min_iou_random_crop():
-
-    def create_random_bboxes(num_bboxes, img_w, img_h):
-        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
-        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
-        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
-        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
-            np.int)
-        return bboxes
-
     results = dict()
     img = mmcv.imread(
         osp.join(osp.dirname(__file__), '../../../data/color.jpg'), 'color')
@@ -327,6 +349,7 @@ def test_min_iou_random_crop():
     h, w, _ = img.shape
     gt_bboxes = create_random_bboxes(1, w, h)
     gt_bboxes_ignore = create_random_bboxes(1, w, h)
+    results['gt_labels'] = np.ones(gt_bboxes.shape[0], dtype=np.int64)
     results['gt_bboxes'] = gt_bboxes
     results['gt_bboxes_ignore'] = gt_bboxes_ignore
     transform = dict(type='MinIoURandomCrop')
@@ -339,6 +362,11 @@ def test_min_iou_random_crop():
     with pytest.raises(AssertionError):
         crop_module(results_test)
     results = crop_module(results)
+    assert results['gt_labels'].shape[0] == results['gt_bboxes'].shape[0]
+    assert results['gt_labels'].dtype == np.int64
+    assert results['gt_bboxes'].dtype == np.float32
+    assert results['gt_bboxes_ignore'].dtype == np.float32
+
     patch = np.array([0, 0, results['img_shape'][1], results['img_shape'][0]])
     ious = bbox_overlaps(patch.reshape(-1, 4),
                          results['gt_bboxes']).reshape(-1)
@@ -403,6 +431,17 @@ def test_pad():
     results = transform(results)
     assert results['img'].shape[0] == results['img'].shape[1]
 
+    # test the pad_val is converted to a dict
+    transform = dict(type='Pad', size_divisor=32, pad_val=0)
+    with pytest.deprecated_call():
+        transform = build_from_cfg(transform, PIPELINES)
+
+    assert isinstance(transform.pad_val, dict)
+    results = transform(results)
+    img_shape = results['img'].shape
+    assert img_shape[0] % 32 == 0
+    assert img_shape[1] % 32 == 0
+
 
 def test_normalize():
     img_norm_cfg = dict(
@@ -554,14 +593,6 @@ def test_random_center_crop_pad():
     results = load(results)
     test_results = copy.deepcopy(results)
 
-    def create_random_bboxes(num_bboxes, img_w, img_h):
-        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
-        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
-        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
-        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
-            np.int)
-        return bboxes
-
     h, w, _ = results['img_shape']
     gt_bboxes = create_random_bboxes(8, w, h)
     gt_bboxes_ignore = create_random_bboxes(2, w, h)
@@ -584,6 +615,8 @@ def test_random_center_crop_pad():
     assert train_results['pad_shape'][:2] == (h - 20, w - 20)
     assert train_results['gt_bboxes'].shape[0] == 8
     assert train_results['gt_bboxes_ignore'].shape[0] == 2
+    assert train_results['gt_bboxes'].dtype == np.float32
+    assert train_results['gt_bboxes_ignore'].dtype == np.float32
 
     test_transform = dict(
         type='RandomCenterCropPad',
@@ -781,18 +814,10 @@ def test_random_shift():
     # TODO: add img_fields test
     results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
 
-    def create_random_bboxes(num_bboxes, img_w, img_h):
-        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
-        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
-        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
-        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
-            np.int)
-        return bboxes
-
     h, w, _ = img.shape
     gt_bboxes = create_random_bboxes(8, w, h)
     gt_bboxes_ignore = create_random_bboxes(2, w, h)
-    results['gt_labels'] = torch.ones(gt_bboxes.shape[0])
+    results['gt_labels'] = np.ones(gt_bboxes.shape[0], dtype=np.int64)
     results['gt_bboxes'] = gt_bboxes
     results['gt_bboxes_ignore'] = gt_bboxes_ignore
     transform = dict(type='RandomShift', shift_ratio=1.0)
@@ -801,6 +826,9 @@ def test_random_shift():
 
     assert results['img'].shape[:2] == (h, w)
     assert results['gt_labels'].shape[0] == results['gt_bboxes'].shape[0]
+    assert results['gt_labels'].dtype == np.int64
+    assert results['gt_bboxes'].dtype == np.float32
+    assert results['gt_bboxes_ignore'].dtype == np.float32
 
 
 def test_random_affine():
@@ -824,18 +852,10 @@ def test_random_affine():
     results['img'] = img
     results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
 
-    def create_random_bboxes(num_bboxes, img_w, img_h):
-        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
-        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
-        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
-        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
-            np.int)
-        return bboxes
-
     h, w, _ = img.shape
     gt_bboxes = create_random_bboxes(8, w, h)
     gt_bboxes_ignore = create_random_bboxes(2, w, h)
-    results['gt_labels'] = torch.ones(gt_bboxes.shape[0])
+    results['gt_labels'] = np.ones(gt_bboxes.shape[0], dtype=np.int64)
     results['gt_bboxes'] = gt_bboxes
     results['gt_bboxes_ignore'] = gt_bboxes_ignore
     transform = dict(type='RandomAffine')
@@ -844,10 +864,13 @@ def test_random_affine():
 
     assert results['img'].shape[:2] == (h, w)
     assert results['gt_labels'].shape[0] == results['gt_bboxes'].shape[0]
+    assert results['gt_labels'].dtype == np.int64
+    assert results['gt_bboxes'].dtype == np.float32
+    assert results['gt_bboxes_ignore'].dtype == np.float32
 
     # test filter bbox
-    gt_bboxes = np.array([[0, 0, 1, 1], [0, 0, 3, 100]])
-    results['gt_labels'] = torch.ones(gt_bboxes.shape[0])
+    gt_bboxes = np.array([[0, 0, 1, 1], [0, 0, 3, 100]], dtype=np.float32)
+    results['gt_labels'] = np.ones(gt_bboxes.shape[0], dtype=np.int64)
     results['gt_bboxes'] = gt_bboxes
     transform = dict(
         type='RandomAffine',
@@ -864,6 +887,10 @@ def test_random_affine():
 
     assert results['gt_bboxes'].shape[0] == 0
     assert results['gt_labels'].shape[0] == 0
+    assert results['gt_labels'].shape[0] == results['gt_bboxes'].shape[0]
+    assert results['gt_labels'].dtype == np.int64
+    assert results['gt_bboxes'].dtype == np.float32
+    assert results['gt_bboxes_ignore'].dtype == np.float32
 
 
 def test_mosaic():
@@ -879,18 +906,10 @@ def test_mosaic():
     # TODO: add img_fields test
     results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
 
-    def create_random_bboxes(num_bboxes, img_w, img_h):
-        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
-        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
-        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
-        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
-            np.int)
-        return bboxes
-
     h, w, _ = img.shape
     gt_bboxes = create_random_bboxes(8, w, h)
     gt_bboxes_ignore = create_random_bboxes(2, w, h)
-    results['gt_labels'] = torch.ones(gt_bboxes.shape[0])
+    results['gt_labels'] = np.ones(gt_bboxes.shape[0], dtype=np.int64)
     results['gt_bboxes'] = gt_bboxes
     results['gt_bboxes_ignore'] = gt_bboxes_ignore
     transform = dict(type='Mosaic', img_scale=(10, 12))
@@ -903,6 +922,10 @@ def test_mosaic():
     results['mix_results'] = [copy.deepcopy(results)] * 3
     results = mosaic_module(results)
     assert results['img'].shape[:2] == (20, 24)
+    assert results['gt_labels'].shape[0] == results['gt_bboxes'].shape[0]
+    assert results['gt_labels'].dtype == np.int64
+    assert results['gt_bboxes'].dtype == np.float32
+    assert results['gt_bboxes_ignore'].dtype == np.float32
 
 
 def test_mixup():
@@ -918,18 +941,10 @@ def test_mixup():
     # TODO: add img_fields test
     results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
 
-    def create_random_bboxes(num_bboxes, img_w, img_h):
-        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
-        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
-        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
-        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
-            np.int)
-        return bboxes
-
     h, w, _ = img.shape
     gt_bboxes = create_random_bboxes(8, w, h)
     gt_bboxes_ignore = create_random_bboxes(2, w, h)
-    results['gt_labels'] = torch.ones(gt_bboxes.shape[0])
+    results['gt_labels'] = np.ones(gt_bboxes.shape[0], dtype=np.int64)
     results['gt_bboxes'] = gt_bboxes
     results['gt_bboxes_ignore'] = gt_bboxes_ignore
     transform = dict(type='MixUp', img_scale=(10, 12))
@@ -946,3 +961,7 @@ def test_mixup():
     results['mix_results'] = [copy.deepcopy(results)]
     results = mixup_module(results)
     assert results['img'].shape[:2] == (288, 512)
+    assert results['gt_labels'].shape[0] == results['gt_bboxes'].shape[0]
+    assert results['gt_labels'].dtype == np.int64
+    assert results['gt_bboxes'].dtype == np.float32
+    assert results['gt_bboxes_ignore'].dtype == np.float32
diff --git a/tests/test_data/test_pipelines/test_transform/test_translate.py b/tests/test_data/test_pipelines/test_transform/test_translate.py
index 87f37d0d..8a1f9ddb 100644
--- a/tests/test_data/test_pipelines/test_transform/test_translate.py
+++ b/tests/test_data/test_pipelines/test_transform/test_translate.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 
 import numpy as np
diff --git a/tests/test_data/test_pipelines/test_transform/utils.py b/tests/test_data/test_pipelines/test_transform/utils.py
new file mode 100644
index 00000000..4075c8a9
--- /dev/null
+++ b/tests/test_data/test_pipelines/test_transform/utils.py
@@ -0,0 +1,78 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import numpy as np
+
+from mmdet.core.mask import BitmapMasks, PolygonMasks
+
+
+def _check_fields(results, pipeline_results, keys):
+    """Check data in fields from two results are same."""
+    for key in keys:
+        if isinstance(results[key], (BitmapMasks, PolygonMasks)):
+            assert np.equal(results[key].to_ndarray(),
+                            pipeline_results[key].to_ndarray()).all()
+        else:
+            assert np.equal(results[key], pipeline_results[key]).all()
+            assert results[key].dtype == pipeline_results[key].dtype
+
+
+def check_result_same(results, pipeline_results):
+    """Check whether the `pipeline_results` is the same with the predefined
+    `results`.
+
+    Args:
+        results (dict): Predefined results which should be the standard output
+            of the transform pipeline.
+        pipeline_results (dict): Results processed by the transform pipeline.
+    """
+    # check image
+    _check_fields(results, pipeline_results,
+                  results.get('img_fields', ['img']))
+    # check bboxes
+    _check_fields(results, pipeline_results, results.get('bbox_fields', []))
+    # check masks
+    _check_fields(results, pipeline_results, results.get('mask_fields', []))
+    # check segmentations
+    _check_fields(results, pipeline_results, results.get('seg_fields', []))
+    # check gt_labels
+    if 'gt_labels' in results:
+        assert np.equal(results['gt_labels'],
+                        pipeline_results['gt_labels']).all()
+
+
+def construct_toy_data(poly2mask=True):
+    img = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.uint8)
+    img = np.stack([img, img, img], axis=-1)
+    results = dict()
+    # image
+    results['img'] = img
+    results['img_shape'] = img.shape
+    results['img_fields'] = ['img']
+    # bboxes
+    results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']
+    results['gt_bboxes'] = np.array([[0., 0., 2., 1.]], dtype=np.float32)
+    results['gt_bboxes_ignore'] = np.array([[2., 0., 3., 1.]],
+                                           dtype=np.float32)
+    # labels
+    results['gt_labels'] = np.array([1], dtype=np.int64)
+    # masks
+    results['mask_fields'] = ['gt_masks']
+    if poly2mask:
+        gt_masks = np.array([[0, 1, 1, 0], [0, 1, 0, 0]],
+                            dtype=np.uint8)[None, :, :]
+        results['gt_masks'] = BitmapMasks(gt_masks, 2, 4)
+    else:
+        raw_masks = [[np.array([0, 0, 2, 0, 2, 1, 0, 1], dtype=np.float)]]
+        results['gt_masks'] = PolygonMasks(raw_masks, 2, 4)
+    # segmentations
+    results['seg_fields'] = ['gt_semantic_seg']
+    results['gt_semantic_seg'] = img[..., 0]
+    return results
+
+
+def create_random_bboxes(num_bboxes, img_w, img_h):
+    bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))
+    bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))
+    bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)
+    bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(
+        np.float32)
+    return bboxes
diff --git a/tests/test_data/test_utils.py b/tests/test_data/test_utils.py
index cd612f2f..289df32c 100644
--- a/tests/test_data/test_utils.py
+++ b/tests/test_data/test_utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 
 from mmdet.datasets import get_loading_pipeline, replace_ImageToTensor
diff --git a/tests/test_downstream/test_mmtrack.py b/tests/test_downstream/test_mmtrack.py
index ae95d9cf..b709d5b4 100644
--- a/tests/test_downstream/test_mmtrack.py
+++ b/tests/test_downstream/test_mmtrack.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 from collections import defaultdict
 
diff --git a/tests/test_metrics/test_box_overlap.py b/tests/test_metrics/test_box_overlap.py
index 94c6400f..1d032532 100644
--- a/tests/test_metrics/test_box_overlap.py
+++ b/tests/test_metrics/test_box_overlap.py
@@ -1,8 +1,11 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import pytest
 import torch
 
 from mmdet.core import BboxOverlaps2D, bbox_overlaps
+from mmdet.core.evaluation.bbox_overlaps import \
+    bbox_overlaps as recall_overlaps
 
 
 def test_bbox_overlaps_2d(eps=1e-7):
@@ -103,3 +106,29 @@ def test_bbox_overlaps_2d(eps=1e-7):
     ious = bbox_overlaps(bboxes1, bboxes2, 'iof', eps=eps)
     assert torch.all(ious >= -1) and torch.all(ious <= 1)
     assert ious.size() == (bboxes1.size(0), bboxes2.size(0))
+
+
+def test_voc_recall_overlaps():
+
+    def _construct_bbox(num_bbox=None):
+        img_h = int(np.random.randint(3, 1000))
+        img_w = int(np.random.randint(3, 1000))
+        if num_bbox is None:
+            num_bbox = np.random.randint(1, 10)
+        x1y1 = torch.rand((num_bbox, 2))
+        x2y2 = torch.max(torch.rand((num_bbox, 2)), x1y1)
+        bboxes = torch.cat((x1y1, x2y2), -1)
+        bboxes[:, 0::2] *= img_w
+        bboxes[:, 1::2] *= img_h
+        return bboxes.numpy(), num_bbox
+
+    bboxes1, num_bbox = _construct_bbox()
+    bboxes2, _ = _construct_bbox(num_bbox)
+    ious = recall_overlaps(
+        bboxes1, bboxes2, 'iou', use_legacy_coordinate=False)
+    assert ious.shape == (num_bbox, num_bbox)
+    assert np.all(ious >= -1) and np.all(ious <= 1)
+
+    ious = recall_overlaps(bboxes1, bboxes2, 'iou', use_legacy_coordinate=True)
+    assert ious.shape == (num_bbox, num_bbox)
+    assert np.all(ious >= -1) and np.all(ious <= 1)
diff --git a/tests/test_metrics/test_losses.py b/tests/test_metrics/test_losses.py
index dac85e1d..5136e3fd 100644
--- a/tests/test_metrics/test_losses.py
+++ b/tests/test_metrics/test_losses.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
diff --git a/tests/test_metrics/test_mean_ap.py b/tests/test_metrics/test_mean_ap.py
new file mode 100644
index 00000000..e5fe3cdf
--- /dev/null
+++ b/tests/test_metrics/test_mean_ap.py
@@ -0,0 +1,87 @@
+import numpy as np
+
+from mmdet.core.evaluation.mean_ap import eval_map, tpfp_default, tpfp_imagenet
+
+det_bboxes = np.array([
+    [0, 0, 10, 10],
+    [10, 10, 20, 20],
+    [32, 32, 38, 42],
+])
+gt_bboxes = np.array([[0, 0, 10, 20], [0, 10, 10, 19], [10, 10, 20, 20]])
+gt_ignore = np.array([[5, 5, 10, 20], [6, 10, 10, 19]])
+
+
+def test_tpfp_imagenet():
+
+    result = tpfp_imagenet(
+        det_bboxes,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_ignore,
+        use_legacy_coordinate=True)
+    tp = result[0]
+    fp = result[1]
+    assert tp.shape == (1, 3)
+    assert fp.shape == (1, 3)
+    assert (tp == np.array([[1, 1, 0]])).all()
+    assert (fp == np.array([[0, 0, 1]])).all()
+
+    result = tpfp_imagenet(
+        det_bboxes,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_ignore,
+        use_legacy_coordinate=False)
+    tp = result[0]
+    fp = result[1]
+    assert tp.shape == (1, 3)
+    assert fp.shape == (1, 3)
+    assert (tp == np.array([[1, 1, 0]])).all()
+    assert (fp == np.array([[0, 0, 1]])).all()
+
+
+def test_tpfp_default():
+
+    result = tpfp_default(
+        det_bboxes,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_ignore,
+        use_legacy_coordinate=True)
+
+    tp = result[0]
+    fp = result[1]
+    assert tp.shape == (1, 3)
+    assert fp.shape == (1, 3)
+    assert (tp == np.array([[1, 1, 0]])).all()
+    assert (fp == np.array([[0, 0, 1]])).all()
+    result = tpfp_default(
+        det_bboxes,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_ignore,
+        use_legacy_coordinate=False)
+
+    tp = result[0]
+    fp = result[1]
+    assert tp.shape == (1, 3)
+    assert fp.shape == (1, 3)
+    assert (tp == np.array([[1, 1, 0]])).all()
+    assert (fp == np.array([[0, 0, 1]])).all()
+
+
+def test_eval_map():
+
+    # 2 image and 2 classes
+    det_results = [[det_bboxes, det_bboxes], [det_bboxes, det_bboxes]]
+
+    labels = np.array([0, 1, 1])
+    labels_ignore = np.array([0, 1])
+    gt_info = {
+        'bboxes': gt_bboxes,
+        'bboxes_ignore': gt_ignore,
+        'labels': labels,
+        'labels_ignore': labels_ignore
+    }
+    annotations = [gt_info, gt_info]
+    mean_ap, eval_results = eval_map(
+        det_results, annotations, use_legacy_coordinate=True)
+    assert 0.291 < mean_ap < 0.293
+    eval_map(det_results, annotations, use_legacy_coordinate=False)
+    assert 0.291 < mean_ap < 0.293
diff --git a/tests/test_metrics/test_recall.py b/tests/test_metrics/test_recall.py
new file mode 100644
index 00000000..f2ca0b19
--- /dev/null
+++ b/tests/test_metrics/test_recall.py
@@ -0,0 +1,46 @@
+import numpy as np
+
+from mmdet.core.evaluation.recall import eval_recalls
+
+det_bboxes = np.array([
+    [0, 0, 10, 10],
+    [10, 10, 20, 20],
+    [32, 32, 38, 42],
+])
+gt_bboxes = np.array([[0, 0, 10, 20], [0, 10, 10, 19], [10, 10, 20, 20]])
+gt_ignore = np.array([[5, 5, 10, 20], [6, 10, 10, 19]])
+
+
+def test_eval_recalls():
+    gts = [gt_bboxes, gt_bboxes, gt_bboxes]
+    proposals = [det_bboxes, det_bboxes, det_bboxes]
+
+    recall = eval_recalls(
+        gts, proposals, proposal_nums=2, use_legacy_coordinate=True)
+    assert recall.shape == (1, 1)
+    assert 0.66 < recall[0][0] < 0.667
+    recall = eval_recalls(
+        gts, proposals, proposal_nums=2, use_legacy_coordinate=False)
+    assert recall.shape == (1, 1)
+    assert 0.66 < recall[0][0] < 0.667
+
+    recall = eval_recalls(
+        gts, proposals, proposal_nums=2, use_legacy_coordinate=True)
+    assert recall.shape == (1, 1)
+    assert 0.66 < recall[0][0] < 0.667
+    recall = eval_recalls(
+        gts,
+        proposals,
+        iou_thrs=[0.1, 0.9],
+        proposal_nums=2,
+        use_legacy_coordinate=False)
+    assert recall.shape == (1, 2)
+    assert recall[0][1] <= recall[0][0]
+    recall = eval_recalls(
+        gts,
+        proposals,
+        iou_thrs=[0.1, 0.9],
+        proposal_nums=2,
+        use_legacy_coordinate=True)
+    assert recall.shape == (1, 2)
+    assert recall[0][1] <= recall[0][0]
diff --git a/tests/test_models/test_backbones/__init__.py b/tests/test_models/test_backbones/__init__.py
index ce4596a6..eb431ba2 100644
--- a/tests/test_models/test_backbones/__init__.py
+++ b/tests/test_models/test_backbones/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .utils import check_norm_state, is_block, is_norm
 
 __all__ = ['is_block', 'is_norm', 'check_norm_state']
diff --git a/tests/test_models/test_backbones/test_csp_darknet.py b/tests/test_models/test_backbones/test_csp_darknet.py
index ceecc12c..2a2ad41d 100644
--- a/tests/test_models/test_backbones/test_csp_darknet.py
+++ b/tests/test_models/test_backbones/test_csp_darknet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 from torch.nn.modules.batchnorm import _BatchNorm
@@ -18,7 +19,6 @@ def test_csp_darknet_backbone():
     # Test CSPDarknet with first stage frozen
     frozen_stages = 1
     model = CSPDarknet(frozen_stages=frozen_stages)
-    model.init_weights()
     model.train()
 
     for mod in model.stem.modules():
@@ -34,105 +34,83 @@ def test_csp_darknet_backbone():
 
     # Test CSPDarknet with norm_eval=True
     model = CSPDarknet(norm_eval=True)
-    model.init_weights()
     model.train()
 
     assert check_norm_state(model.modules(), False)
 
-    # Test CSPDarknet-P5 forward with widen_factor=1.0
-    model = CSPDarknet(arch='P5', widen_factor=1.0, out_indices=range(0, 5))
-    model.init_weights()
-    model.train()
-
-    assert check_norm_state(model.modules(), True)
-
-    imgs = torch.randn(1, 3, 224, 224)
-    feat = model(imgs)
-    assert len(feat) == 5
-    assert feat[0].shape == torch.Size((1, 64, 112, 112))
-    assert feat[1].shape == torch.Size((1, 128, 56, 56))
-    assert feat[2].shape == torch.Size((1, 256, 28, 28))
-    assert feat[3].shape == torch.Size((1, 512, 14, 14))
-    assert feat[4].shape == torch.Size((1, 1024, 7, 7))
-
     # Test CSPDarknet-P5 forward with widen_factor=0.5
-    model = CSPDarknet(arch='P5', widen_factor=0.5, out_indices=range(0, 5))
-    model.init_weights()
+    model = CSPDarknet(arch='P5', widen_factor=0.25, out_indices=range(0, 5))
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 64, 64)
     feat = model(imgs)
     assert len(feat) == 5
-    assert feat[0].shape == torch.Size((1, 32, 112, 112))
-    assert feat[1].shape == torch.Size((1, 64, 56, 56))
-    assert feat[2].shape == torch.Size((1, 128, 28, 28))
-    assert feat[3].shape == torch.Size((1, 256, 14, 14))
-    assert feat[4].shape == torch.Size((1, 512, 7, 7))
+    assert feat[0].shape == torch.Size((1, 16, 32, 32))
+    assert feat[1].shape == torch.Size((1, 32, 16, 16))
+    assert feat[2].shape == torch.Size((1, 64, 8, 8))
+    assert feat[3].shape == torch.Size((1, 128, 4, 4))
+    assert feat[4].shape == torch.Size((1, 256, 2, 2))
 
-    # Test CSPDarknet-P6 forward with widen_factor=1.5
+    # Test CSPDarknet-P6 forward with widen_factor=0.5
     model = CSPDarknet(
         arch='P6',
-        widen_factor=1.5,
+        widen_factor=0.25,
         out_indices=range(0, 6),
         spp_kernal_sizes=(3, 5, 7))
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 320, 320)
+    imgs = torch.randn(1, 3, 128, 128)
     feat = model(imgs)
-    assert feat[0].shape == torch.Size((1, 96, 160, 160))
-    assert feat[1].shape == torch.Size((1, 192, 80, 80))
-    assert feat[2].shape == torch.Size((1, 384, 40, 40))
-    assert feat[3].shape == torch.Size((1, 768, 20, 20))
-    assert feat[4].shape == torch.Size((1, 1152, 10, 10))
-    assert feat[5].shape == torch.Size((1, 1536, 5, 5))
+    assert feat[0].shape == torch.Size((1, 16, 64, 64))
+    assert feat[1].shape == torch.Size((1, 32, 32, 32))
+    assert feat[2].shape == torch.Size((1, 64, 16, 16))
+    assert feat[3].shape == torch.Size((1, 128, 8, 8))
+    assert feat[4].shape == torch.Size((1, 192, 4, 4))
+    assert feat[5].shape == torch.Size((1, 256, 2, 2))
 
     # Test CSPDarknet forward with dict(type='ReLU')
     model = CSPDarknet(
-        widen_factor=1.0, act_cfg=dict(type='ReLU'), out_indices=range(0, 5))
-    model.init_weights()
+        widen_factor=0.125, act_cfg=dict(type='ReLU'), out_indices=range(0, 5))
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 64, 64)
     feat = model(imgs)
     assert len(feat) == 5
-    assert feat[0].shape == torch.Size((1, 64, 112, 112))
-    assert feat[1].shape == torch.Size((1, 128, 56, 56))
-    assert feat[2].shape == torch.Size((1, 256, 28, 28))
-    assert feat[3].shape == torch.Size((1, 512, 14, 14))
-    assert feat[4].shape == torch.Size((1, 1024, 7, 7))
+    assert feat[0].shape == torch.Size((1, 8, 32, 32))
+    assert feat[1].shape == torch.Size((1, 16, 16, 16))
+    assert feat[2].shape == torch.Size((1, 32, 8, 8))
+    assert feat[3].shape == torch.Size((1, 64, 4, 4))
+    assert feat[4].shape == torch.Size((1, 128, 2, 2))
 
     # Test CSPDarknet with BatchNorm forward
-    model = CSPDarknet(widen_factor=1.0, out_indices=range(0, 5))
+    model = CSPDarknet(widen_factor=0.125, out_indices=range(0, 5))
     for m in model.modules():
         if is_norm(m):
             assert isinstance(m, _BatchNorm)
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 64, 64)
     feat = model(imgs)
     assert len(feat) == 5
-    assert feat[0].shape == torch.Size((1, 64, 112, 112))
-    assert feat[1].shape == torch.Size((1, 128, 56, 56))
-    assert feat[2].shape == torch.Size((1, 256, 28, 28))
-    assert feat[3].shape == torch.Size((1, 512, 14, 14))
-    assert feat[4].shape == torch.Size((1, 1024, 7, 7))
+    assert feat[0].shape == torch.Size((1, 8, 32, 32))
+    assert feat[1].shape == torch.Size((1, 16, 16, 16))
+    assert feat[2].shape == torch.Size((1, 32, 8, 8))
+    assert feat[3].shape == torch.Size((1, 64, 4, 4))
+    assert feat[4].shape == torch.Size((1, 128, 2, 2))
 
     # Test CSPDarknet with custom arch forward
     arch_ovewrite = [[32, 56, 3, True, False], [56, 224, 2, True, False],
                      [224, 512, 1, True, False]]
     model = CSPDarknet(
         arch_ovewrite=arch_ovewrite,
-        widen_factor=1.0,
+        widen_factor=0.25,
         out_indices=(0, 1, 2, 3))
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size((1, 32, 112, 112))
-    assert feat[1].shape == torch.Size((1, 56, 56, 56))
-    assert feat[2].shape == torch.Size((1, 224, 28, 28))
-    assert feat[3].shape == torch.Size((1, 512, 14, 14))
+    assert feat[0].shape == torch.Size((1, 8, 16, 16))
+    assert feat[1].shape == torch.Size((1, 14, 8, 8))
+    assert feat[2].shape == torch.Size((1, 56, 4, 4))
+    assert feat[3].shape == torch.Size((1, 128, 2, 2))
diff --git a/tests/test_models/test_backbones/test_detectors_resnet.py b/tests/test_models/test_backbones/test_detectors_resnet.py
index fbd817d6..69f462a5 100644
--- a/tests/test_models/test_backbones/test_detectors_resnet.py
+++ b/tests/test_models/test_backbones/test_detectors_resnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 
 from mmdet.models.backbones import DetectoRS_ResNet
@@ -18,7 +19,7 @@ def test_detectorrs_resnet_backbone():
         output_img=True)
     """Test init_weights config"""
     with pytest.raises(AssertionError):
-        # pretrained and init_cfg cannot be setting at the same time
+        # pretrained and init_cfg cannot be specified at the same time
         DetectoRS_ResNet(
             **detectorrs_cfg, pretrained='Pretrained', init_cfg='Pretrained')
 
diff --git a/tests/test_models/test_backbones/test_hourglass.py b/tests/test_models/test_backbones/test_hourglass.py
index 363c94d5..c26f9c05 100644
--- a/tests/test_models/test_backbones/test_hourglass.py
+++ b/tests/test_models/test_backbones/test_hourglass.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
@@ -23,22 +24,26 @@ def test_hourglass_backbone():
             stage_blocks=[2, 2, 2, 2, 2])
 
     # Test HourglassNet-52
-    model = HourglassNet(num_stacks=1)
-    model.init_weights()
+    model = HourglassNet(
+        num_stacks=1,
+        stage_channels=(64, 64, 96, 96, 96, 128),
+        feat_channel=64)
     model.train()
 
     imgs = torch.randn(1, 3, 256, 256)
     feat = model(imgs)
     assert len(feat) == 1
-    assert feat[0].shape == torch.Size([1, 256, 64, 64])
+    assert feat[0].shape == torch.Size([1, 64, 64, 64])
 
     # Test HourglassNet-104
-    model = HourglassNet(num_stacks=2)
-    model.init_weights()
+    model = HourglassNet(
+        num_stacks=2,
+        stage_channels=(64, 64, 96, 96, 96, 128),
+        feat_channel=64)
     model.train()
 
     imgs = torch.randn(1, 3, 256, 256)
     feat = model(imgs)
     assert len(feat) == 2
-    assert feat[0].shape == torch.Size([1, 256, 64, 64])
-    assert feat[1].shape == torch.Size([1, 256, 64, 64])
+    assert feat[0].shape == torch.Size([1, 64, 64, 64])
+    assert feat[1].shape == torch.Size([1, 64, 64, 64])
diff --git a/tests/test_models/test_backbones/test_hrnet.py b/tests/test_models/test_backbones/test_hrnet.py
new file mode 100644
index 00000000..6ae367b4
--- /dev/null
+++ b/tests/test_models/test_backbones/test_hrnet.py
@@ -0,0 +1,111 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import pytest
+import torch
+
+from mmdet.models.backbones.hrnet import HRModule, HRNet
+from mmdet.models.backbones.resnet import BasicBlock, Bottleneck
+
+
+@pytest.mark.parametrize('block', [BasicBlock, Bottleneck])
+def test_hrmodule(block):
+    # Test multiscale forward
+    num_channles = (32, 64)
+    in_channels = [c * block.expansion for c in num_channles]
+    hrmodule = HRModule(
+        num_branches=2,
+        blocks=block,
+        in_channels=in_channels,
+        num_blocks=(4, 4),
+        num_channels=num_channles,
+    )
+
+    feats = [
+        torch.randn(1, in_channels[0], 64, 64),
+        torch.randn(1, in_channels[1], 32, 32)
+    ]
+    feats = hrmodule(feats)
+
+    assert len(feats) == 2
+    assert feats[0].shape == torch.Size([1, in_channels[0], 64, 64])
+    assert feats[1].shape == torch.Size([1, in_channels[1], 32, 32])
+
+    # Test single scale forward
+    num_channles = (32, 64)
+    in_channels = [c * block.expansion for c in num_channles]
+    hrmodule = HRModule(
+        num_branches=2,
+        blocks=block,
+        in_channels=in_channels,
+        num_blocks=(4, 4),
+        num_channels=num_channles,
+        multiscale_output=False,
+    )
+
+    feats = [
+        torch.randn(1, in_channels[0], 64, 64),
+        torch.randn(1, in_channels[1], 32, 32)
+    ]
+    feats = hrmodule(feats)
+
+    assert len(feats) == 1
+    assert feats[0].shape == torch.Size([1, in_channels[0], 64, 64])
+
+
+def test_hrnet_backbone():
+    # only have 3 stages
+    extra = dict(
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block='BOTTLENECK',
+            num_blocks=(4, ),
+            num_channels=(64, )),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(32, 64)),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(32, 64, 128)))
+
+    with pytest.raises(AssertionError):
+        # HRNet now only support 4 stages
+        HRNet(extra=extra)
+    extra['stage4'] = dict(
+        num_modules=3,
+        num_branches=3,  # should be 4
+        block='BASIC',
+        num_blocks=(4, 4, 4, 4),
+        num_channels=(32, 64, 128, 256))
+
+    with pytest.raises(AssertionError):
+        # len(num_blocks) should equal num_branches
+        HRNet(extra=extra)
+
+    extra['stage4']['num_branches'] = 4
+
+    # Test hrnetv2p_w32
+    model = HRNet(extra=extra)
+    model.init_weights()
+    model.train()
+
+    imgs = torch.randn(1, 3, 256, 256)
+    feats = model(imgs)
+    assert len(feats) == 4
+    assert feats[0].shape == torch.Size([1, 32, 64, 64])
+    assert feats[3].shape == torch.Size([1, 256, 8, 8])
+
+    # Test single scale output
+    model = HRNet(extra=extra, multiscale_output=False)
+    model.init_weights()
+    model.train()
+
+    imgs = torch.randn(1, 3, 256, 256)
+    feats = model(imgs)
+    assert len(feats) == 1
+    assert feats[0].shape == torch.Size([1, 32, 64, 64])
diff --git a/tests/test_models/test_backbones/test_mobilenet_v2.py b/tests/test_models/test_backbones/test_mobilenet_v2.py
index 1f879be8..77df7ea5 100644
--- a/tests/test_models/test_backbones/test_mobilenet_v2.py
+++ b/tests/test_models/test_backbones/test_mobilenet_v2.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 from torch.nn.modules import GroupNorm
@@ -19,7 +20,6 @@ def test_mobilenetv2_backbone():
     # Test MobileNetV2 with first stage frozen
     frozen_stages = 1
     model = MobileNetV2(frozen_stages=frozen_stages)
-    model.init_weights()
     model.train()
 
     for mod in model.conv1.modules():
@@ -35,14 +35,12 @@ def test_mobilenetv2_backbone():
 
     # Test MobileNetV2 with norm_eval=True
     model = MobileNetV2(norm_eval=True)
-    model.init_weights()
     model.train()
 
     assert check_norm_state(model.modules(), False)
 
     # Test MobileNetV2 forward with widen_factor=1.0
     model = MobileNetV2(widen_factor=1.0, out_indices=range(0, 8))
-    model.init_weights()
     model.train()
 
     assert check_norm_state(model.modules(), True)
@@ -61,7 +59,6 @@ def test_mobilenetv2_backbone():
 
     # Test MobileNetV2 forward with widen_factor=0.5
     model = MobileNetV2(widen_factor=0.5, out_indices=range(0, 7))
-    model.init_weights()
     model.train()
 
     imgs = torch.randn(1, 3, 224, 224)
@@ -77,7 +74,6 @@ def test_mobilenetv2_backbone():
 
     # Test MobileNetV2 forward with widen_factor=2.0
     model = MobileNetV2(widen_factor=2.0, out_indices=range(0, 8))
-    model.init_weights()
     model.train()
 
     imgs = torch.randn(1, 3, 224, 224)
@@ -94,7 +90,6 @@ def test_mobilenetv2_backbone():
     # Test MobileNetV2 forward with dict(type='ReLU')
     model = MobileNetV2(
         widen_factor=1.0, act_cfg=dict(type='ReLU'), out_indices=range(0, 7))
-    model.init_weights()
     model.train()
 
     imgs = torch.randn(1, 3, 224, 224)
@@ -113,7 +108,6 @@ def test_mobilenetv2_backbone():
     for m in model.modules():
         if is_norm(m):
             assert isinstance(m, _BatchNorm)
-    model.init_weights()
     model.train()
 
     imgs = torch.randn(1, 3, 224, 224)
@@ -135,7 +129,6 @@ def test_mobilenetv2_backbone():
     for m in model.modules():
         if is_norm(m):
             assert isinstance(m, GroupNorm)
-    model.init_weights()
     model.train()
 
     imgs = torch.randn(1, 3, 224, 224)
@@ -151,7 +144,6 @@ def test_mobilenetv2_backbone():
 
     # Test MobileNetV2 with layers 1, 3, 5 out forward
     model = MobileNetV2(widen_factor=1.0, out_indices=(0, 2, 4))
-    model.init_weights()
     model.train()
 
     imgs = torch.randn(1, 3, 224, 224)
@@ -167,7 +159,6 @@ def test_mobilenetv2_backbone():
     for m in model.modules():
         if is_block(m):
             assert m.with_cp
-    model.init_weights()
     model.train()
 
     imgs = torch.randn(1, 3, 224, 224)
diff --git a/tests/test_models/test_backbones/test_pvt.py b/tests/test_models/test_backbones/test_pvt.py
new file mode 100644
index 00000000..25be3acc
--- /dev/null
+++ b/tests/test_models/test_backbones/test_pvt.py
@@ -0,0 +1,103 @@
+import pytest
+import torch
+
+from mmdet.models.backbones.pvt import (PVTEncoderLayer,
+                                        PyramidVisionTransformer,
+                                        PyramidVisionTransformerV2)
+
+
+def test_pvt_block():
+    # test PVT structure and forward
+    block = PVTEncoderLayer(
+        embed_dims=64, num_heads=4, feedforward_channels=256)
+    assert block.ffn.embed_dims == 64
+    assert block.attn.num_heads == 4
+    assert block.ffn.feedforward_channels == 256
+    x = torch.randn(1, 56 * 56, 64)
+    x_out = block(x, (56, 56))
+    assert x_out.shape == torch.Size([1, 56 * 56, 64])
+
+
+def test_pvt():
+    """Test PVT backbone."""
+
+    with pytest.raises(TypeError):
+        # Pretrained arg must be str or None.
+        PyramidVisionTransformer(pretrained=123)
+
+    # test pretrained image size
+    with pytest.raises(AssertionError):
+        PyramidVisionTransformer(pretrain_img_size=(224, 224, 224))
+
+    # Test absolute position embedding
+    temp = torch.randn((1, 3, 224, 224))
+    model = PyramidVisionTransformer(
+        pretrain_img_size=224, use_abs_pos_embed=True)
+    model.init_weights()
+    model(temp)
+
+    # Test normal inference
+    temp = torch.randn((1, 3, 512, 512))
+    model = PyramidVisionTransformer()
+    outs = model(temp)
+    assert outs[0].shape == (1, 64, 128, 128)
+    assert outs[1].shape == (1, 128, 64, 64)
+    assert outs[2].shape == (1, 320, 32, 32)
+    assert outs[3].shape == (1, 512, 16, 16)
+
+    # Test abnormal inference size
+    temp = torch.randn((1, 3, 511, 511))
+    model = PyramidVisionTransformer()
+    outs = model(temp)
+    assert outs[0].shape == (1, 64, 127, 127)
+    assert outs[1].shape == (1, 128, 63, 63)
+    assert outs[2].shape == (1, 320, 31, 31)
+    assert outs[3].shape == (1, 512, 15, 15)
+
+    # Test abnormal inference size
+    temp = torch.randn((1, 3, 112, 137))
+    model = PyramidVisionTransformer()
+    outs = model(temp)
+    assert outs[0].shape == (1, 64, 28, 34)
+    assert outs[1].shape == (1, 128, 14, 17)
+    assert outs[2].shape == (1, 320, 7, 8)
+    assert outs[3].shape == (1, 512, 3, 4)
+
+
+def test_pvtv2():
+    """Test PVTv2 backbone."""
+
+    with pytest.raises(TypeError):
+        # Pretrained arg must be str or None.
+        PyramidVisionTransformerV2(pretrained=123)
+
+    # test pretrained image size
+    with pytest.raises(AssertionError):
+        PyramidVisionTransformerV2(pretrain_img_size=(224, 224, 224))
+
+    # Test normal inference
+    temp = torch.randn((1, 3, 512, 512))
+    model = PyramidVisionTransformerV2()
+    outs = model(temp)
+    assert outs[0].shape == (1, 64, 128, 128)
+    assert outs[1].shape == (1, 128, 64, 64)
+    assert outs[2].shape == (1, 320, 32, 32)
+    assert outs[3].shape == (1, 512, 16, 16)
+
+    # Test abnormal inference size
+    temp = torch.randn((1, 3, 511, 511))
+    model = PyramidVisionTransformerV2()
+    outs = model(temp)
+    assert outs[0].shape == (1, 64, 128, 128)
+    assert outs[1].shape == (1, 128, 64, 64)
+    assert outs[2].shape == (1, 320, 32, 32)
+    assert outs[3].shape == (1, 512, 16, 16)
+
+    # Test abnormal inference size
+    temp = torch.randn((1, 3, 112, 137))
+    model = PyramidVisionTransformerV2()
+    outs = model(temp)
+    assert outs[0].shape == (1, 64, 28, 35)
+    assert outs[1].shape == (1, 128, 14, 18)
+    assert outs[2].shape == (1, 320, 7, 9)
+    assert outs[3].shape == (1, 512, 4, 5)
diff --git a/tests/test_models/test_backbones/test_regnet.py b/tests/test_models/test_backbones/test_regnet.py
index 81d4abce..2f94b115 100644
--- a/tests/test_models/test_backbones/test_regnet.py
+++ b/tests/test_models/test_backbones/test_regnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
@@ -39,20 +40,19 @@ def test_regnet_backbone(arch_name, arch, out_channels):
 
     # Test RegNet with arch_name
     model = RegNet(arch_name)
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, out_channels[0], 56, 56])
-    assert feat[1].shape == torch.Size([1, out_channels[1], 28, 28])
-    assert feat[2].shape == torch.Size([1, out_channels[2], 14, 14])
-    assert feat[3].shape == torch.Size([1, out_channels[3], 7, 7])
+    assert feat[0].shape == torch.Size([1, out_channels[0], 8, 8])
+    assert feat[1].shape == torch.Size([1, out_channels[1], 4, 4])
+    assert feat[2].shape == torch.Size([1, out_channels[2], 2, 2])
+    assert feat[3].shape == torch.Size([1, out_channels[3], 1, 1])
 
     # Test RegNet with arch
     model = RegNet(arch)
-    assert feat[0].shape == torch.Size([1, out_channels[0], 56, 56])
-    assert feat[1].shape == torch.Size([1, out_channels[1], 28, 28])
-    assert feat[2].shape == torch.Size([1, out_channels[2], 14, 14])
-    assert feat[3].shape == torch.Size([1, out_channels[3], 7, 7])
+    assert feat[0].shape == torch.Size([1, out_channels[0], 8, 8])
+    assert feat[1].shape == torch.Size([1, out_channels[1], 4, 4])
+    assert feat[2].shape == torch.Size([1, out_channels[2], 2, 2])
+    assert feat[3].shape == torch.Size([1, out_channels[3], 1, 1])
diff --git a/tests/test_models/test_backbones/test_renext.py b/tests/test_models/test_backbones/test_renext.py
index d01443e4..4ce2ee61 100644
--- a/tests/test_models/test_backbones/test_renext.py
+++ b/tests/test_models/test_backbones/test_renext.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
@@ -65,16 +66,15 @@ def test_resnext_backbone():
     for m in model.modules():
         if is_block(m):
             assert m.conv2.groups == 32
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 256, 8, 8])
+    assert feat[1].shape == torch.Size([1, 512, 4, 4])
+    assert feat[2].shape == torch.Size([1, 1024, 2, 2])
+    assert feat[3].shape == torch.Size([1, 2048, 1, 1])
 
 
 regnet_test_data = [
diff --git a/tests/test_models/test_backbones/test_res2net.py b/tests/test_models/test_backbones/test_res2net.py
index 95d0118c..6757869d 100644
--- a/tests/test_models/test_backbones/test_res2net.py
+++ b/tests/test_models/test_backbones/test_res2net.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
@@ -50,13 +51,12 @@ def test_res2net_backbone():
     for m in model.modules():
         if is_block(m):
             assert m.scales == 4
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 256, 8, 8])
+    assert feat[1].shape == torch.Size([1, 512, 4, 4])
+    assert feat[2].shape == torch.Size([1, 1024, 2, 2])
+    assert feat[3].shape == torch.Size([1, 2048, 1, 1])
diff --git a/tests/test_models/test_backbones/test_resnest.py b/tests/test_models/test_backbones/test_resnest.py
index 22435916..245fdfd3 100644
--- a/tests/test_models/test_backbones/test_resnest.py
+++ b/tests/test_models/test_backbones/test_resnest.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
@@ -12,15 +13,15 @@ def test_resnest_bottleneck():
 
     # Test ResNeSt Bottleneck structure
     block = BottleneckS(
-        64, 256, radix=2, reduction_factor=4, stride=2, style='pytorch')
+        2, 4, radix=2, reduction_factor=4, stride=2, style='pytorch')
     assert block.avd_layer.stride == 2
-    assert block.conv2.channels == 256
+    assert block.conv2.channels == 4
 
     # Test ResNeSt Bottleneck forward
-    block = BottleneckS(64, 16, radix=2, reduction_factor=4)
-    x = torch.randn(2, 64, 56, 56)
+    block = BottleneckS(16, 4, radix=2, reduction_factor=4)
+    x = torch.randn(2, 16, 56, 56)
     x_out = block(x)
-    assert x_out.shape == torch.Size([2, 64, 56, 56])
+    assert x_out.shape == torch.Size([2, 16, 56, 56])
 
 
 def test_resnest_backbone():
@@ -30,14 +31,17 @@ def test_resnest_backbone():
 
     # Test ResNeSt with radix 2, reduction_factor 4
     model = ResNeSt(
-        depth=50, radix=2, reduction_factor=4, out_indices=(0, 1, 2, 3))
-    model.init_weights()
+        depth=50,
+        base_channels=4,
+        radix=2,
+        reduction_factor=4,
+        out_indices=(0, 1, 2, 3))
     model.train()
 
-    imgs = torch.randn(2, 3, 224, 224)
+    imgs = torch.randn(2, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([2, 256, 56, 56])
-    assert feat[1].shape == torch.Size([2, 512, 28, 28])
-    assert feat[2].shape == torch.Size([2, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([2, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([2, 16, 8, 8])
+    assert feat[1].shape == torch.Size([2, 32, 4, 4])
+    assert feat[2].shape == torch.Size([2, 64, 2, 2])
+    assert feat[3].shape == torch.Size([2, 128, 1, 1])
diff --git a/tests/test_models/test_backbones/test_resnet.py b/tests/test_models/test_backbones/test_resnet.py
index afbdf1c9..5448828f 100644
--- a/tests/test_models/test_backbones/test_resnet.py
+++ b/tests/test_models/test_backbones/test_resnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 from mmcv import assert_params_all_zeros
@@ -310,45 +311,37 @@ def test_resnest_stem():
     assert model.conv1.out_channels == 64
     assert model.norm1.num_features == 64
 
-    # Test default stem_channels, with base_channels=32
-    model = ResNet(50, base_channels=32)
-    assert model.stem_channels == 32
-    assert model.conv1.out_channels == 32
-    assert model.norm1.num_features == 32
-    assert model.layer1[0].conv1.in_channels == 32
-
-    # Test stem_channels=64
-    model = ResNet(50, stem_channels=64)
-    assert model.stem_channels == 64
-    assert model.conv1.out_channels == 64
-    assert model.norm1.num_features == 64
-    assert model.layer1[0].conv1.in_channels == 64
-
-    # Test stem_channels=64, with base_channels=32
-    model = ResNet(50, stem_channels=64, base_channels=32)
-    assert model.stem_channels == 64
-    assert model.conv1.out_channels == 64
-    assert model.norm1.num_features == 64
-    assert model.layer1[0].conv1.in_channels == 64
-
-    # Test stem_channels=128
-    model = ResNet(depth=50, stem_channels=128)
-    model.init_weights()
-    model.train()
-    assert model.conv1.out_channels == 128
-    assert model.layer1[0].conv1.in_channels == 128
+    # Test default stem_channels, with base_channels=3
+    model = ResNet(50, base_channels=3)
+    assert model.stem_channels == 3
+    assert model.conv1.out_channels == 3
+    assert model.norm1.num_features == 3
+    assert model.layer1[0].conv1.in_channels == 3
+
+    # Test stem_channels=3
+    model = ResNet(50, stem_channels=3)
+    assert model.stem_channels == 3
+    assert model.conv1.out_channels == 3
+    assert model.norm1.num_features == 3
+    assert model.layer1[0].conv1.in_channels == 3
+
+    # Test stem_channels=3, with base_channels=2
+    model = ResNet(50, stem_channels=3, base_channels=2)
+    assert model.stem_channels == 3
+    assert model.conv1.out_channels == 3
+    assert model.norm1.num_features == 3
+    assert model.layer1[0].conv1.in_channels == 3
 
     # Test V1d stem_channels
-    model = ResNetV1d(depth=50, stem_channels=128)
-    model.init_weights()
+    model = ResNetV1d(depth=50, stem_channels=6)
     model.train()
-    assert model.stem[0].out_channels == 64
-    assert model.stem[1].num_features == 64
-    assert model.stem[3].out_channels == 64
-    assert model.stem[4].num_features == 64
-    assert model.stem[6].out_channels == 128
-    assert model.stem[7].num_features == 128
-    assert model.layer1[0].conv1.in_channels == 128
+    assert model.stem[0].out_channels == 3
+    assert model.stem[1].num_features == 3
+    assert model.stem[3].out_channels == 3
+    assert model.stem[4].num_features == 3
+    assert model.stem[6].out_channels == 6
+    assert model.stem[7].num_features == 6
+    assert model.layer1[0].conv1.in_channels == 6
 
 
 def test_resnet_backbone():
@@ -387,29 +380,25 @@ def test_resnet_backbone():
     with pytest.raises(TypeError):
         # pretrained must be a string path
         model = ResNet(50, pretrained=0)
-        model.init_weights()
 
     with pytest.raises(AssertionError):
         # Style must be in ['pytorch', 'caffe']
         ResNet(50, style='tensorflow')
 
     # Test ResNet50 norm_eval=True
-    model = ResNet(50, norm_eval=True)
-    model.init_weights()
+    model = ResNet(50, norm_eval=True, base_channels=1)
     model.train()
     assert check_norm_state(model.modules(), False)
 
     # Test ResNet50 with torchvision pretrained weight
     model = ResNet(
         depth=50, norm_eval=True, pretrained='torchvision://resnet50')
-    model.init_weights()
     model.train()
     assert check_norm_state(model.modules(), False)
 
     # Test ResNet50 with first stage frozen
     frozen_stages = 1
-    model = ResNet(50, frozen_stages=frozen_stages)
-    model.init_weights()
+    model = ResNet(50, frozen_stages=frozen_stages, base_channels=1)
     model.train()
     assert model.norm1.training is False
     for layer in [model.conv1, model.norm1]:
@@ -424,9 +413,8 @@ def test_resnet_backbone():
             assert param.requires_grad is False
 
     # Test ResNet50V1d with first stage frozen
-    model = ResNetV1d(depth=50, frozen_stages=frozen_stages)
+    model = ResNetV1d(depth=50, frozen_stages=frozen_stages, base_channels=2)
     assert len(model.stem) == 9
-    model.init_weights()
     model.train()
     assert check_norm_state(model.stem, False)
     for param in model.stem.parameters():
@@ -441,16 +429,15 @@ def test_resnet_backbone():
 
     # Test ResNet18 forward
     model = ResNet(18)
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 64, 56, 56])
-    assert feat[1].shape == torch.Size([1, 128, 28, 28])
-    assert feat[2].shape == torch.Size([1, 256, 14, 14])
-    assert feat[3].shape == torch.Size([1, 512, 7, 7])
+    assert feat[0].shape == torch.Size([1, 64, 8, 8])
+    assert feat[1].shape == torch.Size([1, 128, 4, 4])
+    assert feat[2].shape == torch.Size([1, 256, 2, 2])
+    assert feat[3].shape == torch.Size([1, 512, 1, 1])
 
     # Test ResNet18 with checkpoint forward
     model = ResNet(18, with_cp=True)
@@ -459,65 +446,63 @@ def test_resnet_backbone():
             assert m.with_cp
 
     # Test ResNet50 with BatchNorm forward
-    model = ResNet(50)
+    model = ResNet(50, base_channels=1)
     for m in model.modules():
         if is_norm(m):
             assert isinstance(m, _BatchNorm)
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 4, 8, 8])
+    assert feat[1].shape == torch.Size([1, 8, 4, 4])
+    assert feat[2].shape == torch.Size([1, 16, 2, 2])
+    assert feat[3].shape == torch.Size([1, 32, 1, 1])
 
     # Test ResNet50 with layers 1, 2, 3 out forward
-    model = ResNet(50, out_indices=(0, 1, 2))
-    model.init_weights()
+    model = ResNet(50, out_indices=(0, 1, 2), base_channels=1)
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 3
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
+    assert feat[0].shape == torch.Size([1, 4, 8, 8])
+    assert feat[1].shape == torch.Size([1, 8, 4, 4])
+    assert feat[2].shape == torch.Size([1, 16, 2, 2])
 
     # Test ResNet50 with checkpoint forward
-    model = ResNet(50, with_cp=True)
+    model = ResNet(50, with_cp=True, base_channels=1)
     for m in model.modules():
         if is_block(m):
             assert m.with_cp
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 4, 8, 8])
+    assert feat[1].shape == torch.Size([1, 8, 4, 4])
+    assert feat[2].shape == torch.Size([1, 16, 2, 2])
+    assert feat[3].shape == torch.Size([1, 32, 1, 1])
 
     # Test ResNet50 with GroupNorm forward
     model = ResNet(
-        50, norm_cfg=dict(type='GN', num_groups=32, requires_grad=True))
+        50,
+        base_channels=4,
+        norm_cfg=dict(type='GN', num_groups=2, requires_grad=True))
     for m in model.modules():
         if is_norm(m):
             assert isinstance(m, GroupNorm)
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 16, 8, 8])
+    assert feat[1].shape == torch.Size([1, 32, 4, 4])
+    assert feat[2].shape == torch.Size([1, 64, 2, 2])
+    assert feat[3].shape == torch.Size([1, 128, 1, 1])
 
     # Test ResNet50 with 1 GeneralizedAttention after conv2, 1 NonLocal2D
     # after conv2, 1 ContextBlock after conv3 in layers 2, 3, 4
@@ -537,39 +522,38 @@ def test_resnet_backbone():
             stages=(False, True, True, False),
             position='after_conv3')
     ]
-    model = ResNet(50, plugins=plugins)
+    model = ResNet(50, plugins=plugins, base_channels=8)
     for m in model.layer1.modules():
         if is_block(m):
             assert not hasattr(m, 'context_block')
             assert not hasattr(m, 'gen_attention_block')
-            assert m.nonlocal_block.in_channels == 64
+            assert m.nonlocal_block.in_channels == 8
     for m in model.layer2.modules():
         if is_block(m):
-            assert m.nonlocal_block.in_channels == 128
-            assert m.gen_attention_block.in_channels == 128
-            assert m.context_block.in_channels == 512
+            assert m.nonlocal_block.in_channels == 16
+            assert m.gen_attention_block.in_channels == 16
+            assert m.context_block.in_channels == 64
 
     for m in model.layer3.modules():
         if is_block(m):
-            assert m.nonlocal_block.in_channels == 256
-            assert m.gen_attention_block.in_channels == 256
-            assert m.context_block.in_channels == 1024
+            assert m.nonlocal_block.in_channels == 32
+            assert m.gen_attention_block.in_channels == 32
+            assert m.context_block.in_channels == 128
 
     for m in model.layer4.modules():
         if is_block(m):
-            assert m.nonlocal_block.in_channels == 512
-            assert m.gen_attention_block.in_channels == 512
+            assert m.nonlocal_block.in_channels == 64
+            assert m.gen_attention_block.in_channels == 64
             assert not hasattr(m, 'context_block')
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 32, 8, 8])
+    assert feat[1].shape == torch.Size([1, 64, 4, 4])
+    assert feat[2].shape == torch.Size([1, 128, 2, 2])
+    assert feat[3].shape == torch.Size([1, 256, 1, 1])
 
     # Test ResNet50 with 1 ContextBlock after conv2, 1 ContextBlock after
     # conv3 in layers 2, 3, 4
@@ -584,7 +568,7 @@ def test_resnet_backbone():
             position='after_conv3')
     ]
 
-    model = ResNet(50, plugins=plugins)
+    model = ResNet(50, plugins=plugins, base_channels=8)
     for m in model.layer1.modules():
         if is_block(m):
             assert not hasattr(m, 'context_block')
@@ -593,33 +577,32 @@ def test_resnet_backbone():
     for m in model.layer2.modules():
         if is_block(m):
             assert not hasattr(m, 'context_block')
-            assert m.context_block1.in_channels == 512
-            assert m.context_block2.in_channels == 512
+            assert m.context_block1.in_channels == 64
+            assert m.context_block2.in_channels == 64
 
     for m in model.layer3.modules():
         if is_block(m):
             assert not hasattr(m, 'context_block')
-            assert m.context_block1.in_channels == 1024
-            assert m.context_block2.in_channels == 1024
+            assert m.context_block1.in_channels == 128
+            assert m.context_block2.in_channels == 128
 
     for m in model.layer4.modules():
         if is_block(m):
             assert not hasattr(m, 'context_block')
             assert not hasattr(m, 'context_block1')
             assert not hasattr(m, 'context_block2')
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 32, 8, 8])
+    assert feat[1].shape == torch.Size([1, 64, 4, 4])
+    assert feat[2].shape == torch.Size([1, 128, 2, 2])
+    assert feat[3].shape == torch.Size([1, 256, 1, 1])
 
     # Test ResNet50 zero initialization of residual
-    model = ResNet(50, zero_init_residual=True)
+    model = ResNet(50, zero_init_residual=True, base_channels=1)
     model.init_weights()
     for m in model.modules():
         if isinstance(m, Bottleneck):
@@ -628,39 +611,22 @@ def test_resnet_backbone():
             assert assert_params_all_zeros(m.norm2)
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 4, 8, 8])
+    assert feat[1].shape == torch.Size([1, 8, 4, 4])
+    assert feat[2].shape == torch.Size([1, 16, 2, 2])
+    assert feat[3].shape == torch.Size([1, 32, 1, 1])
 
     # Test ResNetV1d forward
-    model = ResNetV1d(depth=50)
-    model.init_weights()
+    model = ResNetV1d(depth=50, base_channels=2)
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
-    feat = model(imgs)
-    assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
-
-    imgs = torch.randn(1, 3, 224, 224)
-    feat = model(imgs)
-    assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
-
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 4
-    assert feat[0].shape == torch.Size([1, 256, 56, 56])
-    assert feat[1].shape == torch.Size([1, 512, 28, 28])
-    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
-    assert feat[3].shape == torch.Size([1, 2048, 7, 7])
+    assert feat[0].shape == torch.Size([1, 8, 8, 8])
+    assert feat[1].shape == torch.Size([1, 16, 4, 4])
+    assert feat[2].shape == torch.Size([1, 32, 2, 2])
+    assert feat[3].shape == torch.Size([1, 64, 1, 1])
diff --git a/tests/test_models/test_backbones/test_swin.py b/tests/test_models/test_backbones/test_swin.py
new file mode 100644
index 00000000..180a202f
--- /dev/null
+++ b/tests/test_models/test_backbones/test_swin.py
@@ -0,0 +1,82 @@
+import pytest
+import torch
+
+from mmdet.models.backbones.swin import SwinBlock, SwinTransformer
+
+
+def test_swin_block():
+    # test SwinBlock structure and forward
+    block = SwinBlock(embed_dims=64, num_heads=4, feedforward_channels=256)
+    assert block.ffn.embed_dims == 64
+    assert block.attn.w_msa.num_heads == 4
+    assert block.ffn.feedforward_channels == 256
+    x = torch.randn(1, 56 * 56, 64)
+    x_out = block(x, (56, 56))
+    assert x_out.shape == torch.Size([1, 56 * 56, 64])
+
+    # Test BasicBlock with checkpoint forward
+    block = SwinBlock(
+        embed_dims=64, num_heads=4, feedforward_channels=256, with_cp=True)
+    assert block.with_cp
+    x = torch.randn(1, 56 * 56, 64)
+    x_out = block(x, (56, 56))
+    assert x_out.shape == torch.Size([1, 56 * 56, 64])
+
+
+def test_swin_transformer():
+    """Test Swin Transformer backbone."""
+
+    with pytest.raises(TypeError):
+        # Pretrained arg must be str or None.
+        SwinTransformer(pretrained=123)
+
+    with pytest.raises(AssertionError):
+        # Because swin uses non-overlapping patch embed, so the stride of patch
+        # embed must be equal to patch size.
+        SwinTransformer(strides=(2, 2, 2, 2), patch_size=4)
+
+    # test pretrained image size
+    with pytest.raises(AssertionError):
+        SwinTransformer(pretrain_img_size=(224, 224, 224))
+
+    # Test absolute position embedding
+    temp = torch.randn((1, 3, 224, 224))
+    model = SwinTransformer(pretrain_img_size=224, use_abs_pos_embed=True)
+    model.init_weights()
+    model(temp)
+
+    # Test patch norm
+    model = SwinTransformer(patch_norm=False)
+    model(temp)
+
+    # Test normal inference
+    temp = torch.randn((1, 3, 512, 512))
+    model = SwinTransformer()
+    outs = model(temp)
+    assert outs[0].shape == (1, 96, 128, 128)
+    assert outs[1].shape == (1, 192, 64, 64)
+    assert outs[2].shape == (1, 384, 32, 32)
+    assert outs[3].shape == (1, 768, 16, 16)
+
+    # Test abnormal inference size
+    temp = torch.randn((1, 3, 511, 511))
+    model = SwinTransformer()
+    outs = model(temp)
+    assert outs[0].shape == (1, 96, 128, 128)
+    assert outs[1].shape == (1, 192, 64, 64)
+    assert outs[2].shape == (1, 384, 32, 32)
+    assert outs[3].shape == (1, 768, 16, 16)
+
+    # Test abnormal inference size
+    temp = torch.randn((1, 3, 112, 137))
+    model = SwinTransformer()
+    outs = model(temp)
+    assert outs[0].shape == (1, 96, 28, 35)
+    assert outs[1].shape == (1, 192, 14, 18)
+    assert outs[2].shape == (1, 384, 7, 9)
+    assert outs[3].shape == (1, 768, 4, 5)
+
+    model = SwinTransformer(frozen_stages=4)
+    model.train()
+    for p in model.parameters():
+        assert not p.requires_grad
diff --git a/tests/test_models/test_backbones/test_trident_resnet.py b/tests/test_models/test_backbones/test_trident_resnet.py
index ebb4415b..a79b97e1 100644
--- a/tests/test_models/test_backbones/test_trident_resnet.py
+++ b/tests/test_models/test_backbones/test_trident_resnet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
@@ -171,10 +172,9 @@ def test_trident_resnet_backbone():
         TridentResNet(50, num_stages=4, **tridentresnet_config)
 
     model = TridentResNet(50, num_stages=3, **tridentresnet_config)
-    model.init_weights()
     model.train()
 
-    imgs = torch.randn(1, 3, 224, 224)
+    imgs = torch.randn(1, 3, 32, 32)
     feat = model(imgs)
     assert len(feat) == 1
-    assert feat[0].shape == torch.Size([3, 1024, 14, 14])
+    assert feat[0].shape == torch.Size([3, 1024, 2, 2])
diff --git a/tests/test_models/test_backbones/utils.py b/tests/test_models/test_backbones/utils.py
index 5314e4d8..9baa9944 100644
--- a/tests/test_models/test_backbones/utils.py
+++ b/tests/test_models/test_backbones/utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from torch.nn.modules import GroupNorm
 from torch.nn.modules.batchnorm import _BatchNorm
 
diff --git a/tests/test_models/test_dense_heads/test_anchor_head.py b/tests/test_models/test_dense_heads/test_anchor_head.py
index 23cb3640..7414be30 100644
--- a/tests/test_models/test_dense_heads/test_anchor_head.py
+++ b/tests/test_models/test_dense_heads/test_anchor_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_atss_head.py b/tests/test_models/test_dense_heads/test_atss_head.py
index 3757a345..18597f46 100644
--- a/tests/test_models/test_dense_heads/test_atss_head.py
+++ b/tests/test_models/test_dense_heads/test_atss_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_autoassign_head.py b/tests/test_models/test_dense_heads/test_autoassign_head.py
index ebcf6fed..72cdddf0 100644
--- a/tests/test_models/test_dense_heads/test_autoassign_head.py
+++ b/tests/test_models/test_dense_heads/test_autoassign_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_centernet_head.py b/tests/test_models/test_dense_heads/test_centernet_head.py
index 2ecb6184..8993a488 100644
--- a/tests/test_models/test_dense_heads/test_centernet_head.py
+++ b/tests/test_models/test_dense_heads/test_centernet_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 from mmcv import ConfigDict
diff --git a/tests/test_models/test_dense_heads/test_corner_head.py b/tests/test_models/test_dense_heads/test_corner_head.py
index 91d1c218..0b549ff4 100644
--- a/tests/test_models/test_dense_heads/test_corner_head.py
+++ b/tests/test_models/test_dense_heads/test_corner_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core.evaluation.bbox_overlaps import bbox_overlaps
diff --git a/tests/test_models/test_dense_heads/test_dense_heads_attr.py b/tests/test_models/test_dense_heads/test_dense_heads_attr.py
index f6be7f15..d4a57de0 100644
--- a/tests/test_models/test_dense_heads/test_dense_heads_attr.py
+++ b/tests/test_models/test_dense_heads/test_dense_heads_attr.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 
 from terminaltables import AsciiTable
diff --git a/tests/test_models/test_dense_heads/test_detr_head.py b/tests/test_models/test_dense_heads/test_detr_head.py
index 51f97d48..cc2da231 100644
--- a/tests/test_models/test_dense_heads/test_detr_head.py
+++ b/tests/test_models/test_dense_heads/test_detr_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmcv import ConfigDict
 
diff --git a/tests/test_models/test_dense_heads/test_fcos_head.py b/tests/test_models/test_dense_heads/test_fcos_head.py
index 663e8151..5fbe14f7 100644
--- a/tests/test_models/test_dense_heads/test_fcos_head.py
+++ b/tests/test_models/test_dense_heads/test_fcos_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_fsaf_head.py b/tests/test_models/test_dense_heads/test_fsaf_head.py
index 1d85937f..78510550 100644
--- a/tests/test_models/test_dense_heads/test_fsaf_head.py
+++ b/tests/test_models/test_dense_heads/test_fsaf_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_ga_anchor_head.py b/tests/test_models/test_dense_heads/test_ga_anchor_head.py
index 4da346d3..374f71b4 100644
--- a/tests/test_models/test_dense_heads/test_ga_anchor_head.py
+++ b/tests/test_models/test_dense_heads/test_ga_anchor_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_gfl_head.py b/tests/test_models/test_dense_heads/test_gfl_head.py
index b035a579..6c522fa7 100644
--- a/tests/test_models/test_dense_heads/test_gfl_head.py
+++ b/tests/test_models/test_dense_heads/test_gfl_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_ld_head.py b/tests/test_models/test_dense_heads/test_ld_head.py
index 6a7541ad..017135d1 100644
--- a/tests/test_models/test_dense_heads/test_ld_head.py
+++ b/tests/test_models/test_dense_heads/test_ld_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_paa_head.py b/tests/test_models/test_dense_heads/test_paa_head.py
index 224e9491..0fceb9aa 100644
--- a/tests/test_models/test_dense_heads/test_paa_head.py
+++ b/tests/test_models/test_dense_heads/test_paa_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import numpy as np
 import torch
diff --git a/tests/test_models/test_dense_heads/test_pisa_head.py b/tests/test_models/test_dense_heads/test_pisa_head.py
index 6b1d42db..996320ac 100644
--- a/tests/test_models/test_dense_heads/test_pisa_head.py
+++ b/tests/test_models/test_dense_heads/test_pisa_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_sabl_retina_head.py b/tests/test_models/test_dense_heads/test_sabl_retina_head.py
index c958be6f..4e89d9a2 100644
--- a/tests/test_models/test_dense_heads/test_sabl_retina_head.py
+++ b/tests/test_models/test_dense_heads/test_sabl_retina_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_solo_head.py b/tests/test_models/test_dense_heads/test_solo_head.py
new file mode 100644
index 00000000..16cb4f7c
--- /dev/null
+++ b/tests/test_models/test_dense_heads/test_solo_head.py
@@ -0,0 +1,284 @@
+import pytest
+import torch
+
+from mmdet.models.dense_heads import (DecoupledSOLOHead,
+                                      DecoupledSOLOLightHead, SOLOHead)
+
+
+def test_solo_head_loss():
+    """Tests solo head loss when truth is empty and non-empty."""
+    s = 256
+    img_metas = [{
+        'img_shape': (s, s, 3),
+        'scale_factor': 1,
+        'pad_shape': (s, s, 3)
+    }]
+    self = SOLOHead(
+        num_classes=4,
+        in_channels=1,
+        num_grids=[40, 36, 24, 16, 12],
+        loss_mask=dict(type='DiceLoss', use_sigmoid=True, loss_weight=3.0),
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0))
+    feat = [
+        torch.rand(1, 1, s // feat_size, s // feat_size)
+        for feat_size in [4, 8, 16, 32, 64]
+    ]
+    mask_preds, cls_preds = self.forward(feat)
+    # Test that empty ground truth encourages the network to
+    # predict background.
+    gt_bboxes = [torch.empty((0, 4))]
+    gt_labels = [torch.LongTensor([])]
+    gt_masks = [torch.empty((0, 550, 550))]
+    gt_bboxes_ignore = None
+    empty_gt_losses = self.loss(
+        mask_preds,
+        cls_preds,
+        gt_labels,
+        gt_masks,
+        img_metas,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_bboxes_ignore)
+    # When there is no truth, the cls loss should be nonzero but there should
+    # be no box loss.
+    empty_mask_loss = empty_gt_losses['loss_mask']
+    empty_cls_loss = empty_gt_losses['loss_cls']
+    assert empty_cls_loss.item() > 0, 'cls loss should be non-zero'
+    assert empty_mask_loss.item() == 0, (
+        'there should be no mask loss when there are no true masks')
+
+    # When truth is non-empty then both cls and box loss should be nonzero for
+    # random inputs.
+    gt_bboxes = [
+        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),
+    ]
+    gt_labels = [torch.LongTensor([2])]
+    gt_masks = [(torch.rand((1, 256, 256)) > 0.5).float()]
+    one_gt_losses = self.loss(
+        mask_preds,
+        cls_preds,
+        gt_labels,
+        gt_masks,
+        img_metas,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_bboxes_ignore)
+    onegt_mask_loss = one_gt_losses['loss_mask']
+    onegt_cls_loss = one_gt_losses['loss_cls']
+    assert onegt_cls_loss.item() > 0, 'cls loss should be non-zero'
+    assert onegt_mask_loss.item() > 0, 'mask loss should be non-zero'
+
+    # When the length of num_grids, scale_ranges, and num_levels are not equal.
+    with pytest.raises(AssertionError):
+        SOLOHead(
+            num_classes=4,
+            in_channels=1,
+            num_grids=[36, 24, 16, 12],
+            loss_mask=dict(type='DiceLoss', use_sigmoid=True, loss_weight=3.0),
+            loss_cls=dict(
+                type='FocalLoss',
+                use_sigmoid=True,
+                gamma=2.0,
+                alpha=0.25,
+                loss_weight=1.0))
+
+    # When input feature length is not equal to num_levels.
+    with pytest.raises(AssertionError):
+        feat = [
+            torch.rand(1, 1, s // feat_size, s // feat_size)
+            for feat_size in [4, 8, 16, 32]
+        ]
+        self.forward(feat)
+
+
+def test_desolo_head_loss():
+    """Tests solo head loss when truth is empty and non-empty."""
+    s = 256
+    img_metas = [{
+        'img_shape': (s, s, 3),
+        'scale_factor': 1,
+        'pad_shape': (s, s, 3)
+    }]
+    self = DecoupledSOLOHead(
+        num_classes=4,
+        in_channels=1,
+        num_grids=[40, 36, 24, 16, 12],
+        loss_mask=dict(
+            type='DiceLoss', use_sigmoid=True, activate=False,
+            loss_weight=3.0),
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0))
+    feat = [
+        torch.rand(1, 1, s // feat_size, s // feat_size)
+        for feat_size in [4, 8, 16, 32, 64]
+    ]
+    mask_preds_x, mask_preds_y, cls_preds = self.forward(feat)
+    # Test that empty ground truth encourages the network to
+    # predict background.
+    gt_bboxes = [torch.empty((0, 4))]
+    gt_labels = [torch.LongTensor([])]
+    gt_masks = [torch.empty((0, 550, 550))]
+    gt_bboxes_ignore = None
+    empty_gt_losses = self.loss(
+        mask_preds_x,
+        mask_preds_y,
+        cls_preds,
+        gt_labels,
+        gt_masks,
+        img_metas,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_bboxes_ignore)
+    # When there is no truth, the cls loss should be nonzero but there should
+    # be no box loss.
+    empty_mask_loss = empty_gt_losses['loss_mask']
+    empty_cls_loss = empty_gt_losses['loss_cls']
+    assert empty_cls_loss.item() > 0, 'cls loss should be non-zero'
+    assert empty_mask_loss.item() == 0, (
+        'there should be no mask loss when there are no true masks')
+
+    # When truth is non-empty then both cls and box loss should be nonzero for
+    # random inputs.
+    gt_bboxes = [
+        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),
+    ]
+    gt_labels = [torch.LongTensor([2])]
+    gt_masks = [(torch.rand((1, 256, 256)) > 0.5).float()]
+    one_gt_losses = self.loss(
+        mask_preds_x,
+        mask_preds_y,
+        cls_preds,
+        gt_labels,
+        gt_masks,
+        img_metas,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_bboxes_ignore)
+    onegt_mask_loss = one_gt_losses['loss_mask']
+    onegt_cls_loss = one_gt_losses['loss_cls']
+    assert onegt_cls_loss.item() > 0, 'cls loss should be non-zero'
+    assert onegt_mask_loss.item() > 0, 'mask loss should be non-zero'
+
+    # When the length of num_grids, scale_ranges, and num_levels are not equal.
+    with pytest.raises(AssertionError):
+        DecoupledSOLOHead(
+            num_classes=4,
+            in_channels=1,
+            num_grids=[36, 24, 16, 12],
+            loss_mask=dict(
+                type='DiceLoss',
+                use_sigmoid=True,
+                activate=False,
+                loss_weight=3.0),
+            loss_cls=dict(
+                type='FocalLoss',
+                use_sigmoid=True,
+                gamma=2.0,
+                alpha=0.25,
+                loss_weight=1.0))
+
+    # When input feature length is not equal to num_levels.
+    with pytest.raises(AssertionError):
+        feat = [
+            torch.rand(1, 1, s // feat_size, s // feat_size)
+            for feat_size in [4, 8, 16, 32]
+        ]
+        self.forward(feat)
+
+
+def test_desolo_light_head_loss():
+    """Tests solo head loss when truth is empty and non-empty."""
+    s = 256
+    img_metas = [{
+        'img_shape': (s, s, 3),
+        'scale_factor': 1,
+        'pad_shape': (s, s, 3)
+    }]
+    self = DecoupledSOLOLightHead(
+        num_classes=4,
+        in_channels=1,
+        num_grids=[40, 36, 24, 16, 12],
+        loss_mask=dict(
+            type='DiceLoss', use_sigmoid=True, activate=False,
+            loss_weight=3.0),
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0))
+    feat = [
+        torch.rand(1, 1, s // feat_size, s // feat_size)
+        for feat_size in [4, 8, 16, 32, 64]
+    ]
+    mask_preds_x, mask_preds_y, cls_preds = self.forward(feat)
+    # Test that empty ground truth encourages the network to
+    # predict background.
+    gt_bboxes = [torch.empty((0, 4))]
+    gt_labels = [torch.LongTensor([])]
+    gt_masks = [torch.empty((0, 550, 550))]
+    gt_bboxes_ignore = None
+    empty_gt_losses = self.loss(
+        mask_preds_x,
+        mask_preds_y,
+        cls_preds,
+        gt_labels,
+        gt_masks,
+        img_metas,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_bboxes_ignore)
+    # When there is no truth, the cls loss should be nonzero but there should
+    # be no box loss.
+    empty_mask_loss = empty_gt_losses['loss_mask']
+    empty_cls_loss = empty_gt_losses['loss_cls']
+    assert empty_cls_loss.item() > 0, 'cls loss should be non-zero'
+    assert empty_mask_loss.item() == 0, (
+        'there should be no mask loss when there are no true masks')
+
+    # When truth is non-empty then both cls and box loss should be nonzero for
+    # random inputs.
+    gt_bboxes = [
+        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),
+    ]
+    gt_labels = [torch.LongTensor([2])]
+    gt_masks = [(torch.rand((1, 256, 256)) > 0.5).float()]
+    one_gt_losses = self.loss(
+        mask_preds_x,
+        mask_preds_y,
+        cls_preds,
+        gt_labels,
+        gt_masks,
+        img_metas,
+        gt_bboxes,
+        gt_bboxes_ignore=gt_bboxes_ignore)
+    onegt_mask_loss = one_gt_losses['loss_mask']
+    onegt_cls_loss = one_gt_losses['loss_cls']
+    assert onegt_cls_loss.item() > 0, 'cls loss should be non-zero'
+    assert onegt_mask_loss.item() > 0, 'mask loss should be non-zero'
+
+    # When the length of num_grids, scale_ranges, and num_levels are not equal.
+    with pytest.raises(AssertionError):
+        DecoupledSOLOLightHead(
+            num_classes=4,
+            in_channels=1,
+            num_grids=[36, 24, 16, 12],
+            loss_mask=dict(type='DiceLoss', use_sigmoid=True, loss_weight=3.0),
+            loss_cls=dict(
+                type='FocalLoss',
+                use_sigmoid=True,
+                gamma=2.0,
+                alpha=0.25,
+                loss_weight=1.0))
+
+    # When input feature length is not equal to num_levels.
+    with pytest.raises(AssertionError):
+        feat = [
+            torch.rand(1, 1, s // feat_size, s // feat_size)
+            for feat_size in [4, 8, 16, 32]
+        ]
+        self.forward(feat)
diff --git a/tests/test_models/test_dense_heads/test_vfnet_head.py b/tests/test_models/test_dense_heads/test_vfnet_head.py
index 4fd43dd9..7fec4e57 100644
--- a/tests/test_models/test_dense_heads/test_vfnet_head.py
+++ b/tests/test_models/test_dense_heads/test_vfnet_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_yolact_head.py b/tests/test_models/test_dense_heads/test_yolact_head.py
index aff57c4a..e82e0d7f 100644
--- a/tests/test_models/test_dense_heads/test_yolact_head.py
+++ b/tests/test_models/test_dense_heads/test_yolact_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_yolof_head.py b/tests/test_models/test_dense_heads/test_yolof_head.py
index ef21b66c..98103744 100644
--- a/tests/test_models/test_dense_heads/test_yolof_head.py
+++ b/tests/test_models/test_dense_heads/test_yolof_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_dense_heads/test_yolox_head.py b/tests/test_models/test_dense_heads/test_yolox_head.py
index b9783a53..cb63527e 100644
--- a/tests/test_models/test_dense_heads/test_yolox_head.py
+++ b/tests/test_models/test_dense_heads/test_yolox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 from mmcv.cnn import ConvModule, DepthwiseSeparableConvModule
diff --git a/tests/test_models/test_forward.py b/tests/test_models/test_forward.py
index 50941a39..8a7c5e19 100644
--- a/tests/test_models/test_forward.py
+++ b/tests/test_models/test_forward.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """pytest tests/test_forward.py."""
 import copy
 from os.path import dirname, exists, join
@@ -42,14 +43,25 @@ def _get_detector_cfg(fname):
     return model
 
 
+def _replace_r50_with_r18(model):
+    """Replace ResNet50 with ResNet18 in config."""
+    model = copy.deepcopy(model)
+    if model.backbone.type == 'ResNet':
+        model.backbone.depth = 18
+        model.backbone.base_channels = 2
+        model.neck.in_channels = [2, 4, 8, 16]
+    return model
+
+
 def test_sparse_rcnn_forward():
     config_path = 'sparse_rcnn/sparse_rcnn_r50_fpn_1x_coco.py'
     model = _get_detector_cfg(config_path)
+    model = _replace_r50_with_r18(model)
     model.backbone.init_cfg = None
     from mmdet.models import build_detector
     detector = build_detector(model)
     detector.init_weights()
-    input_shape = (1, 3, 550, 550)
+    input_shape = (1, 3, 100, 100)
     mm_inputs = _demo_mm_inputs(input_shape, num_items=[5])
     imgs = mm_inputs.pop('imgs')
     img_metas = mm_inputs.pop('img_metas')
@@ -109,12 +121,13 @@ def test_sparse_rcnn_forward():
 
 def test_rpn_forward():
     model = _get_detector_cfg('rpn/rpn_r50_fpn_1x_coco.py')
+    model = _replace_r50_with_r18(model)
     model.backbone.init_cfg = None
 
     from mmdet.models import build_detector
     detector = build_detector(model)
 
-    input_shape = (1, 3, 224, 224)
+    input_shape = (1, 3, 100, 100)
     mm_inputs = _demo_mm_inputs(input_shape)
 
     imgs = mm_inputs.pop('imgs')
@@ -147,7 +160,8 @@ def test_rpn_forward():
         # 'free_anchor/retinanet_free_anchor_r50_fpn_1x_coco.py',
         # 'atss/atss_r50_fpn_1x_coco.py',  # not ready for topk
         'reppoints/reppoints_moment_r50_fpn_1x_coco.py',
-        'yolo/yolov3_d53_mstrain-608_273e_coco.py'
+        'yolo/yolov3_mobilenetv2_320_300e_coco.py',
+        'yolox/yolox_tiny_8x8_300e_coco.py'
     ])
 def test_single_stage_forward_gpu(cfg_file):
     if not torch.cuda.is_available():
@@ -155,12 +169,13 @@ def test_single_stage_forward_gpu(cfg_file):
         pytest.skip('test requires GPU and torch+cuda')
 
     model = _get_detector_cfg(cfg_file)
+    model = _replace_r50_with_r18(model)
     model.backbone.init_cfg = None
 
     from mmdet.models import build_detector
     detector = build_detector(model)
 
-    input_shape = (2, 3, 224, 224)
+    input_shape = (2, 3, 128, 128)
     mm_inputs = _demo_mm_inputs(input_shape)
 
     imgs = mm_inputs.pop('imgs')
@@ -192,12 +207,13 @@ def test_single_stage_forward_gpu(cfg_file):
 def test_faster_rcnn_ohem_forward():
     model = _get_detector_cfg(
         'faster_rcnn/faster_rcnn_r50_fpn_ohem_1x_coco.py')
+    model = _replace_r50_with_r18(model)
     model.backbone.init_cfg = None
 
     from mmdet.models import build_detector
     detector = build_detector(model)
 
-    input_shape = (1, 3, 256, 256)
+    input_shape = (1, 3, 100, 100)
 
     # Test forward train with a non-empty truth batch
     mm_inputs = _demo_mm_inputs(input_shape, num_items=[10])
@@ -231,21 +247,32 @@ def test_faster_rcnn_ohem_forward():
     loss, _ = detector._parse_losses(losses)
     assert float(loss.item()) > 0
 
+    # Test RoI forward train with an empty proposals
+    feature = detector.extract_feat(imgs[0][None, :])
+    losses = detector.roi_head.forward_train(
+        feature,
+        img_metas, [torch.empty((0, 5))],
+        gt_bboxes=gt_bboxes,
+        gt_labels=gt_labels)
+    assert isinstance(losses, dict)
+
 
 @pytest.mark.parametrize(
     'cfg_file',
     [
-        'cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py',
+        # 'cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py',
         'mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py',
-        'grid_rcnn/grid_rcnn_r50_fpn_gn-head_2x_coco.py',
-        'ms_rcnn/ms_rcnn_r50_fpn_1x_coco.py',
-        'htc/htc_r50_fpn_1x_coco.py',
-        'scnet/scnet_r50_fpn_20e_coco.py',
-        'seesaw_loss/mask_rcnn_r50_fpn_random_seesaw_loss_normed_mask_mstrain_2x_lvis_v1.py'  # noqa: E501
+        # 'grid_rcnn/grid_rcnn_r50_fpn_gn-head_2x_coco.py',
+        # 'ms_rcnn/ms_rcnn_r50_fpn_1x_coco.py',
+        # 'htc/htc_r50_fpn_1x_coco.py',
+        # 'panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py',
+        # 'scnet/scnet_r50_fpn_20e_coco.py',
+        # 'seesaw_loss/mask_rcnn_r50_fpn_random_seesaw_loss_normed_mask_mstrain_2x_lvis_v1.py'  # noqa: E501
     ])
 def test_two_stage_forward(cfg_file):
     models_with_semantic = [
         'htc/htc_r50_fpn_1x_coco.py',
+        'panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py',
         'scnet/scnet_r50_fpn_20e_coco.py',
     ]
     if cfg_file in models_with_semantic:
@@ -254,6 +281,7 @@ def test_two_stage_forward(cfg_file):
         with_semantic = False
 
     model = _get_detector_cfg(cfg_file)
+    model = _replace_r50_with_r18(model)
     model.backbone.init_cfg = None
 
     # Save cost
@@ -269,7 +297,7 @@ def test_two_stage_forward(cfg_file):
     from mmdet.models import build_detector
     detector = build_detector(model)
 
-    input_shape = (1, 3, 256, 256)
+    input_shape = (1, 3, 128, 128)
 
     # Test forward train with a non-empty truth batch
     mm_inputs = _demo_mm_inputs(
@@ -295,6 +323,18 @@ def test_two_stage_forward(cfg_file):
     assert float(loss.item()) > 0
     loss.backward()
 
+    # Test RoI forward train with an empty proposals
+    if cfg_file in [
+            'panoptic_fpn/panoptic_fpn_r50_fpn_1x_coco.py'  # noqa: E501
+    ]:
+        mm_inputs.pop('gt_semantic_seg')
+
+    feature = detector.extract_feat(imgs[0][None, :])
+    losses = detector.roi_head.forward_train(feature, img_metas,
+                                             [torch.empty(
+                                                 (0, 5))], **mm_inputs)
+    assert isinstance(losses, dict)
+
     # Test forward test
     with torch.no_grad():
         img_list = [g[None, :] for g in imgs]
@@ -362,6 +402,7 @@ def test_two_stage_forward(cfg_file):
     'cfg_file', ['ghm/retinanet_ghm_r50_fpn_1x_coco.py', 'ssd/ssd300_coco.py'])
 def test_single_stage_forward_cpu(cfg_file):
     model = _get_detector_cfg(cfg_file)
+    model = _replace_r50_with_r18(model)
     model.backbone.init_cfg = None
 
     from mmdet.models import build_detector
@@ -474,6 +515,7 @@ def _demo_mm_inputs(input_shape=(1, 3, 300, 300),
 
 def test_yolact_forward():
     model = _get_detector_cfg('yolact/yolact_r50_1x8_coco.py')
+    model = _replace_r50_with_r18(model)
     model.backbone.init_cfg = None
 
     from mmdet.models import build_detector
@@ -513,61 +555,8 @@ def test_yolact_forward():
 
 def test_detr_forward():
     model = _get_detector_cfg('detr/detr_r50_8x2_150e_coco.py')
-    model.backbone.init_cfg = None
-
-    from mmdet.models import build_detector
-    detector = build_detector(model)
-
-    input_shape = (1, 3, 100, 100)
-    mm_inputs = _demo_mm_inputs(input_shape)
-
-    imgs = mm_inputs.pop('imgs')
-    img_metas = mm_inputs.pop('img_metas')
-
-    # Test forward train with non-empty truth batch
-    detector.train()
-    gt_bboxes = mm_inputs['gt_bboxes']
-    gt_labels = mm_inputs['gt_labels']
-    losses = detector.forward(
-        imgs,
-        img_metas,
-        gt_bboxes=gt_bboxes,
-        gt_labels=gt_labels,
-        return_loss=True)
-    assert isinstance(losses, dict)
-    loss, _ = detector._parse_losses(losses)
-    assert float(loss.item()) > 0
-
-    # Test forward train with an empty truth batch
-    mm_inputs = _demo_mm_inputs(input_shape, num_items=[0])
-    imgs = mm_inputs.pop('imgs')
-    img_metas = mm_inputs.pop('img_metas')
-    gt_bboxes = mm_inputs['gt_bboxes']
-    gt_labels = mm_inputs['gt_labels']
-    losses = detector.forward(
-        imgs,
-        img_metas,
-        gt_bboxes=gt_bboxes,
-        gt_labels=gt_labels,
-        return_loss=True)
-    assert isinstance(losses, dict)
-    loss, _ = detector._parse_losses(losses)
-    assert float(loss.item()) > 0
-
-    # Test forward test
-    detector.eval()
-    with torch.no_grad():
-        img_list = [g[None, :] for g in imgs]
-        batch_results = []
-        for one_img, one_meta in zip(img_list, img_metas):
-            result = detector.forward([one_img], [[one_meta]],
-                                      rescale=True,
-                                      return_loss=False)
-            batch_results.append(result)
-
-
-def test_kd_single_stage_forward():
-    model = _get_detector_cfg('ld/ld_r18_gflv1_r101_fpn_coco_1x.py')
+    model.backbone.depth = 18
+    model.bbox_head.in_channels = 512
     model.backbone.init_cfg = None
 
     from mmdet.models import build_detector
diff --git a/tests/test_models/test_loss.py b/tests/test_models/test_loss.py
index b2c1a8ea..a8ebd109 100644
--- a/tests/test_models/test_loss.py
+++ b/tests/test_models/test_loss.py
@@ -1,5 +1,7 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
+from mmcv.utils import digit_version
 
 from mmdet.models.losses import (BalancedL1Loss, CrossEntropyLoss,
                                  DistributionFocalLoss, FocalLoss,
@@ -7,6 +9,7 @@ from mmdet.models.losses import (BalancedL1Loss, CrossEntropyLoss,
                                  KnowledgeDistillationKLDivLoss, L1Loss,
                                  MSELoss, QualityFocalLoss, SeesawLoss,
                                  SmoothL1Loss, VarifocalLoss)
+from mmdet.models.losses.dice_loss import DiceLoss
 from mmdet.models.losses.ghm_loss import GHMC, GHMR
 from mmdet.models.losses.iou_loss import (BoundedIoULoss, CIoULoss, DIoULoss,
                                           GIoULoss, IoULoss)
@@ -27,7 +30,7 @@ def test_iou_type_loss_zeros_weight(loss_class):
     BalancedL1Loss, BoundedIoULoss, CIoULoss, CrossEntropyLoss, DIoULoss,
     FocalLoss, DistributionFocalLoss, MSELoss, SeesawLoss, GaussianFocalLoss,
     GIoULoss, IoULoss, L1Loss, QualityFocalLoss, VarifocalLoss, GHMR, GHMC,
-    SmoothL1Loss, KnowledgeDistillationKLDivLoss
+    SmoothL1Loss, KnowledgeDistillationKLDivLoss, DiceLoss
 ])
 def test_loss_with_reduction_override(loss_class):
     pred = torch.rand((10, 4))
@@ -46,10 +49,11 @@ def test_loss_with_reduction_override(loss_class):
     IoULoss, BoundedIoULoss, GIoULoss, DIoULoss, CIoULoss, MSELoss, L1Loss,
     SmoothL1Loss, BalancedL1Loss
 ])
-def test_regression_losses(loss_class):
-    pred = torch.rand((10, 4))
-    target = torch.rand((10, 4))
-    weight = torch.rand((10, 4))
+@pytest.mark.parametrize('input_shape', [(10, 4), (0, 4)])
+def test_regression_losses(loss_class, input_shape):
+    pred = torch.rand(input_shape)
+    target = torch.rand(input_shape)
+    weight = torch.rand(input_shape)
 
     # Test loss forward
     loss = loss_class()(pred, target)
@@ -82,9 +86,16 @@ def test_regression_losses(loss_class):
 
 
 @pytest.mark.parametrize('loss_class', [FocalLoss, CrossEntropyLoss])
-def test_classification_losses(loss_class):
-    pred = torch.rand((10, 5))
-    target = torch.randint(0, 5, (10, ))
+@pytest.mark.parametrize('input_shape', [(10, 5), (0, 5)])
+def test_classification_losses(loss_class, input_shape):
+    if input_shape[0] == 0 and digit_version(
+            torch.__version__) < digit_version('1.5.0'):
+        pytest.skip(
+            f'CELoss in PyTorch {torch.__version__} does not support empty'
+            f'tensor.')
+
+    pred = torch.rand(input_shape)
+    target = torch.randint(0, 5, (input_shape[0], ))
 
     # Test loss forward
     loss = loss_class()(pred, target)
@@ -113,10 +124,11 @@ def test_classification_losses(loss_class):
 
 
 @pytest.mark.parametrize('loss_class', [GHMR])
-def test_GHMR_loss(loss_class):
-    pred = torch.rand((10, 4))
-    target = torch.rand((10, 4))
-    weight = torch.rand((10, 4))
+@pytest.mark.parametrize('input_shape', [(10, 4), (0, 4)])
+def test_GHMR_loss(loss_class, input_shape):
+    pred = torch.rand(input_shape)
+    target = torch.rand(input_shape)
+    weight = torch.rand(input_shape)
 
     # Test loss forward
     loss = loss_class()(pred, target, weight)
@@ -152,3 +164,53 @@ def test_loss_with_ignore_index(use_sigmoid):
 
     assert torch.allclose(loss, loss_with_ignore)
     assert torch.allclose(loss, loss_with_forward_ignore)
+
+
+def test_dice_loss():
+    loss_class = DiceLoss
+    pred = torch.rand((10, 4, 4))
+    target = torch.rand((10, 4, 4))
+    weight = torch.rand((10))
+
+    # Test loss forward
+    loss = loss_class()(pred, target)
+    assert isinstance(loss, torch.Tensor)
+
+    # Test loss forward with weight
+    loss = loss_class()(pred, target, weight)
+    assert isinstance(loss, torch.Tensor)
+
+    # Test loss forward with reduction_override
+    loss = loss_class()(pred, target, reduction_override='mean')
+    assert isinstance(loss, torch.Tensor)
+
+    # Test loss forward with avg_factor
+    loss = loss_class()(pred, target, avg_factor=10)
+    assert isinstance(loss, torch.Tensor)
+
+    with pytest.raises(ValueError):
+        # loss can evaluate with avg_factor only if
+        # reduction is None, 'none' or 'mean'.
+        reduction_override = 'sum'
+        loss_class()(
+            pred, target, avg_factor=10, reduction_override=reduction_override)
+
+    # Test loss forward with avg_factor and reduction
+    for reduction_override in [None, 'none', 'mean']:
+        loss_class()(
+            pred, target, avg_factor=10, reduction_override=reduction_override)
+        assert isinstance(loss, torch.Tensor)
+
+    # Test loss forward with has_acted=False and use_sigmoid=False
+    with pytest.raises(NotImplementedError):
+        loss_class(use_sigmoid=False, activate=True)(pred, target)
+
+    # Test loss forward with weight.ndim != loss.ndim
+    with pytest.raises(AssertionError):
+        weight = torch.rand((2, 8))
+        loss_class()(pred, target, weight)
+
+    # Test loss forward with len(weight) != len(pred)
+    with pytest.raises(AssertionError):
+        weight = torch.rand((8))
+        loss_class()(pred, target, weight)
diff --git a/tests/test_models/test_loss_compatibility.py b/tests/test_models/test_loss_compatibility.py
index 440d7f80..97759b8d 100644
--- a/tests/test_models/test_loss_compatibility.py
+++ b/tests/test_models/test_loss_compatibility.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """pytest tests/test_loss_compatibility.py."""
 import copy
 from os.path import dirname, exists, join
diff --git a/tests/test_models/test_necks.py b/tests/test_models/test_necks.py
index 3de9c703..f68476d6 100644
--- a/tests/test_models/test_necks.py
+++ b/tests/test_models/test_necks.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 from torch.nn.modules.batchnorm import _BatchNorm
diff --git a/tests/test_models/test_plugins.py b/tests/test_models/test_plugins.py
index 4f9cb870..59416b20 100644
--- a/tests/test_models/test_plugins.py
+++ b/tests/test_models/test_plugins.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
diff --git a/tests/test_models/test_roi_heads/__init__.py b/tests/test_models/test_roi_heads/__init__.py
index 9bb64023..83cfd581 100644
--- a/tests/test_models/test_roi_heads/__init__.py
+++ b/tests/test_models/test_roi_heads/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .utils import _dummy_bbox_sampling
 
 __all__ = ['_dummy_bbox_sampling']
diff --git a/tests/test_models/test_roi_heads/test_bbox_head.py b/tests/test_models/test_roi_heads/test_bbox_head.py
index 24c2a0d5..07753b83 100644
--- a/tests/test_models/test_roi_heads/test_bbox_head.py
+++ b/tests/test_models/test_roi_heads/test_bbox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import numpy as np
 import pytest
diff --git a/tests/test_models/test_roi_heads/test_mask_head.py b/tests/test_models/test_roi_heads/test_mask_head.py
index 31826cd5..12ad0a50 100644
--- a/tests/test_models/test_roi_heads/test_mask_head.py
+++ b/tests/test_models/test_roi_heads/test_mask_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_roi_heads/test_roi_extractor.py b/tests/test_models/test_roi_heads/test_roi_extractor.py
index 22743f2d..b79dff9b 100644
--- a/tests/test_models/test_roi_heads/test_roi_extractor.py
+++ b/tests/test_models/test_roi_heads/test_roi_extractor.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
diff --git a/tests/test_models/test_roi_heads/test_sabl_bbox_head.py b/tests/test_models/test_roi_heads/test_sabl_bbox_head.py
index 05178088..d412e3a8 100644
--- a/tests/test_models/test_roi_heads/test_sabl_bbox_head.py
+++ b/tests/test_models/test_roi_heads/test_sabl_bbox_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import torch
 
diff --git a/tests/test_models/test_roi_heads/utils.py b/tests/test_models/test_roi_heads/utils.py
index e5c6d58f..748cb0e3 100644
--- a/tests/test_models/test_roi_heads/utils.py
+++ b/tests/test_models/test_roi_heads/utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import torch
 
 from mmdet.core import build_assigner, build_sampler
diff --git a/tests/test_models/test_utils/test_brick_wrappers.py b/tests/test_models/test_utils/test_brick_wrappers.py
new file mode 100644
index 00000000..9aa5bd01
--- /dev/null
+++ b/tests/test_models/test_utils/test_brick_wrappers.py
@@ -0,0 +1,93 @@
+from unittest.mock import patch
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from mmdet.models.utils import AdaptiveAvgPool2d, adaptive_avg_pool2d
+
+if torch.__version__ != 'parrots':
+    torch_version = '1.7'
+else:
+    torch_version = 'parrots'
+
+
+@patch('torch.__version__', torch_version)
+def test_adaptive_avg_pool2d():
+    # Test the empty batch dimension
+    # Test the two input conditions
+    x_empty = torch.randn(0, 3, 4, 5)
+    # 1. tuple[int, int]
+    wrapper_out = adaptive_avg_pool2d(x_empty, (2, 2))
+    assert wrapper_out.shape == (0, 3, 2, 2)
+    # 2. int
+    wrapper_out = adaptive_avg_pool2d(x_empty, 2)
+    assert wrapper_out.shape == (0, 3, 2, 2)
+
+    # wrapper op with 3-dim input
+    x_normal = torch.randn(3, 3, 4, 5)
+    wrapper_out = adaptive_avg_pool2d(x_normal, (2, 2))
+    ref_out = F.adaptive_avg_pool2d(x_normal, (2, 2))
+    assert wrapper_out.shape == (3, 3, 2, 2)
+    assert torch.equal(wrapper_out, ref_out)
+
+    wrapper_out = adaptive_avg_pool2d(x_normal, 2)
+    ref_out = F.adaptive_avg_pool2d(x_normal, 2)
+    assert wrapper_out.shape == (3, 3, 2, 2)
+    assert torch.equal(wrapper_out, ref_out)
+
+
+@patch('torch.__version__', torch_version)
+def test_AdaptiveAvgPool2d():
+    # Test the empty batch dimension
+    x_empty = torch.randn(0, 3, 4, 5)
+    # Test the four input conditions
+    # 1. tuple[int, int]
+    wrapper = AdaptiveAvgPool2d((2, 2))
+    wrapper_out = wrapper(x_empty)
+    assert wrapper_out.shape == (0, 3, 2, 2)
+
+    # 2. int
+    wrapper = AdaptiveAvgPool2d(2)
+    wrapper_out = wrapper(x_empty)
+    assert wrapper_out.shape == (0, 3, 2, 2)
+
+    # 3. tuple[None, int]
+    wrapper = AdaptiveAvgPool2d((None, 2))
+    wrapper_out = wrapper(x_empty)
+    assert wrapper_out.shape == (0, 3, 4, 2)
+
+    # 3. tuple[int, None]
+    wrapper = AdaptiveAvgPool2d((2, None))
+    wrapper_out = wrapper(x_empty)
+    assert wrapper_out.shape == (0, 3, 2, 5)
+
+    # Test the normal batch dimension
+    x_normal = torch.randn(3, 3, 4, 5)
+    wrapper = AdaptiveAvgPool2d((2, 2))
+    ref = nn.AdaptiveAvgPool2d((2, 2))
+    wrapper_out = wrapper(x_normal)
+    ref_out = ref(x_normal)
+    assert wrapper_out.shape == (3, 3, 2, 2)
+    assert torch.equal(wrapper_out, ref_out)
+
+    wrapper = AdaptiveAvgPool2d(2)
+    ref = nn.AdaptiveAvgPool2d(2)
+    wrapper_out = wrapper(x_normal)
+    ref_out = ref(x_normal)
+    assert wrapper_out.shape == (3, 3, 2, 2)
+    assert torch.equal(wrapper_out, ref_out)
+
+    wrapper = AdaptiveAvgPool2d((None, 2))
+    ref = nn.AdaptiveAvgPool2d((None, 2))
+    wrapper_out = wrapper(x_normal)
+    ref_out = ref(x_normal)
+    assert wrapper_out.shape == (3, 3, 4, 2)
+    assert torch.equal(wrapper_out, ref_out)
+
+    wrapper = AdaptiveAvgPool2d((2, None))
+    ref = nn.AdaptiveAvgPool2d((2, None))
+    wrapper_out = wrapper(x_normal)
+    ref_out = ref(x_normal)
+    assert wrapper_out.shape == (3, 3, 2, 5)
+    assert torch.equal(wrapper_out, ref_out)
diff --git a/tests/test_models/test_utils/test_conv_upsample.py b/tests/test_models/test_utils/test_conv_upsample.py
index dda6392d..95a0ccc1 100644
--- a/tests/test_models/test_utils/test_conv_upsample.py
+++ b/tests/test_models/test_utils/test_conv_upsample.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
diff --git a/tests/test_models/test_utils/test_inverted_residual.py b/tests/test_models/test_utils/test_inverted_residual.py
index d5422708..14a331a9 100644
--- a/tests/test_models/test_utils/test_inverted_residual.py
+++ b/tests/test_models/test_utils/test_inverted_residual.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 from mmcv.cnn import is_norm
diff --git a/tests/test_models/test_utils/test_model_misc.py b/tests/test_models/test_utils/test_model_misc.py
index 7ccc1bd2..850f4ed0 100644
--- a/tests/test_models/test_utils/test_model_misc.py
+++ b/tests/test_models/test_utils/test_model_misc.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import torch
 
@@ -5,22 +6,22 @@ from mmdet.models.utils import interpolate_as
 
 
 def test_interpolate_as():
-    logits = torch.rand((1, 5, 4, 4))
-    targets = torch.rand((1, 1, 16, 16))
+    source = torch.rand((1, 5, 4, 4))
+    target = torch.rand((1, 1, 16, 16))
 
-    # Test 4D logits and targets
-    result = interpolate_as(logits, targets)
+    # Test 4D source and target
+    result = interpolate_as(source, target)
     assert result.shape == torch.Size((1, 5, 16, 16))
 
-    # Test 3D targets
-    result = interpolate_as(logits, targets.squeeze(0))
+    # Test 3D target
+    result = interpolate_as(source, target.squeeze(0))
     assert result.shape == torch.Size((1, 5, 16, 16))
 
-    # Test 3D logits
-    result = interpolate_as(logits.squeeze(0), targets)
+    # Test 3D source
+    result = interpolate_as(source.squeeze(0), target)
     assert result.shape == torch.Size((5, 16, 16))
 
     # Test type(target) == np.ndarray
-    targets = np.random.rand(16, 16)
-    result = interpolate_as(logits.squeeze(0), targets)
+    target = np.random.rand(16, 16)
+    result = interpolate_as(source.squeeze(0), target)
     assert result.shape == torch.Size((5, 16, 16))
diff --git a/tests/test_models/test_utils/test_position_encoding.py b/tests/test_models/test_utils/test_position_encoding.py
index e12127f5..11194101 100644
--- a/tests/test_models/test_utils/test_position_encoding.py
+++ b/tests/test_models/test_utils/test_position_encoding.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
diff --git a/tests/test_models/test_utils/test_se_layer.py b/tests/test_models/test_utils/test_se_layer.py
index c2cd239c..ae7ec7f1 100644
--- a/tests/test_models/test_utils/test_se_layer.py
+++ b/tests/test_models/test_utils/test_se_layer.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
diff --git a/tests/test_models/test_utils/test_transformer.py b/tests/test_models/test_utils/test_transformer.py
index 6058b2ab..916a9fc3 100644
--- a/tests/test_models/test_utils/test_transformer.py
+++ b/tests/test_models/test_utils/test_transformer.py
@@ -1,9 +1,468 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
+import torch
 from mmcv.utils import ConfigDict
 
-from mmdet.models.utils.transformer import (DetrTransformerDecoder,
-                                            DetrTransformerEncoder,
-                                            Transformer)
+from mmdet.models.utils.transformer import (AdaptivePadding,
+                                            DetrTransformerDecoder,
+                                            DetrTransformerEncoder, PatchEmbed,
+                                            PatchMerging, Transformer)
+
+
+def test_adaptive_padding():
+
+    for padding in ('same', 'corner'):
+        kernel_size = 16
+        stride = 16
+        dilation = 1
+        input = torch.rand(1, 1, 15, 17)
+        pool = AdaptivePadding(
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation,
+            padding=padding)
+        out = pool(input)
+        # padding to divisible by 16
+        assert (out.shape[2], out.shape[3]) == (16, 32)
+        input = torch.rand(1, 1, 16, 17)
+        out = pool(input)
+        # padding to divisible by 16
+        assert (out.shape[2], out.shape[3]) == (16, 32)
+
+        kernel_size = (2, 2)
+        stride = (2, 2)
+        dilation = (1, 1)
+
+        adap_pad = AdaptivePadding(
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation,
+            padding=padding)
+        input = torch.rand(1, 1, 11, 13)
+        out = adap_pad(input)
+        # padding to divisible by 2
+        assert (out.shape[2], out.shape[3]) == (12, 14)
+
+        kernel_size = (2, 2)
+        stride = (10, 10)
+        dilation = (1, 1)
+
+        adap_pad = AdaptivePadding(
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation,
+            padding=padding)
+        input = torch.rand(1, 1, 10, 13)
+        out = adap_pad(input)
+        #  no padding
+        assert (out.shape[2], out.shape[3]) == (10, 13)
+
+        kernel_size = (11, 11)
+        adap_pad = AdaptivePadding(
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation,
+            padding=padding)
+        input = torch.rand(1, 1, 11, 13)
+        out = adap_pad(input)
+        #  all padding
+        assert (out.shape[2], out.shape[3]) == (21, 21)
+
+        # test padding as kernel is (7,9)
+        input = torch.rand(1, 1, 11, 13)
+        stride = (3, 4)
+        kernel_size = (4, 5)
+        dilation = (2, 2)
+        # actually (7, 9)
+        adap_pad = AdaptivePadding(
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation,
+            padding=padding)
+        dilation_out = adap_pad(input)
+        assert (dilation_out.shape[2], dilation_out.shape[3]) == (16, 21)
+        kernel_size = (7, 9)
+        dilation = (1, 1)
+        adap_pad = AdaptivePadding(
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation,
+            padding=padding)
+        kernel79_out = adap_pad(input)
+        assert (kernel79_out.shape[2], kernel79_out.shape[3]) == (16, 21)
+        assert kernel79_out.shape == dilation_out.shape
+
+    # assert only support "same" "corner"
+    with pytest.raises(AssertionError):
+        AdaptivePadding(
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation,
+            padding=1)
+
+
+def test_patch_embed():
+    B = 2
+    H = 3
+    W = 4
+    C = 3
+    embed_dims = 10
+    kernel_size = 3
+    stride = 1
+    dummy_input = torch.rand(B, C, H, W)
+    patch_merge_1 = PatchEmbed(
+        in_channels=C,
+        embed_dims=embed_dims,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=0,
+        dilation=1,
+        norm_cfg=None)
+
+    x1, shape = patch_merge_1(dummy_input)
+    # test out shape
+    assert x1.shape == (2, 2, 10)
+    # test outsize is correct
+    assert shape == (1, 2)
+    # test L = out_h * out_w
+    assert shape[0] * shape[1] == x1.shape[1]
+
+    B = 2
+    H = 10
+    W = 10
+    C = 3
+    embed_dims = 10
+    kernel_size = 5
+    stride = 2
+    dummy_input = torch.rand(B, C, H, W)
+    # test dilation
+    patch_merge_2 = PatchEmbed(
+        in_channels=C,
+        embed_dims=embed_dims,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=0,
+        dilation=2,
+        norm_cfg=None,
+    )
+
+    x2, shape = patch_merge_2(dummy_input)
+    # test out shape
+    assert x2.shape == (2, 1, 10)
+    # test outsize is correct
+    assert shape == (1, 1)
+    # test L = out_h * out_w
+    assert shape[0] * shape[1] == x2.shape[1]
+
+    stride = 2
+    input_size = (10, 10)
+
+    dummy_input = torch.rand(B, C, H, W)
+    # test stride and norm
+    patch_merge_3 = PatchEmbed(
+        in_channels=C,
+        embed_dims=embed_dims,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=0,
+        dilation=2,
+        norm_cfg=dict(type='LN'),
+        input_size=input_size)
+
+    x3, shape = patch_merge_3(dummy_input)
+    # test out shape
+    assert x3.shape == (2, 1, 10)
+    # test outsize is correct
+    assert shape == (1, 1)
+    # test L = out_h * out_w
+    assert shape[0] * shape[1] == x3.shape[1]
+
+    # test thte init_out_size with nn.Unfold
+    assert patch_merge_3.init_out_size[1] == (input_size[0] - 2 * 4 -
+                                              1) // 2 + 1
+    assert patch_merge_3.init_out_size[0] == (input_size[0] - 2 * 4 -
+                                              1) // 2 + 1
+    H = 11
+    W = 12
+    input_size = (H, W)
+    dummy_input = torch.rand(B, C, H, W)
+    # test stride and norm
+    patch_merge_3 = PatchEmbed(
+        in_channels=C,
+        embed_dims=embed_dims,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=0,
+        dilation=2,
+        norm_cfg=dict(type='LN'),
+        input_size=input_size)
+
+    _, shape = patch_merge_3(dummy_input)
+    # when input_size equal to real input
+    # the out_size shoule be equal to `init_out_size`
+    assert shape == patch_merge_3.init_out_size
+
+    input_size = (H, W)
+    dummy_input = torch.rand(B, C, H, W)
+    # test stride and norm
+    patch_merge_3 = PatchEmbed(
+        in_channels=C,
+        embed_dims=embed_dims,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=0,
+        dilation=2,
+        norm_cfg=dict(type='LN'),
+        input_size=input_size)
+
+    _, shape = patch_merge_3(dummy_input)
+    # when input_size equal to real input
+    # the out_size shoule be equal to `init_out_size`
+    assert shape == patch_merge_3.init_out_size
+
+    # test adap padding
+    for padding in ('same', 'corner'):
+        in_c = 2
+        embed_dims = 3
+        B = 2
+
+        # test stride is 1
+        input_size = (5, 5)
+        kernel_size = (5, 5)
+        stride = (1, 1)
+        dilation = 1
+        bias = False
+
+        x = torch.rand(B, in_c, *input_size)
+        patch_embed = PatchEmbed(
+            in_channels=in_c,
+            embed_dims=embed_dims,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_embed(x)
+        assert x_out.size() == (B, 25, 3)
+        assert out_size == (5, 5)
+        assert x_out.size(1) == out_size[0] * out_size[1]
+
+        # test kernel_size == stride
+        input_size = (5, 5)
+        kernel_size = (5, 5)
+        stride = (5, 5)
+        dilation = 1
+        bias = False
+
+        x = torch.rand(B, in_c, *input_size)
+        patch_embed = PatchEmbed(
+            in_channels=in_c,
+            embed_dims=embed_dims,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_embed(x)
+        assert x_out.size() == (B, 1, 3)
+        assert out_size == (1, 1)
+        assert x_out.size(1) == out_size[0] * out_size[1]
+
+        # test kernel_size == stride
+        input_size = (6, 5)
+        kernel_size = (5, 5)
+        stride = (5, 5)
+        dilation = 1
+        bias = False
+
+        x = torch.rand(B, in_c, *input_size)
+        patch_embed = PatchEmbed(
+            in_channels=in_c,
+            embed_dims=embed_dims,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_embed(x)
+        assert x_out.size() == (B, 2, 3)
+        assert out_size == (2, 1)
+        assert x_out.size(1) == out_size[0] * out_size[1]
+
+        # test different kernel_size with diffrent stride
+        input_size = (6, 5)
+        kernel_size = (6, 2)
+        stride = (6, 2)
+        dilation = 1
+        bias = False
+
+        x = torch.rand(B, in_c, *input_size)
+        patch_embed = PatchEmbed(
+            in_channels=in_c,
+            embed_dims=embed_dims,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_embed(x)
+        assert x_out.size() == (B, 3, 3)
+        assert out_size == (1, 3)
+        assert x_out.size(1) == out_size[0] * out_size[1]
+
+
+def test_patch_merging():
+
+    # Test the model with int padding
+    in_c = 3
+    out_c = 4
+    kernel_size = 3
+    stride = 3
+    padding = 1
+    dilation = 1
+    bias = False
+    # test the case `pad_to_stride` is False
+    patch_merge = PatchMerging(
+        in_channels=in_c,
+        out_channels=out_c,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=padding,
+        dilation=dilation,
+        bias=bias)
+    B, L, C = 1, 100, 3
+    input_size = (10, 10)
+    x = torch.rand(B, L, C)
+    x_out, out_size = patch_merge(x, input_size)
+    assert x_out.size() == (1, 16, 4)
+    assert out_size == (4, 4)
+    # assert out size is consistent with real output
+    assert x_out.size(1) == out_size[0] * out_size[1]
+    in_c = 4
+    out_c = 5
+    kernel_size = 6
+    stride = 3
+    padding = 2
+    dilation = 2
+    bias = False
+    patch_merge = PatchMerging(
+        in_channels=in_c,
+        out_channels=out_c,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=padding,
+        dilation=dilation,
+        bias=bias)
+    B, L, C = 1, 100, 4
+    input_size = (10, 10)
+    x = torch.rand(B, L, C)
+    x_out, out_size = patch_merge(x, input_size)
+    assert x_out.size() == (1, 4, 5)
+    assert out_size == (2, 2)
+    # assert out size is consistent with real output
+    assert x_out.size(1) == out_size[0] * out_size[1]
+
+    # Test with adaptive padding
+    for padding in ('same', 'corner'):
+        in_c = 2
+        out_c = 3
+        B = 2
+
+        # test stride is 1
+        input_size = (5, 5)
+        kernel_size = (5, 5)
+        stride = (1, 1)
+        dilation = 1
+        bias = False
+        L = input_size[0] * input_size[1]
+
+        x = torch.rand(B, L, in_c)
+        patch_merge = PatchMerging(
+            in_channels=in_c,
+            out_channels=out_c,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_merge(x, input_size)
+        assert x_out.size() == (B, 25, 3)
+        assert out_size == (5, 5)
+        assert x_out.size(1) == out_size[0] * out_size[1]
+
+        # test kernel_size == stride
+        input_size = (5, 5)
+        kernel_size = (5, 5)
+        stride = (5, 5)
+        dilation = 1
+        bias = False
+        L = input_size[0] * input_size[1]
+
+        x = torch.rand(B, L, in_c)
+        patch_merge = PatchMerging(
+            in_channels=in_c,
+            out_channels=out_c,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_merge(x, input_size)
+        assert x_out.size() == (B, 1, 3)
+        assert out_size == (1, 1)
+        assert x_out.size(1) == out_size[0] * out_size[1]
+
+        # test kernel_size == stride
+        input_size = (6, 5)
+        kernel_size = (5, 5)
+        stride = (5, 5)
+        dilation = 1
+        bias = False
+        L = input_size[0] * input_size[1]
+
+        x = torch.rand(B, L, in_c)
+        patch_merge = PatchMerging(
+            in_channels=in_c,
+            out_channels=out_c,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_merge(x, input_size)
+        assert x_out.size() == (B, 2, 3)
+        assert out_size == (2, 1)
+        assert x_out.size(1) == out_size[0] * out_size[1]
+
+        # test different kernel_size with diffrent stride
+        input_size = (6, 5)
+        kernel_size = (6, 2)
+        stride = (6, 2)
+        dilation = 1
+        bias = False
+        L = input_size[0] * input_size[1]
+
+        x = torch.rand(B, L, in_c)
+        patch_merge = PatchMerging(
+            in_channels=in_c,
+            out_channels=out_c,
+            kernel_size=kernel_size,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            bias=bias)
+
+        x_out, out_size = patch_merge(x, input_size)
+        assert x_out.size() == (B, 3, 3)
+        assert out_size == (1, 3)
+        assert x_out.size(1) == out_size[0] * out_size[1]
 
 
 def test_detr_transformer_dencoder_encoder_layer():
diff --git a/tests/test_onnx/__init__.py b/tests/test_onnx/__init__.py
index 320516c0..76d466f5 100644
--- a/tests/test_onnx/__init__.py
+++ b/tests/test_onnx/__init__.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from .utils import ort_validate
 
 __all__ = ['ort_validate']
diff --git a/tests/test_onnx/test_head.py b/tests/test_onnx/test_head.py
index bd05adbd..c9cf286a 100644
--- a/tests/test_onnx/test_head.py
+++ b/tests/test_onnx/test_head.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 from functools import partial
 
@@ -234,9 +235,9 @@ def test_yolov3_head_get_bboxes():
     yolo_head_data = 'yolov3_head_get_bboxes.pkl'
     pred_maps = mmcv.load(osp.join(data_path, yolo_head_data))
 
-    yolo_model.get_bboxes = partial(
-        yolo_model.get_bboxes, img_metas=img_metas, with_nms=False)
-    ort_validate(yolo_model.get_bboxes, pred_maps)
+    yolo_model.onnx_export = partial(
+        yolo_model.onnx_export, img_metas=img_metas, with_nms=False)
+    ort_validate(yolo_model.onnx_export, pred_maps)
 
 
 def fcos_config():
diff --git a/tests/test_onnx/test_neck.py b/tests/test_onnx/test_neck.py
index f16a3dcc..a1a5cc8b 100644
--- a/tests/test_onnx/test_neck.py
+++ b/tests/test_onnx/test_neck.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 
 import mmcv
diff --git a/tests/test_onnx/utils.py b/tests/test_onnx/utils.py
index 89b9c13a..ad95e9e1 100644
--- a/tests/test_onnx/utils.py
+++ b/tests/test_onnx/utils.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os
 import os.path as osp
 import warnings
diff --git a/tests/test_runtime/async_benchmark.py b/tests/test_runtime/async_benchmark.py
index 8dab48ad..aa692c4e 100644
--- a/tests/test_runtime/async_benchmark.py
+++ b/tests/test_runtime/async_benchmark.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import asyncio
 import os
 import shutil
diff --git a/tests/test_runtime/test_async.py b/tests/test_runtime/test_async.py
index e9733f61..1af1501c 100644
--- a/tests/test_runtime/test_async.py
+++ b/tests/test_runtime/test_async.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """Tests for async interface."""
 
 import asyncio
diff --git a/tests/test_runtime/test_config.py b/tests/test_runtime/test_config.py
index 27156192..f160d374 100644
--- a/tests/test_runtime/test_config.py
+++ b/tests/test_runtime/test_config.py
@@ -1,9 +1,8 @@
-from os.path import dirname, exists, join, relpath
+# Copyright (c) OpenMMLab. All rights reserved.
+from os.path import dirname, exists, join
 from unittest.mock import Mock
 
 import pytest
-import torch
-from mmcv.runner import build_optimizer
 
 from mmdet.core import BitmapMasks, PolygonMasks
 from mmdet.datasets.builder import DATASETS
@@ -61,68 +60,6 @@ def _check_numclasscheckhook(detector, config_mod):
     compatible_check.before_val_epoch(dummy_runner)
 
 
-def test_config_build_detector():
-    """Test that all detection models defined in the configs can be
-    initialized."""
-    from mmcv import Config
-    from mmdet.models import build_detector
-
-    config_dpath = _get_config_directory()
-    print(f'Found config_dpath = {config_dpath}')
-
-    import glob
-    config_fpaths = list(glob.glob(join(config_dpath, '**', '*.py')))
-    config_fpaths = [
-        p for p in config_fpaths
-        if p.find('_base_') == -1 and p.find('common') == -1
-    ]
-    config_names = [relpath(p, config_dpath) for p in config_fpaths]
-
-    print(f'Using {len(config_names)} config files')
-
-    for config_fname in config_names:
-        config_fpath = join(config_dpath, config_fname)
-        config_mod = Config.fromfile(config_fpath)
-        print(f'Building detector, config_fpath = {config_fpath}')
-
-        # Remove pretrained keys to allow for testing in an offline environment
-        if 'pretrained' in config_mod.model:
-            config_mod.model['pretrained'] = None
-
-        detector = build_detector(config_mod.model)
-        assert detector is not None
-
-        # Check whether NumClassCheckHook is used.
-        custom_hooks = config_mod.get('custom_hooks', [])
-        assert custom_hooks is None or isinstance(custom_hooks, list)
-        check_class_num = False
-        if custom_hooks is not None:
-            hooks = [hook['type'] for hook in custom_hooks]
-            if 'NumClassCheckHook' in hooks:
-                check_class_num = True
-        if check_class_num:
-            _check_numclasscheckhook(detector, config_mod)
-
-        optimizer = build_optimizer(detector, config_mod.optimizer)
-        assert isinstance(optimizer, torch.optim.Optimizer)
-
-        if 'roi_head' in config_mod.model.keys():
-            # for two stage detector
-            # detectors must have bbox head
-            assert detector.roi_head.with_bbox and detector.with_bbox
-            assert detector.roi_head.with_mask == detector.with_mask
-
-            head_config = config_mod.model['roi_head']
-            _check_roi_head(head_config, detector.roi_head)
-
-        # else:
-        #     # for single stage detector
-        #     # detectors must have bbox head
-        #     # assert detector.with_bbox
-        #     head_config = config_mod.model['bbox_head']
-        #     _check_bbox_head(head_config, detector.bbox_head)
-
-
 def _check_roi_head(config, head):
     # check consistency between head_config and roi_head
     assert config['type'] == head.__class__.__name__
diff --git a/tests/test_runtime/test_eval_hook.py b/tests/test_runtime/test_eval_hook.py
index 435f99bd..ac0f5e9b 100644
--- a/tests/test_runtime/test_eval_hook.py
+++ b/tests/test_runtime/test_eval_hook.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 import tempfile
 import unittest.mock as mock
diff --git a/tests/test_runtime/test_fp16.py b/tests/test_runtime/test_fp16.py
index d43fb19c..e3dd4324 100644
--- a/tests/test_runtime/test_fp16.py
+++ b/tests/test_runtime/test_fp16.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import pytest
 import torch
diff --git a/tests/test_utils/test_anchor.py b/tests/test_utils/test_anchor.py
index ebab2290..83a1befb 100644
--- a/tests/test_utils/test_anchor.py
+++ b/tests/test_utils/test_anchor.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """
 CommandLine:
     pytest tests/test_utils/test_anchor.py
diff --git a/tests/test_utils/test_assigner.py b/tests/test_utils/test_assigner.py
index 949234b6..3a42d35d 100644
--- a/tests/test_utils/test_assigner.py
+++ b/tests/test_utils/test_assigner.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 """Tests the Assigner objects.
 
 CommandLine:
diff --git a/tests/test_utils/test_coder.py b/tests/test_utils/test_coder.py
index 2dca4131..4e8877b3 100644
--- a/tests/test_utils/test_coder.py
+++ b/tests/test_utils/test_coder.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import pytest
 import torch
 
diff --git a/tests/test_utils/test_general_data.py b/tests/test_utils/test_general_data.py
new file mode 100644
index 00000000..c5525fde
--- /dev/null
+++ b/tests/test_utils/test_general_data.py
@@ -0,0 +1,591 @@
+import copy
+
+import numpy as np
+import pytest
+import torch
+
+from mmdet.core import GeneralData, InstanceData
+
+
+def _equal(a, b):
+    if isinstance(a, (torch.Tensor, np.ndarray)):
+        return (a == b).all()
+    else:
+        return a == b
+
+
+def test_general_data():
+
+    # test init
+    meta_info = dict(
+        img_size=[256, 256],
+        path='dadfaff',
+        scale_factor=np.array([1.5, 1.5]),
+        img_shape=torch.rand(4))
+
+    data = dict(
+        bboxes=torch.rand(4, 4),
+        labels=torch.rand(4),
+        masks=np.random.rand(4, 2, 2))
+
+    instance_data = GeneralData(meta_info=meta_info)
+    assert 'img_size' in instance_data
+    assert instance_data.img_size == [256, 256]
+    assert instance_data['img_size'] == [256, 256]
+    assert 'path' in instance_data
+    assert instance_data.path == 'dadfaff'
+
+    # test nice_repr
+    repr_instance_data = instance_data.new(data=data)
+    nice_repr = str(repr_instance_data)
+    for line in nice_repr.split('\n'):
+        if 'masks' in line:
+            assert 'shape' in line
+            assert '(4, 2, 2)' in line
+        if 'bboxes' in line:
+            assert 'shape' in line
+            assert 'torch.Size([4, 4])' in line
+        if 'path' in line:
+            assert 'dadfaff' in line
+        if 'scale_factor' in line:
+            assert '[1.5 1.5]' in line
+
+    instance_data = GeneralData(
+        meta_info=meta_info, data=dict(bboxes=torch.rand(5)))
+    assert 'bboxes' in instance_data
+    assert len(instance_data.bboxes) == 5
+
+    # data should be a dict
+    with pytest.raises(AssertionError):
+        GeneralData(data=1)
+
+    # test set data
+    instance_data = GeneralData()
+    instance_data.set_data(data)
+    assert 'bboxes' in instance_data
+    assert len(instance_data.bboxes) == 4
+    assert 'masks' in instance_data
+    assert len(instance_data.masks) == 4
+    # data should be a dict
+    with pytest.raises(AssertionError):
+        instance_data.set_data(data=1)
+
+    # test set_meta
+    instance_data = GeneralData()
+    instance_data.set_meta_info(meta_info)
+    assert 'img_size' in instance_data
+    assert instance_data.img_size == [256, 256]
+    assert instance_data['img_size'] == [256, 256]
+    assert 'path' in instance_data
+    assert instance_data.path == 'dadfaff'
+    # can skip same value when overwrite
+    instance_data.set_meta_info(meta_info)
+
+    # meta should be a dict
+    with pytest.raises(AssertionError):
+        instance_data.set_meta_info(meta_info='fjhka')
+
+    # attribute in `_meta_info_field` is immutable once initialized
+    instance_data.set_meta_info(meta_info)
+    # meta should be immutable
+    with pytest.raises(KeyError):
+        instance_data.set_meta_info(dict(img_size=[254, 251]))
+    with pytest.raises(KeyError):
+        duplicate_meta_info = copy.deepcopy(meta_info)
+        duplicate_meta_info['path'] = 'dada'
+        instance_data.set_meta_info(duplicate_meta_info)
+    with pytest.raises(KeyError):
+        duplicate_meta_info = copy.deepcopy(meta_info)
+        duplicate_meta_info['scale_factor'] = np.array([1.5, 1.6])
+        instance_data.set_meta_info(duplicate_meta_info)
+
+    # test new_instance_data
+    instance_data = GeneralData(meta_info)
+    new_instance_data = instance_data.new()
+    for k, v in instance_data.meta_info_items():
+        assert k in new_instance_data
+        _equal(v, new_instance_data[k])
+
+    instance_data = GeneralData(meta_info, data=data)
+    temp_meta = copy.deepcopy(meta_info)
+    temp_data = copy.deepcopy(data)
+    temp_data['time'] = '12212'
+    temp_meta['img_norm'] = np.random.random(3)
+
+    new_instance_data = instance_data.new(meta_info=temp_meta, data=temp_data)
+    for k, v in new_instance_data.meta_info_items():
+        if k in instance_data:
+            _equal(v, instance_data[k])
+        else:
+            assert _equal(v, temp_meta[k])
+            assert k == 'img_norm'
+
+    for k, v in new_instance_data.items():
+        if k in instance_data:
+            _equal(v, instance_data[k])
+        else:
+            assert k == 'time'
+            assert _equal(v, temp_data[k])
+
+    # test keys
+    instance_data = GeneralData(meta_info, data=dict(bboxes=10))
+    assert 'bboxes' in instance_data.keys()
+    instance_data.b = 10
+    assert 'b' in instance_data
+
+    # test meta keys
+    instance_data = GeneralData(meta_info, data=dict(bboxes=10))
+    assert 'path' in instance_data.meta_info_keys()
+    assert len(instance_data.meta_info_keys()) == len(meta_info)
+    instance_data.set_meta_info(dict(workdir='fafaf'))
+    assert 'workdir' in instance_data
+    assert len(instance_data.meta_info_keys()) == len(meta_info) + 1
+
+    # test values
+    instance_data = GeneralData(meta_info, data=dict(bboxes=10))
+    assert 10 in instance_data.values()
+    assert len(instance_data.values()) == 1
+
+    # test meta values
+    instance_data = GeneralData(meta_info, data=dict(bboxes=10))
+    # torch 1.3 eq() can not compare str and tensor
+    from mmdet import digit_version
+    if digit_version(torch.__version__) >= [1, 4]:
+        assert 'dadfaff' in instance_data.meta_info_values()
+    assert len(instance_data.meta_info_values()) == len(meta_info)
+
+    # test items
+    instance_data = GeneralData(data=data)
+    for k, v in instance_data.items():
+        assert k in data
+        assert _equal(v, data[k])
+
+    # test meta_info_items
+    instance_data = GeneralData(meta_info=meta_info)
+    for k, v in instance_data.meta_info_items():
+        assert k in meta_info
+        assert _equal(v, meta_info[k])
+
+    # test __setattr__
+    new_instance_data = GeneralData(data=data)
+    new_instance_data.mask = torch.rand(3, 4, 5)
+    new_instance_data.bboxes = torch.rand(2, 4)
+    assert 'mask' in new_instance_data
+    assert len(new_instance_data.mask) == 3
+    assert len(new_instance_data.bboxes) == 2
+
+    # test instance_data_field has been updated
+    assert 'mask' in new_instance_data._data_fields
+    assert 'bboxes' in new_instance_data._data_fields
+
+    for k in data:
+        assert k in new_instance_data._data_fields
+
+    # '_meta_info_field', '_data_fields' is immutable.
+    with pytest.raises(AttributeError):
+        new_instance_data._data_fields = None
+    with pytest.raises(AttributeError):
+        new_instance_data._meta_info_fields = None
+    with pytest.raises(AttributeError):
+        del new_instance_data._data_fields
+    with pytest.raises(AttributeError):
+        del new_instance_data._meta_info_fields
+
+    # key in _meta_info_field is immutable
+    new_instance_data.set_meta_info(meta_info)
+    with pytest.raises(KeyError):
+        del new_instance_data.img_size
+    with pytest.raises(KeyError):
+        del new_instance_data.scale_factor
+    for k in new_instance_data.meta_info_keys():
+        with pytest.raises(AttributeError):
+            new_instance_data[k] = None
+
+    # test __delattr__
+    # test key can be removed in instance_data_field
+    assert 'mask' in new_instance_data._data_fields
+    assert 'mask' in new_instance_data.keys()
+    assert 'mask' in new_instance_data
+    assert hasattr(new_instance_data, 'mask')
+    del new_instance_data.mask
+    assert 'mask' not in new_instance_data.keys()
+    assert 'mask' not in new_instance_data
+    assert 'mask' not in new_instance_data._data_fields
+    assert not hasattr(new_instance_data, 'mask')
+
+    # tset __delitem__
+    new_instance_data.mask = torch.rand(1, 2, 3)
+    assert 'mask' in new_instance_data._data_fields
+    assert 'mask' in new_instance_data
+    assert hasattr(new_instance_data, 'mask')
+    del new_instance_data['mask']
+    assert 'mask' not in new_instance_data
+    assert 'mask' not in new_instance_data._data_fields
+    assert 'mask' not in new_instance_data
+    assert not hasattr(new_instance_data, 'mask')
+
+    # test __setitem__
+    new_instance_data['mask'] = torch.rand(1, 2, 3)
+    assert 'mask' in new_instance_data._data_fields
+    assert 'mask' in new_instance_data.keys()
+    assert hasattr(new_instance_data, 'mask')
+
+    # test data_fields has been updated
+    assert 'mask' in new_instance_data.keys()
+    assert 'mask' in new_instance_data._data_fields
+
+    # '_meta_info_field', '_data_fields' is immutable.
+    with pytest.raises(AttributeError):
+        del new_instance_data['_data_fields']
+    with pytest.raises(AttributeError):
+        del new_instance_data['_meta_info_field']
+
+    #  test __getitem__
+    new_instance_data.mask is new_instance_data['mask']
+
+    # test get
+    assert new_instance_data.get('mask') is new_instance_data.mask
+    assert new_instance_data.get('none_attribute', None) is None
+    assert new_instance_data.get('none_attribute', 1) == 1
+
+    # test pop
+    mask = new_instance_data.mask
+    assert new_instance_data.pop('mask') is mask
+    assert new_instance_data.pop('mask', None) is None
+    assert new_instance_data.pop('mask', 1) == 1
+
+    # '_meta_info_field', '_data_fields' is immutable.
+    with pytest.raises(KeyError):
+        new_instance_data.pop('_data_fields')
+    with pytest.raises(KeyError):
+        new_instance_data.pop('_meta_info_field')
+    # attribute in `_meta_info_field` is immutable
+    with pytest.raises(KeyError):
+        new_instance_data.pop('img_size')
+    # test pop attribute in instance_data_filed
+    new_instance_data['mask'] = torch.rand(1, 2, 3)
+    new_instance_data.pop('mask')
+    # test data_field has been updated
+    assert 'mask' not in new_instance_data
+    assert 'mask' not in new_instance_data._data_fields
+    assert 'mask' not in new_instance_data
+
+    # test_keys
+    new_instance_data.mask = torch.ones(1, 2, 3)
+    'mask' in new_instance_data.keys()
+    has_flag = False
+    for key in new_instance_data.keys():
+        if key == 'mask':
+            has_flag = True
+    assert has_flag
+
+    # test values
+    assert len(list(new_instance_data.keys())) == len(
+        list(new_instance_data.values()))
+    mask = new_instance_data.mask
+    has_flag = False
+    for value in new_instance_data.values():
+        if value is mask:
+            has_flag = True
+    assert has_flag
+
+    # test items
+    assert len(list(new_instance_data.keys())) == len(
+        list(new_instance_data.items()))
+    mask = new_instance_data.mask
+    has_flag = False
+    for key, value in new_instance_data.items():
+        if value is mask:
+            assert key == 'mask'
+            has_flag = True
+    assert has_flag
+
+    # test device
+    new_instance_data = GeneralData()
+    if torch.cuda.is_available():
+        newnew_instance_data = new_instance_data.new()
+        devices = ('cpu', 'cuda')
+        for i in range(10):
+            device = devices[i % 2]
+            newnew_instance_data[f'{i}'] = torch.rand(1, 2, 3, device=device)
+        newnew_instance_data = newnew_instance_data.cpu()
+        for value in newnew_instance_data.values():
+            assert not value.is_cuda
+        newnew_instance_data = new_instance_data.new()
+        devices = ('cuda', 'cpu')
+        for i in range(10):
+            device = devices[i % 2]
+            newnew_instance_data[f'{i}'] = torch.rand(1, 2, 3, device=device)
+        newnew_instance_data = newnew_instance_data.cuda()
+        for value in newnew_instance_data.values():
+            assert value.is_cuda
+    # test to
+    double_instance_data = instance_data.new()
+    double_instance_data.long = torch.LongTensor(1, 2, 3, 4)
+    double_instance_data.bool = torch.BoolTensor(1, 2, 3, 4)
+    double_instance_data = instance_data.to(torch.double)
+    for k, v in double_instance_data.items():
+        if isinstance(v, torch.Tensor):
+            assert v.dtype is torch.double
+
+    # test .cpu() .cuda()
+    if torch.cuda.is_available():
+        cpu_instance_data = double_instance_data.new()
+        cpu_instance_data.mask = torch.rand(1)
+        cuda_tensor = torch.rand(1, 2, 3).cuda()
+        cuda_instance_data = cpu_instance_data.to(cuda_tensor.device)
+        for value in cuda_instance_data.values():
+            assert value.is_cuda
+        cpu_instance_data = cuda_instance_data.cpu()
+        for value in cpu_instance_data.values():
+            assert not value.is_cuda
+        cuda_instance_data = cpu_instance_data.cuda()
+        for value in cuda_instance_data.values():
+            assert value.is_cuda
+
+    # test detach
+    grad_instance_data = double_instance_data.new()
+    grad_instance_data.mask = torch.rand(2, requires_grad=True)
+    grad_instance_data.mask_1 = torch.rand(2, requires_grad=True)
+    detach_instance_data = grad_instance_data.detach()
+    for value in detach_instance_data.values():
+        assert not value.requires_grad
+
+    # test numpy
+    tensor_instance_data = double_instance_data.new()
+    tensor_instance_data.mask = torch.rand(2, requires_grad=True)
+    tensor_instance_data.mask_1 = torch.rand(2, requires_grad=True)
+    numpy_instance_data = tensor_instance_data.numpy()
+    for value in numpy_instance_data.values():
+        assert isinstance(value, np.ndarray)
+    if torch.cuda.is_available():
+        tensor_instance_data = double_instance_data.new()
+        tensor_instance_data.mask = torch.rand(2)
+        tensor_instance_data.mask_1 = torch.rand(2)
+        tensor_instance_data = tensor_instance_data.cuda()
+        numpy_instance_data = tensor_instance_data.numpy()
+        for value in numpy_instance_data.values():
+            assert isinstance(value, np.ndarray)
+
+    instance_data['_c'] = 10000
+    instance_data.get('dad', None) is None
+    assert hasattr(instance_data, '_c')
+    del instance_data['_c']
+    assert not hasattr(instance_data, '_c')
+    instance_data.a = 1000
+    instance_data['a'] = 2000
+    assert instance_data['a'] == 2000
+    assert instance_data.a == 2000
+    assert instance_data.get('a') == instance_data['a'] == instance_data.a
+    instance_data._meta = 1000
+    assert '_meta' in instance_data.keys()
+    if torch.cuda.is_available():
+        instance_data.bbox = torch.ones(2, 3, 4, 5).cuda()
+        instance_data.score = torch.ones(2, 3, 4, 4)
+    else:
+        instance_data.bbox = torch.ones(2, 3, 4, 5)
+
+    assert len(instance_data.new().keys()) == 0
+    with pytest.raises(AttributeError):
+        instance_data.img_size = 100
+
+    for k, v in instance_data.items():
+        if k == 'bbox':
+            assert isinstance(v, torch.Tensor)
+    assert 'a' in instance_data
+    instance_data.pop('a')
+    assert 'a' not in instance_data
+
+    cpu_instance_data = instance_data.cpu()
+    for k, v in cpu_instance_data.items():
+        if isinstance(v, torch.Tensor):
+            assert not v.is_cuda
+
+    assert isinstance(cpu_instance_data.numpy().bbox, np.ndarray)
+
+    if torch.cuda.is_available():
+        cuda_resutls = instance_data.cuda()
+        for k, v in cuda_resutls.items():
+            if isinstance(v, torch.Tensor):
+                assert v.is_cuda
+
+
+def test_instance_data():
+    meta_info = dict(
+        img_size=(256, 256),
+        path='dadfaff',
+        scale_factor=np.array([1.5, 1.5, 1, 1]))
+
+    data = dict(
+        bboxes=torch.rand(4, 4),
+        masks=torch.rand(4, 2, 2),
+        labels=np.random.rand(4),
+        size=[(i, i) for i in range(4)])
+
+    # test init
+    instance_data = InstanceData(meta_info)
+    assert 'path' in instance_data
+    instance_data = InstanceData(meta_info, data=data)
+    assert len(instance_data) == 4
+    instance_data.set_data(data)
+    assert len(instance_data) == 4
+
+    meta_info = copy.deepcopy(meta_info)
+    meta_info['img_name'] = 'flag'
+
+    # test newinstance_data
+    new_instance_data = instance_data.new(meta_info=meta_info)
+    for k, v in new_instance_data.meta_info_items():
+        if k in instance_data:
+            _equal(v, instance_data[k])
+        else:
+            assert _equal(v, meta_info[k])
+            assert k == 'img_name'
+    # meta info is immutable
+    with pytest.raises(KeyError):
+        meta_info = copy.deepcopy(meta_info)
+        meta_info['path'] = 'fdasfdsd'
+        instance_data.new(meta_info=meta_info)
+
+    # data fields should have same length
+    with pytest.raises(AssertionError):
+        temp_data = copy.deepcopy(data)
+        temp_data['bboxes'] = torch.rand(5, 4)
+        instance_data.new(data=temp_data)
+
+    temp_data = copy.deepcopy(data)
+    temp_data['scores'] = torch.rand(4)
+    new_instance_data = instance_data.new(data=temp_data)
+    for k, v in new_instance_data.items():
+        if k in instance_data:
+            _equal(v, instance_data[k])
+        else:
+            assert k == 'scores'
+            assert _equal(v, temp_data[k])
+
+    instance_data = instance_data.new()
+
+    # test __setattr__
+    # '_meta_info_field', '_data_fields' is immutable.
+    with pytest.raises(AttributeError):
+        instance_data._data_fields = dict()
+    with pytest.raises(AttributeError):
+        instance_data._data_fields = dict()
+
+    # all attribute in instance_data_field should be
+    # (torch.Tensor, np.ndarray, list))
+    with pytest.raises(AssertionError):
+        instance_data.a = 1000
+
+    # instance_data field should has same length
+    new_instance_data = instance_data.new()
+    new_instance_data.det_bbox = torch.rand(100, 4)
+    new_instance_data.det_label = torch.arange(100)
+    with pytest.raises(AssertionError):
+        new_instance_data.scores = torch.rand(101, 1)
+    new_instance_data.none = [None] * 100
+    with pytest.raises(AssertionError):
+        new_instance_data.scores = [None] * 101
+    new_instance_data.numpy_det = np.random.random([100, 1])
+    with pytest.raises(AssertionError):
+        new_instance_data.scores = np.random.random([101, 1])
+
+    # isinstance(str, slice, int, torch.LongTensor, torch.BoolTensor)
+    item = torch.Tensor([1, 2, 3, 4])
+    with pytest.raises(AssertionError):
+        new_instance_data[item]
+    len(new_instance_data[item.long()]) == 1
+
+    # when input is a bool tensor, The shape of
+    # the input at index 0 should equal to
+    # the value length in instance_data_field
+    with pytest.raises(AssertionError):
+        new_instance_data[item.bool()]
+
+    for i in range(len(new_instance_data)):
+        assert new_instance_data[i].det_label == i
+        assert len(new_instance_data[i]) == 1
+
+    # assert the index should in 0 ~ len(instance_data) -1
+    with pytest.raises(IndexError):
+        new_instance_data[101]
+
+    # assert the index should not be an empty tensor
+    new_new_instance_data = new_instance_data.new()
+    with pytest.raises(AssertionError):
+        new_new_instance_data[0]
+
+    # test str
+    with pytest.raises(AssertionError):
+        instance_data.img_size_dummmy = meta_info['img_size']
+
+    # test slice
+    ten_ressults = new_instance_data[:10]
+    len(ten_ressults) == 10
+    for v in ten_ressults.values():
+        assert len(v) == 10
+
+    # test Longtensor
+    long_tensor = torch.randint(100, (50, ))
+    long_index_instance_data = new_instance_data[long_tensor]
+    assert len(long_index_instance_data) == len(long_tensor)
+    for key, value in long_index_instance_data.items():
+        if not isinstance(value, list):
+            assert (long_index_instance_data[key] == new_instance_data[key]
+                    [long_tensor]).all()
+        else:
+            len(long_tensor) == len(value)
+
+    # test bool tensor
+    bool_tensor = torch.rand(100) > 0.5
+    bool_index_instance_data = new_instance_data[bool_tensor]
+    assert len(bool_index_instance_data) == bool_tensor.sum()
+    for key, value in bool_index_instance_data.items():
+        if not isinstance(value, list):
+            assert (bool_index_instance_data[key] == new_instance_data[key]
+                    [bool_tensor]).all()
+        else:
+            assert len(value) == bool_tensor.sum()
+
+    num_instance = 1000
+    instance_data_list = []
+
+    # assert len(instance_lists) > 0
+    with pytest.raises(AssertionError):
+        instance_data.cat(instance_data_list)
+
+    for _ in range(2):
+        instance_data['bbox'] = torch.rand(num_instance, 4)
+        instance_data['label'] = torch.rand(num_instance, 1)
+        instance_data['mask'] = torch.rand(num_instance, 224, 224)
+        instance_data['instances_infos'] = [1] * num_instance
+        instance_data['cpu_bbox'] = np.random.random((num_instance, 4))
+        if torch.cuda.is_available():
+            instance_data.cuda_tensor = torch.rand(num_instance).cuda()
+            assert instance_data.cuda_tensor.is_cuda
+            cuda_instance_data = instance_data.cuda()
+            assert cuda_instance_data.cuda_tensor.is_cuda
+
+        assert len(instance_data[0]) == 1
+        with pytest.raises(IndexError):
+            return instance_data[num_instance + 1]
+        with pytest.raises(AssertionError):
+            instance_data.centerness = torch.rand(num_instance + 1, 1)
+
+        mask_tensor = torch.rand(num_instance) > 0.5
+        length = mask_tensor.sum()
+        assert len(instance_data[mask_tensor]) == length
+
+        index_tensor = torch.LongTensor([1, 5, 8, 110, 399])
+        length = len(index_tensor)
+
+        assert len(instance_data[index_tensor]) == length
+
+        instance_data_list.append(instance_data)
+
+    cat_resutls = InstanceData.cat(instance_data_list)
+    assert len(cat_resutls) == num_instance * 2
+
+    instances = InstanceData(data=dict(bboxes=torch.rand(4, 4)))
+    # cat only single instance
+    assert len(InstanceData.cat([instances])) == 4
diff --git a/tests/test_utils/test_hook.py b/tests/test_utils/test_hook.py
index 461eb49f..afd17678 100644
--- a/tests/test_utils/test_hook.py
+++ b/tests/test_utils/test_hook.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import logging
 import shutil
 import sys
@@ -11,7 +12,7 @@ import torch.nn as nn
 from mmcv.runner import (CheckpointHook, IterTimerHook, PaviLoggerHook,
                          build_runner)
 from torch.nn.init import constant_
-from torch.utils.data import DataLoader
+from torch.utils.data import DataLoader, Dataset
 
 from mmdet.core.hook import ExpMomentumEMAHook, YOLOXLrUpdaterHook
 from mmdet.core.hook.sync_norm_hook import SyncNormHook
@@ -253,8 +254,72 @@ def test_sync_random_size_hook():
     # Only used to prevent program errors
     SyncRandomSizeHook()
 
-    loader = DataLoader(torch.ones((5, 2)))
+    class DemoDataset(Dataset):
+
+        def __getitem__(self, item):
+            return torch.ones(2)
+
+        def __len__(self):
+            return 5
+
+        def update_dynamic_scale(self, dynamic_scale):
+            pass
+
+    loader = DataLoader(DemoDataset())
     runner = _build_demo_runner()
-    runner.register_hook_from_cfg(dict(type='SyncRandomSizeHook'))
+    runner.register_hook_from_cfg(
+        dict(type='SyncRandomSizeHook', device='cpu'))
     runner.run([loader, loader], [('train', 1), ('val', 1)])
     shutil.rmtree(runner.work_dir)
+
+    if torch.cuda.is_available():
+        runner = _build_demo_runner()
+        runner.register_hook_from_cfg(
+            dict(type='SyncRandomSizeHook', device='cuda'))
+        runner.run([loader, loader], [('train', 1), ('val', 1)])
+        shutil.rmtree(runner.work_dir)
+
+
+@pytest.mark.parametrize('set_loss', [
+    dict(set_loss_nan=False, set_loss_inf=False),
+    dict(set_loss_nan=True, set_loss_inf=False),
+    dict(set_loss_nan=False, set_loss_inf=True)
+])
+def test_check_invalid_loss_hook(set_loss):
+    # Check whether loss is valid during training.
+
+    class DemoModel(nn.Module):
+
+        def __init__(self, set_loss_nan=False, set_loss_inf=False):
+            super().__init__()
+            self.set_loss_nan = set_loss_nan
+            self.set_loss_inf = set_loss_inf
+            self.linear = nn.Linear(2, 1)
+
+        def forward(self, x):
+            return self.linear(x)
+
+        def train_step(self, x, optimizer, **kwargs):
+            if self.set_loss_nan:
+                return dict(loss=torch.tensor(float('nan')))
+            elif self.set_loss_inf:
+                return dict(loss=torch.tensor(float('inf')))
+            else:
+                return dict(loss=self(x))
+
+    loader = DataLoader(torch.ones((5, 2)))
+    runner = _build_demo_runner()
+
+    demo_model = DemoModel(**set_loss)
+    runner.model = demo_model
+    runner.register_hook_from_cfg(
+        dict(type='CheckInvalidLossHook', interval=1))
+    if not set_loss['set_loss_nan'] \
+            and not set_loss['set_loss_inf']:
+        # check loss is valid
+        runner.run([loader], [('train', 1)])
+    else:
+        # check loss is nan or inf
+        with pytest.raises(AssertionError):
+            runner.run([loader], [('train', 1)])
+    shutil.rmtree(runner.work_dir)
diff --git a/tests/test_utils/test_masks.py b/tests/test_utils/test_masks.py
index 808cf08c..1e1d456b 100644
--- a/tests/test_utils/test_masks.py
+++ b/tests/test_utils/test_masks.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import pytest
 import torch
@@ -129,6 +130,35 @@ def test_bitmap_mask_resize():
     assert (resized_masks.masks == truth).all()
 
 
+def test_bitmap_mask_get_bboxes():
+    # resize with empty bitmap masks
+    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))
+    bitmap_masks = BitmapMasks(raw_masks, 28, 28)
+    bboxes = bitmap_masks.get_bboxes()
+    assert len(bboxes) == 0
+
+    # resize with bitmap masks contain 1 instances
+    raw_masks = np.array([[[0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 1, 0, 0, 0, 0],
+                           [0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 1, 0, 0, 0],
+                           [0, 0, 1, 1, 1, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0],
+                           [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0,
+                                                      0]]])
+    bitmap_masks = BitmapMasks(raw_masks, 8, 8)
+    bboxes = bitmap_masks.get_bboxes()
+    assert len(bboxes) == 1
+    truth = np.array([[1, 1, 6, 6]])
+    assert (bboxes == truth).all()
+
+    # resize to non-square
+    raw_masks = np.array([[[1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0],
+                           [0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0,
+                                                      0]]])
+    bitmap_masks = BitmapMasks(raw_masks, 4, 8)
+    bboxes = bitmap_masks.get_bboxes()
+    truth = np.array([[0, 0, 6, 3]])
+    assert (bboxes == truth).all()
+
+
 def test_bitmap_mask_flip():
     # flip with empty bitmap masks
     raw_masks = dummy_raw_bitmap_masks((0, 28, 28))
@@ -386,6 +416,7 @@ def test_polygon_mask_resize():
     assert resized_masks.height == 56
     assert resized_masks.width == 72
     assert resized_masks.to_ndarray().shape == (0, 56, 72)
+    assert len(resized_masks.get_bboxes()) == 0
 
     # resize with polygon masks contain 1 instance 1 part
     raw_masks1 = [[np.array([1, 1, 3, 1, 4, 3, 2, 4, 1, 3], dtype=np.float)]]
@@ -403,6 +434,9 @@ def test_polygon_mask_resize():
          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
         np.uint8)
     assert (resized_masks1.to_ndarray() == truth1).all()
+    bboxes = resized_masks1.get_bboxes()
+    bbox_truth = np.array([[2, 2, 8, 8]])
+    assert (bboxes == bbox_truth).all()
 
     # resize with polygon masks contain 1 instance 2 part
     raw_masks2 = [[
diff --git a/tests/test_utils/test_misc.py b/tests/test_utils/test_misc.py
index 16be906c..f01f72f2 100644
--- a/tests/test_utils/test_misc.py
+++ b/tests/test_utils/test_misc.py
@@ -1,10 +1,11 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import numpy as np
 import pytest
 import torch
 
 from mmdet.core.bbox import distance2bbox
 from mmdet.core.mask.structures import BitmapMasks, PolygonMasks
-from mmdet.core.utils import mask2ndarray
+from mmdet.core.utils import center_of_mass, mask2ndarray
 
 
 def dummy_raw_polygon_masks(size):
@@ -90,3 +91,20 @@ def test_distance2bbox():
     deltas = torch.zeros((2, 0, 4))
     out = distance2bbox(rois, deltas, max_shape=(120, 100))
     assert rois.shape == out.shape
+
+
+@pytest.mark.parametrize('mask', [
+    torch.ones((28, 28)),
+    torch.zeros((28, 28)),
+    torch.rand(28, 28) > 0.5,
+    torch.tensor([[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]])
+])
+def test_center_of_mass(mask):
+    center_h, center_w = center_of_mass(mask)
+    if mask.shape[0] == 4:
+        assert center_h == 1.5
+        assert center_w == 1.5
+    assert isinstance(center_h, torch.Tensor) \
+           and isinstance(center_w, torch.Tensor)
+    assert 0 <= center_h <= 28 \
+           and 0 <= center_w <= 28
diff --git a/tests/test_utils/test_nms.py b/tests/test_utils/test_nms.py
new file mode 100644
index 00000000..5fa92dc5
--- /dev/null
+++ b/tests/test_utils/test_nms.py
@@ -0,0 +1,75 @@
+import pytest
+import torch
+
+from mmdet.core.post_processing import mask_matrix_nms
+
+
+def _create_mask(N, h, w):
+    masks = torch.rand((N, h, w)) > 0.5
+    labels = torch.rand(N)
+    scores = torch.rand(N)
+    return masks, labels, scores
+
+
+def test_nms_input_errors():
+    with pytest.raises(AssertionError):
+        mask_matrix_nms(
+            torch.rand((10, 28, 28)), torch.rand(11), torch.rand(11))
+    with pytest.raises(AssertionError):
+        masks = torch.rand((10, 28, 28))
+        mask_matrix_nms(
+            masks,
+            torch.rand(11),
+            torch.rand(11),
+            mask_area=masks.sum((1, 2)).float()[:8])
+    with pytest.raises(NotImplementedError):
+        mask_matrix_nms(
+            torch.rand((10, 28, 28)),
+            torch.rand(10),
+            torch.rand(10),
+            kernel='None')
+    # test an empty results
+    masks, labels, scores = _create_mask(0, 28, 28)
+    score, label, mask, keep_ind = \
+        mask_matrix_nms(masks, labels, scores)
+    assert len(score) == len(label) == \
+           len(mask) == len(keep_ind) == 0
+
+    # do not use update_thr, nms_pre and max_num
+    masks, labels, scores = _create_mask(1000, 28, 28)
+    score, label, mask, keep_ind = \
+        mask_matrix_nms(masks, labels, scores)
+    assert len(score) == len(label) == \
+           len(mask) == len(keep_ind) == 1000
+    # only use nms_pre
+    score, label, mask, keep_ind = \
+        mask_matrix_nms(masks, labels, scores, nms_pre=500)
+    assert len(score) == len(label) == \
+           len(mask) == len(keep_ind) == 500
+    # use max_num
+    score, label, mask, keep_ind = \
+        mask_matrix_nms(masks, labels, scores,
+                        nms_pre=500, max_num=100)
+    assert len(score) == len(label) == \
+           len(mask) == len(keep_ind) == 100
+
+    masks, labels, _ = _create_mask(1, 28, 28)
+    scores = torch.Tensor([1.0])
+    masks = masks.expand(1000, 28, 28)
+    labels = labels.expand(1000)
+    scores = scores.expand(1000)
+
+    # assert scores is decayed and update_thr is worked
+    # if with the same mask, label, and all scores = 1
+    # the first score will set to 1, others will decay.
+    score, label, mask, keep_ind = \
+        mask_matrix_nms(masks,
+                        labels,
+                        scores,
+                        nms_pre=500,
+                        max_num=100,
+                        kernel='gaussian',
+                        sigma=2.0,
+                        filter_thr=0.5)
+    assert len(score) == 1
+    assert score[0] == 1
diff --git a/tests/test_utils/test_version.py b/tests/test_utils/test_version.py
index 6ddf45c0..87d2fabb 100644
--- a/tests/test_utils/test_version.py
+++ b/tests/test_utils/test_version.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from mmdet import digit_version
 
 
diff --git a/tests/test_utils/test_visualization.py b/tests/test_utils/test_visualization.py
index 9c7969b4..aad66bb0 100644
--- a/tests/test_utils/test_visualization.py
+++ b/tests/test_utils/test_visualization.py
@@ -1,4 +1,4 @@
-# Copyright (c) Open-MMLab. All rights reserved.
+# Copyright (c) OpenMMLab. All rights reserved.
 import os
 import os.path as osp
 import tempfile
diff --git a/tools/analysis_tools/analyze_logs.py b/tools/analysis_tools/analyze_logs.py
index 37797741..8ca81d38 100644
--- a/tools/analysis_tools/analyze_logs.py
+++ b/tools/analysis_tools/analyze_logs.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import json
 from collections import defaultdict
diff --git a/tools/analysis_tools/analyze_results.py b/tools/analysis_tools/analyze_results.py
index 2e8e67d3..60e20fea 100644
--- a/tools/analysis_tools/analyze_results.py
+++ b/tools/analysis_tools/analyze_results.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os.path as osp
 
diff --git a/tools/analysis_tools/benchmark.py b/tools/analysis_tools/benchmark.py
index 9e8bec6b..70b18180 100644
--- a/tools/analysis_tools/benchmark.py
+++ b/tools/analysis_tools/benchmark.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import time
diff --git a/tools/analysis_tools/coco_error_analysis.py b/tools/analysis_tools/coco_error_analysis.py
index 722efe6d..102ea4eb 100644
--- a/tools/analysis_tools/coco_error_analysis.py
+++ b/tools/analysis_tools/coco_error_analysis.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import os
 from argparse import ArgumentParser
diff --git a/tools/analysis_tools/eval_metric.py b/tools/analysis_tools/eval_metric.py
index 5732719d..c55bced3 100644
--- a/tools/analysis_tools/eval_metric.py
+++ b/tools/analysis_tools/eval_metric.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
 import mmcv
diff --git a/tools/analysis_tools/get_flops.py b/tools/analysis_tools/get_flops.py
index e3cfe8e8..7f1c3bfc 100644
--- a/tools/analysis_tools/get_flops.py
+++ b/tools/analysis_tools/get_flops.py
@@ -1,5 +1,7 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
+import numpy as np
 import torch
 from mmcv import Config, DictAction
 
@@ -30,6 +32,12 @@ def parse_args():
         'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
         'Note that the quotation marks are necessary and that no white space '
         'is allowed.')
+    parser.add_argument(
+        '--size-divisor',
+        type=int,
+        default=32,
+        help='Pad the input image, the minimum size that is divisible '
+        'by size_divisor, -1 means do not pad the image.')
     args = parser.parse_args()
     return args
 
@@ -39,11 +47,18 @@ def main():
     args = parse_args()
 
     if len(args.shape) == 1:
-        input_shape = (3, args.shape[0], args.shape[0])
+        h = w = args.shape[0]
     elif len(args.shape) == 2:
-        input_shape = (3, ) + tuple(args.shape)
+        h, w = args.shape
     else:
         raise ValueError('invalid input shape')
+    orig_shape = (3, h, w)
+    divisor = args.size_divisor
+    if divisor > 0:
+        h = int(np.ceil(h / divisor)) * divisor
+        w = int(np.ceil(w / divisor)) * divisor
+
+    input_shape = (3, h, w)
 
     cfg = Config.fromfile(args.config)
     if args.cfg_options is not None:
@@ -70,6 +85,11 @@ def main():
 
     flops, params = get_model_complexity_info(model, input_shape)
     split_line = '=' * 30
+
+    if divisor > 0 and \
+            input_shape != orig_shape:
+        print(f'{split_line}\nUse size divisor set input shape '
+              f'from {orig_shape} to {input_shape}\n')
     print(f'{split_line}\nInput shape: {input_shape}\n'
           f'Flops: {flops}\nParams: {params}\n{split_line}')
     print('!!!Please be cautious if you use the results in papers. '
diff --git a/tools/analysis_tools/optimize_anchors.py b/tools/analysis_tools/optimize_anchors.py
new file mode 100644
index 00000000..d0da0cbc
--- /dev/null
+++ b/tools/analysis_tools/optimize_anchors.py
@@ -0,0 +1,370 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+"""Optimize anchor settings on a specific dataset.
+
+This script provides two method to optimize YOLO anchors including k-means
+anchor cluster and differential evolution. You can use ``--algorithm k-means``
+and ``--algorithm differential_evolution`` to switch two method.
+
+Example:
+    Use k-means anchor cluster::
+
+        python tools/analysis_tools/optimize_anchors.py ${CONFIG} \
+        --algorithm k-means --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} \
+        --output-dir ${OUTPUT_DIR}
+    Use differential evolution to optimize anchors::
+
+        python tools/analysis_tools/optimize_anchors.py ${CONFIG} \
+        --algorithm differential_evolution \
+        --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} \
+        --output-dir ${OUTPUT_DIR}
+"""
+import argparse
+import os.path as osp
+
+import mmcv
+import numpy as np
+import torch
+from mmcv import Config
+from scipy.optimize import differential_evolution
+
+from mmdet.core import bbox_cxcywh_to_xyxy, bbox_overlaps, bbox_xyxy_to_cxcywh
+from mmdet.datasets import build_dataset
+from mmdet.utils import get_root_logger
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Optimize anchor parameters.')
+    parser.add_argument('config', help='Train config file path.')
+    parser.add_argument(
+        '--device', default='cuda:0', help='Device used for calculating.')
+    parser.add_argument(
+        '--input-shape',
+        type=int,
+        nargs='+',
+        default=[608, 608],
+        help='input image size')
+    parser.add_argument(
+        '--algorithm',
+        default='differential_evolution',
+        help='Algorithm used for anchor optimizing.'
+        'Support k-means and differential_evolution for YOLO.')
+    parser.add_argument(
+        '--iters',
+        default=1000,
+        type=int,
+        help='Maximum iterations for optimizer.')
+    parser.add_argument(
+        '--output-dir',
+        default=None,
+        type=str,
+        help='Path to save anchor optimize result.')
+
+    args = parser.parse_args()
+    return args
+
+
+class BaseAnchorOptimizer:
+    """Base class for anchor optimizer.
+
+    Args:
+        dataset (obj:`Dataset`): Dataset object.
+        input_shape (list[int]): Input image shape of the model.
+            Format in [width, height].
+        logger (obj:`logging.Logger`): The logger for logging.
+        device (str, optional): Device used for calculating.
+            Default: 'cuda:0'
+        out_dir (str, optional): Path to save anchor optimize result.
+            Default: None
+    """
+
+    def __init__(self,
+                 dataset,
+                 input_shape,
+                 logger,
+                 device='cuda:0',
+                 out_dir=None):
+        self.dataset = dataset
+        self.input_shape = input_shape
+        self.logger = logger
+        self.device = device
+        self.out_dir = out_dir
+        bbox_whs, img_shapes = self.get_whs_and_shapes()
+        ratios = img_shapes.max(1, keepdims=True) / np.array([input_shape])
+
+        # resize to input shape
+        self.bbox_whs = bbox_whs / ratios
+
+    def get_whs_and_shapes(self):
+        """Get widths and heights of bboxes and shapes of images.
+
+        Returns:
+            tuple[np.ndarray]: Array of bbox shapes and array of image
+            shapes with shape (num_bboxes, 2) in [width, height] format.
+        """
+        self.logger.info('Collecting bboxes from annotation...')
+        bbox_whs = []
+        img_shapes = []
+        prog_bar = mmcv.ProgressBar(len(self.dataset))
+        for idx in range(len(self.dataset)):
+            ann = self.dataset.get_ann_info(idx)
+            data_info = self.dataset.data_infos[idx]
+            img_shape = np.array([data_info['width'], data_info['height']])
+            gt_bboxes = ann['bboxes']
+            for bbox in gt_bboxes:
+                wh = bbox[2:4] - bbox[0:2]
+                img_shapes.append(img_shape)
+                bbox_whs.append(wh)
+            prog_bar.update()
+        print('\n')
+        bbox_whs = np.array(bbox_whs)
+        img_shapes = np.array(img_shapes)
+        self.logger.info(f'Collected {bbox_whs.shape[0]} bboxes.')
+        return bbox_whs, img_shapes
+
+    def get_zero_center_bbox_tensor(self):
+        """Get a tensor of bboxes centered at (0, 0).
+
+        Returns:
+            Tensor: Tensor of bboxes with shape (num_bboxes, 4)
+            in [xmin, ymin, xmax, ymax] format.
+        """
+        whs = torch.from_numpy(self.bbox_whs).to(
+            self.device, dtype=torch.float32)
+        bboxes = bbox_cxcywh_to_xyxy(
+            torch.cat([torch.zeros_like(whs), whs], dim=1))
+        return bboxes
+
+    def optimize(self):
+        raise NotImplementedError
+
+    def save_result(self, anchors, path=None):
+        anchor_results = []
+        for w, h in anchors:
+            anchor_results.append([round(w), round(h)])
+        self.logger.info(f'Anchor optimize result:{anchor_results}')
+        if path:
+            json_path = osp.join(path, 'anchor_optimize_result.json')
+            mmcv.dump(anchor_results, json_path)
+            self.logger.info(f'Result saved in {json_path}')
+
+
+class YOLOKMeansAnchorOptimizer(BaseAnchorOptimizer):
+    r"""YOLO anchor optimizer using k-means. Code refer to `AlexeyAB/darknet.
+    <https://github.com/AlexeyAB/darknet/blob/master/src/detector.c>`_.
+
+    Args:
+        num_anchors (int) : Number of anchors.
+        iters (int): Maximum iterations for k-means.
+    """
+
+    def __init__(self, num_anchors, iters, **kwargs):
+
+        super(YOLOKMeansAnchorOptimizer, self).__init__(**kwargs)
+        self.num_anchors = num_anchors
+        self.iters = iters
+
+    def optimize(self):
+        anchors = self.kmeans_anchors()
+        self.save_result(anchors, self.out_dir)
+
+    def kmeans_anchors(self):
+        self.logger.info(
+            f'Start cluster {self.num_anchors} YOLO anchors with K-means...')
+        bboxes = self.get_zero_center_bbox_tensor()
+        cluster_center_idx = torch.randint(
+            0, bboxes.shape[0], (self.num_anchors, )).to(self.device)
+
+        assignments = torch.zeros((bboxes.shape[0], )).to(self.device)
+        cluster_centers = bboxes[cluster_center_idx]
+        if self.num_anchors == 1:
+            cluster_centers = self.kmeans_maximization(bboxes, assignments,
+                                                       cluster_centers)
+            anchors = bbox_xyxy_to_cxcywh(cluster_centers)[:, 2:].cpu().numpy()
+            anchors = sorted(anchors, key=lambda x: x[0] * x[1])
+            return anchors
+
+        prog_bar = mmcv.ProgressBar(self.iters)
+        for i in range(self.iters):
+            converged, assignments = self.kmeans_expectation(
+                bboxes, assignments, cluster_centers)
+            if converged:
+                self.logger.info(f'K-means process has converged at iter {i}.')
+                break
+            cluster_centers = self.kmeans_maximization(bboxes, assignments,
+                                                       cluster_centers)
+            prog_bar.update()
+        print('\n')
+        avg_iou = bbox_overlaps(bboxes,
+                                cluster_centers).max(1)[0].mean().item()
+
+        anchors = bbox_xyxy_to_cxcywh(cluster_centers)[:, 2:].cpu().numpy()
+        anchors = sorted(anchors, key=lambda x: x[0] * x[1])
+        self.logger.info(f'Anchor cluster finish. Average IOU: {avg_iou}')
+
+        return anchors
+
+    def kmeans_maximization(self, bboxes, assignments, centers):
+        """Maximization part of EM algorithm(Expectation-Maximization)"""
+        new_centers = torch.zeros_like(centers)
+        for i in range(centers.shape[0]):
+            mask = (assignments == i)
+            if mask.sum():
+                new_centers[i, :] = bboxes[mask].mean(0)
+        return new_centers
+
+    def kmeans_expectation(self, bboxes, assignments, centers):
+        """Expectation part of EM algorithm(Expectation-Maximization)"""
+        ious = bbox_overlaps(bboxes, centers)
+        closest = ious.argmax(1)
+        converged = (closest == assignments).all()
+        return converged, closest
+
+
+class YOLODEAnchorOptimizer(BaseAnchorOptimizer):
+    """YOLO anchor optimizer using differential evolution algorithm.
+
+    Args:
+        num_anchors (int) : Number of anchors.
+        iters (int): Maximum iterations for k-means.
+        strategy (str): The differential evolution strategy to use.
+            Should be one of:
+
+                - 'best1bin'
+                - 'best1exp'
+                - 'rand1exp'
+                - 'randtobest1exp'
+                - 'currenttobest1exp'
+                - 'best2exp'
+                - 'rand2exp'
+                - 'randtobest1bin'
+                - 'currenttobest1bin'
+                - 'best2bin'
+                - 'rand2bin'
+                - 'rand1bin'
+
+            Default: 'best1bin'.
+        population_size (int): Total population size of evolution algorithm.
+            Default: 15.
+        convergence_thr (float): Tolerance for convergence, the
+            optimizing stops when ``np.std(pop) <= abs(convergence_thr)
+            + convergence_thr * np.abs(np.mean(population_energies))``,
+            respectively. Default: 0.0001.
+        mutation (tuple[float]): Range of dithering randomly changes the
+            mutation constant. Default: (0.5, 1).
+        recombination (float): Recombination constant of crossover probability.
+            Default: 0.7.
+    """
+
+    def __init__(self,
+                 num_anchors,
+                 iters,
+                 strategy='best1bin',
+                 population_size=15,
+                 convergence_thr=0.0001,
+                 mutation=(0.5, 1),
+                 recombination=0.7,
+                 **kwargs):
+
+        super(YOLODEAnchorOptimizer, self).__init__(**kwargs)
+
+        self.num_anchors = num_anchors
+        self.iters = iters
+        self.strategy = strategy
+        self.population_size = population_size
+        self.convergence_thr = convergence_thr
+        self.mutation = mutation
+        self.recombination = recombination
+
+    def optimize(self):
+        anchors = self.differential_evolution()
+        self.save_result(anchors, self.out_dir)
+
+    def differential_evolution(self):
+        bboxes = self.get_zero_center_bbox_tensor()
+
+        bounds = []
+        for i in range(self.num_anchors):
+            bounds.extend([(0, self.input_shape[0]), (0, self.input_shape[1])])
+
+        result = differential_evolution(
+            func=self.avg_iou_cost,
+            bounds=bounds,
+            args=(bboxes, ),
+            strategy=self.strategy,
+            maxiter=self.iters,
+            popsize=self.population_size,
+            tol=self.convergence_thr,
+            mutation=self.mutation,
+            recombination=self.recombination,
+            updating='immediate',
+            disp=True)
+        self.logger.info(
+            f'Anchor evolution finish. Average IOU: {1 - result.fun}')
+        anchors = [(w, h) for w, h in zip(result.x[::2], result.x[1::2])]
+        anchors = sorted(anchors, key=lambda x: x[0] * x[1])
+        return anchors
+
+    @staticmethod
+    def avg_iou_cost(anchor_params, bboxes):
+        assert len(anchor_params) % 2 == 0
+        anchor_whs = torch.tensor(
+            [[w, h]
+             for w, h in zip(anchor_params[::2], anchor_params[1::2])]).to(
+                 bboxes.device, dtype=bboxes.dtype)
+        anchor_boxes = bbox_cxcywh_to_xyxy(
+            torch.cat([torch.zeros_like(anchor_whs), anchor_whs], dim=1))
+        ious = bbox_overlaps(bboxes, anchor_boxes)
+        max_ious, _ = ious.max(1)
+        cost = 1 - max_ious.mean().item()
+        return cost
+
+
+def main():
+    logger = get_root_logger()
+    args = parse_args()
+    cfg = args.config
+    cfg = Config.fromfile(cfg)
+
+    input_shape = args.input_shape
+    assert len(input_shape) == 2
+
+    anchor_type = cfg.model.bbox_head.anchor_generator.type
+    assert anchor_type == 'YOLOAnchorGenerator', \
+        f'Only support optimize YOLOAnchor, but get {anchor_type}.'
+
+    base_sizes = cfg.model.bbox_head.anchor_generator.base_sizes
+    num_anchors = sum([len(sizes) for sizes in base_sizes])
+
+    train_data_cfg = cfg.data.train
+    while 'dataset' in train_data_cfg:
+        train_data_cfg = train_data_cfg['dataset']
+    dataset = build_dataset(train_data_cfg)
+
+    if args.algorithm == 'k-means':
+        optimizer = YOLOKMeansAnchorOptimizer(
+            dataset=dataset,
+            input_shape=input_shape,
+            device=args.device,
+            num_anchors=num_anchors,
+            iters=args.iters,
+            logger=logger,
+            out_dir=args.output_dir)
+    elif args.algorithm == 'differential_evolution':
+        optimizer = YOLODEAnchorOptimizer(
+            dataset=dataset,
+            input_shape=input_shape,
+            device=args.device,
+            num_anchors=num_anchors,
+            iters=args.iters,
+            logger=logger,
+            out_dir=args.output_dir)
+    else:
+        raise NotImplementedError(
+            f'Only support k-means and differential_evolution, '
+            f'but get {args.algorithm}')
+
+    optimizer.optimize()
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/analysis_tools/robustness_eval.py b/tools/analysis_tools/robustness_eval.py
index cc2e27b6..da5ec289 100644
--- a/tools/analysis_tools/robustness_eval.py
+++ b/tools/analysis_tools/robustness_eval.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 from argparse import ArgumentParser
 
diff --git a/tools/analysis_tools/test_robustness.py b/tools/analysis_tools/test_robustness.py
index ae30c019..ab09366d 100644
--- a/tools/analysis_tools/test_robustness.py
+++ b/tools/analysis_tools/test_robustness.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import copy
 import os
diff --git a/tools/dataset_converters/cityscapes.py b/tools/dataset_converters/cityscapes.py
index bde3dac4..c8e44b96 100644
--- a/tools/dataset_converters/cityscapes.py
+++ b/tools/dataset_converters/cityscapes.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import glob
 import os.path as osp
diff --git a/tools/dataset_converters/images2coco.py b/tools/dataset_converters/images2coco.py
index ce57e581..1c4e2f14 100644
--- a/tools/dataset_converters/images2coco.py
+++ b/tools/dataset_converters/images2coco.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 
diff --git a/tools/dataset_converters/pascal_voc.py b/tools/dataset_converters/pascal_voc.py
index f109307c..20f88019 100644
--- a/tools/dataset_converters/pascal_voc.py
+++ b/tools/dataset_converters/pascal_voc.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os.path as osp
 import xml.etree.ElementTree as ET
diff --git a/tools/deployment/mmdet2torchserve.py b/tools/deployment/mmdet2torchserve.py
index d1d8501b..70a081a2 100644
--- a/tools/deployment/mmdet2torchserve.py
+++ b/tools/deployment/mmdet2torchserve.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 from argparse import ArgumentParser, Namespace
 from pathlib import Path
 from tempfile import TemporaryDirectory
diff --git a/tools/deployment/mmdet_handler.py b/tools/deployment/mmdet_handler.py
index 568fcd2f..18fc2301 100644
--- a/tools/deployment/mmdet_handler.py
+++ b/tools/deployment/mmdet_handler.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import base64
 import os
 
@@ -62,7 +63,8 @@ class MMdetHandler(BaseHandler):
                     score = float(bbox[-1])
                     if score >= self.threshold:
                         output[image_index].append({
-                            class_name: bbox_coords,
+                            'class_name': class_name,
+                            'bbox': bbox_coords,
                             'score': score
                         })
 
diff --git a/tools/deployment/onnx2tensorrt.py b/tools/deployment/onnx2tensorrt.py
index 05636d0c..487bf2b3 100644
--- a/tools/deployment/onnx2tensorrt.py
+++ b/tools/deployment/onnx2tensorrt.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
diff --git a/tools/deployment/pytorch2onnx.py b/tools/deployment/pytorch2onnx.py
index 0e10000d..c1789b44 100644
--- a/tools/deployment/pytorch2onnx.py
+++ b/tools/deployment/pytorch2onnx.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os.path as osp
 import warnings
@@ -22,7 +23,8 @@ def pytorch2onnx(model,
                  verify=False,
                  test_img=None,
                  do_simplify=False,
-                 dynamic_export=None):
+                 dynamic_export=None,
+                 skip_postprocess=False):
 
     input_config = {
         'input_shape': input_shape,
@@ -32,6 +34,26 @@ def pytorch2onnx(model,
     # prepare input
     one_img, one_meta = preprocess_example_input(input_config)
     img_list, img_meta_list = [one_img], [[one_meta]]
+
+    if skip_postprocess:
+        warnings.warn('Not all models support export onnx without post '
+                      'process, especially two stage detectors!')
+        model.forward = model.forward_dummy
+        torch.onnx.export(
+            model,
+            one_img,
+            output_file,
+            input_names=['input'],
+            export_params=True,
+            keep_initializers_as_inputs=True,
+            do_constant_folding=True,
+            verbose=show,
+            opset_version=opset_version)
+
+        print(f'Successfully exported ONNX model without '
+              f'post process: {output_file}')
+        return
+
     # replace original forward function
     origin_forward = model.forward
     model.forward = partial(
@@ -49,8 +71,8 @@ def pytorch2onnx(model,
         dynamic_axes = {
             input_name: {
                 0: 'batch',
-                2: 'width',
-                3: 'height'
+                2: 'height',
+                3: 'width'
             },
             'dets': {
                 0: 'batch',
@@ -99,8 +121,16 @@ def pytorch2onnx(model,
         ), f'Requires to install onnx-simplify>={min_required_version}'
 
         input_dic = {'input': img_list[0].detach().cpu().numpy()}
-        onnxsim.simplify(
-            output_file, input_data=input_dic, custom_lib=ort_custom_op_path)
+        model_opt, check_ok = onnxsim.simplify(
+            output_file,
+            input_data=input_dic,
+            custom_lib=ort_custom_op_path,
+            dynamic_input_shape=dynamic_export)
+        if check_ok:
+            onnx.save(model_opt, output_file)
+            print(f'Successfully simplified ONNX model: {output_file}')
+        else:
+            warnings.warn('Failed to simplify ONNX model.')
     print(f'Successfully exported ONNX model: {output_file}')
 
     if verify:
@@ -252,6 +282,12 @@ def parse_args():
         '--dynamic-export',
         action='store_true',
         help='Whether to export onnx with dynamic axis.')
+    parser.add_argument(
+        '--skip-postprocess',
+        action='store_true',
+        help='Whether to export model without post process. Experimental '
+        'option. We do not guarantee the correctness of the exported '
+        'model.')
     args = parser.parse_args()
     return args
 
@@ -305,4 +341,5 @@ if __name__ == '__main__':
         verify=args.verify,
         test_img=args.test_img,
         do_simplify=args.simplify,
-        dynamic_export=args.dynamic_export)
+        dynamic_export=args.dynamic_export,
+        skip_postprocess=args.skip_postprocess)
diff --git a/tools/deployment/test.py b/tools/deployment/test.py
index a8341aaf..b32b7733 100644
--- a/tools/deployment/test.py
+++ b/tools/deployment/test.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
 import mmcv
diff --git a/tools/deployment/test_torchserver.py b/tools/deployment/test_torchserver.py
new file mode 100644
index 00000000..dd45234b
--- /dev/null
+++ b/tools/deployment/test_torchserver.py
@@ -0,0 +1,74 @@
+from argparse import ArgumentParser
+
+import numpy as np
+import requests
+
+from mmdet.apis import inference_detector, init_detector, show_result_pyplot
+from mmdet.core import bbox2result
+
+
+def parse_args():
+    parser = ArgumentParser()
+    parser.add_argument('img', help='Image file')
+    parser.add_argument('config', help='Config file')
+    parser.add_argument('checkpoint', help='Checkpoint file')
+    parser.add_argument('model_name', help='The model name in the server')
+    parser.add_argument(
+        '--inference-addr',
+        default='127.0.0.1:8080',
+        help='Address and port of the inference server')
+    parser.add_argument(
+        '--device', default='cuda:0', help='Device used for inference')
+    parser.add_argument(
+        '--score-thr', type=float, default=0.5, help='bbox score threshold')
+    args = parser.parse_args()
+    return args
+
+
+def parse_result(input, model_class):
+    bbox = []
+    label = []
+    score = []
+    for anchor in input:
+        bbox.append(anchor['bbox'])
+        label.append(model_class.index(anchor['class_name']))
+        score.append([anchor['score']])
+    bboxes = np.append(bbox, score, axis=1)
+    labels = np.array(label)
+    result = bbox2result(bboxes, labels, len(model_class))
+    return result
+
+
+def main(args):
+    # build the model from a config file and a checkpoint file
+    model = init_detector(args.config, args.checkpoint, device=args.device)
+    # test a single image
+    model_result = inference_detector(model, args.img)
+    for i, anchor_set in enumerate(model_result):
+        anchor_set = anchor_set[anchor_set[:, 4] >= 0.5]
+        model_result[i] = anchor_set
+    # show the results
+    show_result_pyplot(
+        model,
+        args.img,
+        model_result,
+        score_thr=args.score_thr,
+        title='pytorch_result')
+    url = 'http://' + args.inference_addr + '/predictions/' + args.model_name
+    with open(args.img, 'rb') as image:
+        response = requests.post(url, image)
+    server_result = parse_result(response.json(), model.CLASSES)
+    show_result_pyplot(
+        model,
+        args.img,
+        server_result,
+        score_thr=args.score_thr,
+        title='server_result')
+
+    for i in range(len(model.CLASSES)):
+        assert np.allclose(model_result[i], server_result[i])
+
+
+if __name__ == '__main__':
+    args = parse_args()
+    main(args)
diff --git a/tools/misc/browse_dataset.py b/tools/misc/browse_dataset.py
index 61676590..3b8b8d60 100644
--- a/tools/misc/browse_dataset.py
+++ b/tools/misc/browse_dataset.py
@@ -1,5 +1,7 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
+from collections import Sequence
 from pathlib import Path
 
 import mmcv
@@ -45,6 +47,12 @@ def parse_args():
 
 
 def retrieve_data_cfg(config_path, skip_type, cfg_options):
+
+    def skip_pipeline_steps(config):
+        config['pipeline'] = [
+            x for x in config.pipeline if x['type'] not in skip_type
+        ]
+
     cfg = Config.fromfile(config_path)
     if cfg_options is not None:
         cfg.merge_from_dict(cfg_options)
@@ -56,9 +64,11 @@ def retrieve_data_cfg(config_path, skip_type, cfg_options):
     while 'dataset' in train_data_cfg and train_data_cfg[
             'type'] != 'MultiImageMixDataset':
         train_data_cfg = train_data_cfg['dataset']
-    train_data_cfg['pipeline'] = [
-        x for x in train_data_cfg.pipeline if x['type'] not in skip_type
-    ]
+
+    if isinstance(train_data_cfg, Sequence):
+        [skip_pipeline_steps(c) for c in train_data_cfg]
+    else:
+        skip_pipeline_steps(train_data_cfg)
 
     return cfg
 
diff --git a/tools/misc/print_config.py b/tools/misc/print_config.py
index 3627f81f..e44cda06 100644
--- a/tools/misc/print_config.py
+++ b/tools/misc/print_config.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import warnings
 
diff --git a/tools/model_converters/detectron2pytorch.py b/tools/model_converters/detectron2pytorch.py
index 961e6f57..b7264d53 100644
--- a/tools/model_converters/detectron2pytorch.py
+++ b/tools/model_converters/detectron2pytorch.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 from collections import OrderedDict
 
diff --git a/tools/model_converters/publish_model.py b/tools/model_converters/publish_model.py
index e8926246..219fcdf3 100644
--- a/tools/model_converters/publish_model.py
+++ b/tools/model_converters/publish_model.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import subprocess
 
diff --git a/tools/model_converters/regnet2mmdet.py b/tools/model_converters/regnet2mmdet.py
index 9f4e316d..fbf8c8f3 100644
--- a/tools/model_converters/regnet2mmdet.py
+++ b/tools/model_converters/regnet2mmdet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 from collections import OrderedDict
 
diff --git a/tools/model_converters/selfsup2mmdet.py b/tools/model_converters/selfsup2mmdet.py
index 86daef65..bc8cce1b 100644
--- a/tools/model_converters/selfsup2mmdet.py
+++ b/tools/model_converters/selfsup2mmdet.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 from collections import OrderedDict
 
diff --git a/tools/model_converters/upgrade_model_version.py b/tools/model_converters/upgrade_model_version.py
index 232c8bc4..36ee607c 100644
--- a/tools/model_converters/upgrade_model_version.py
+++ b/tools/model_converters/upgrade_model_version.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import re
 import tempfile
diff --git a/tools/model_converters/upgrade_ssd_version.py b/tools/model_converters/upgrade_ssd_version.py
index 1d756faa..befff455 100644
--- a/tools/model_converters/upgrade_ssd_version.py
+++ b/tools/model_converters/upgrade_ssd_version.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import tempfile
 from collections import OrderedDict
diff --git a/tools/test.py b/tools/test.py
index 461d3ef4..f16ac635 100644
--- a/tools/test.py
+++ b/tools/test.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
diff --git a/tools/train.py b/tools/train.py
index 2fce106f..f59f1953 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -1,3 +1,4 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import copy
 import os
